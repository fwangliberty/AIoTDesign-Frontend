{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Classification_split_70_30.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwangliberty/AIoTDesign-Frontend/blob/master/Baseline_Classification_split_70_30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq5p9swvZ04G"
      },
      "source": [
        "# Baseline Models for CICIDS 2017 Data Set with 71 Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXg4HMXOZ04H"
      },
      "source": [
        "The dataset is splitted into 70:15:15, and then about 7000 benign cases are randomly selected.  We use the following classification methods: PCA+RF,Naive Bayes model, Decision Tree Classifier, Random Foresty with DecisionTree, Logistic Regression Classifier, Adaboost, Voting, kNN, and DNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOKT1sRZ04I"
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH9hOnpmzv6-"
      },
      "source": [
        "def display_metrics(y_test, y_pred, label_names):\r\n",
        "  print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\r\n",
        "\r\n",
        "  print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\r\n",
        "  print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\r\n",
        "  print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\r\n",
        "\r\n",
        "  print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\r\n",
        "  print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\r\n",
        "  print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\r\n",
        "\r\n",
        "  print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\r\n",
        "  print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\r\n",
        "  print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\r\n",
        "\r\n",
        "  print('\\nClassification Report\\n')\r\n",
        "  print(classification_report(y_test, y_pred, target_names=label_names))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCwZZFMTZ04J"
      },
      "source": [
        "def display_all(df):\n",
        "    with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 100): \n",
        "        print(df)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDogrv3Z04J"
      },
      "source": [
        "def make_value2index(attacks):\n",
        "    #make dictionary\n",
        "    attacks = sorted(attacks)\n",
        "    d = {}\n",
        "    counter=0\n",
        "    for attack in attacks:\n",
        "        d[attack] = counter\n",
        "        counter+=1\n",
        "    return d"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3b8hIQZ04K"
      },
      "source": [
        "# chganges label from string to integer/index\n",
        "def encode_label(Y_str):\n",
        "    labels_d = make_value2index(np.unique(Y_str))\n",
        "    Y = [labels_d[y_str] for y_str  in Y_str]\n",
        "    Y = np.array(Y)\n",
        "    return np.array(Y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMisXWZWZ04K"
      },
      "source": [
        "## Step 1. Loading csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwxZ7DrXZ04L"
      },
      "source": [
        "# All columns\n",
        "col_names = ['Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoCkMRAcaIA6"
      },
      "source": [
        "### Option 1. Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uaw_A5kaHSj",
        "outputId": "d48ce417-d97a-4f43-af03-0282f64af3b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qUtBnilZ04M"
      },
      "source": [
        "# load three csv files generated by mlp4nids (Multi-layer perceptron for network intrusion detection )\n",
        "# first load the train set\n",
        "df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set_2.csv',names=col_names, skiprows=1)  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBn8DsjhX_U4",
        "outputId": "861af72f-d961-4c51-b3d2-d829e5d654af"
      },
      "source": [
        "print('Train set size: ', df_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size:  (706812, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYvaCAjZ04P",
        "outputId": "481fbd3c-8ffe-49d0-ff9f-b55fdac9be9f"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set_2.csv',names=col_names, skiprows=1)  \n",
        "print('Test set size: ', df_test.shape)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set_2.csv',names=col_names, skiprows=1)  \n",
        "print('Validation set size: ', df_val.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set size:  (314139, 72)\n",
            "Validation set size:  (235605, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cbbuInpXhvz"
      },
      "source": [
        "### Option 2. Load from local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLcmd-A8Xhv0"
      },
      "source": [
        "dataroot = '../data/cicids2017clean/train_set.csv'\n",
        "df_train = pd.read_csv(dataroot, names=col_names, skiprows=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9oqAbFyXhv0"
      },
      "source": [
        "dataroot = '../data/cicids2017clean/crossval_set.csv'\n",
        "df_val = pd.read_csv(dataroot, names=col_names, skiprows=1) \n",
        "dataroot = '../data/cicids2017clean/test_set.csv'\n",
        "df_test = pd.read_csv(dataroot, names=col_names, skiprows=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls46tMA9Xhv0"
      },
      "source": [
        "## Step 2. Exploring the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "q-oBvrtQXhv0",
        "outputId": "a0beed12-5e2e-4346-8876-d89800482ce2"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>38444</td>\n",
              "      <td>1035</td>\n",
              "      <td>6</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>205128.205100</td>\n",
              "      <td>51282.051280</td>\n",
              "      <td>3.900000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>20</td>\n",
              "      <td>25641.025640</td>\n",
              "      <td>25641.025640</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>2.309401</td>\n",
              "      <td>5.333333e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.00000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1024</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>PortScan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>443</td>\n",
              "      <td>51111</td>\n",
              "      <td>6</td>\n",
              "      <td>470</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4255.319149</td>\n",
              "      <td>4.700000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>470</td>\n",
              "      <td>470</td>\n",
              "      <td>470</td>\n",
              "      <td>470.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>470</td>\n",
              "      <td>470</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>4255.319149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14229</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>51284</td>\n",
              "      <td>22</td>\n",
              "      <td>6</td>\n",
              "      <td>12131553</td>\n",
              "      <td>21</td>\n",
              "      <td>32</td>\n",
              "      <td>2008</td>\n",
              "      <td>2745</td>\n",
              "      <td>640</td>\n",
              "      <td>0</td>\n",
              "      <td>95.619048</td>\n",
              "      <td>140.045163</td>\n",
              "      <td>976</td>\n",
              "      <td>0</td>\n",
              "      <td>85.78125</td>\n",
              "      <td>220.240592</td>\n",
              "      <td>391.788257</td>\n",
              "      <td>4.368773</td>\n",
              "      <td>2.332991e+05</td>\n",
              "      <td>6.333102e+05</td>\n",
              "      <td>2341420</td>\n",
              "      <td>3</td>\n",
              "      <td>10000000</td>\n",
              "      <td>500022.3</td>\n",
              "      <td>889631.024000</td>\n",
              "      <td>2401652</td>\n",
              "      <td>316</td>\n",
              "      <td>12100000</td>\n",
              "      <td>391336.0</td>\n",
              "      <td>786144.7435</td>\n",
              "      <td>2341420</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>680</td>\n",
              "      <td>1032</td>\n",
              "      <td>1.731023</td>\n",
              "      <td>2.637750</td>\n",
              "      <td>0</td>\n",
              "      <td>976</td>\n",
              "      <td>88.018519</td>\n",
              "      <td>189.590272</td>\n",
              "      <td>3.594447e+04</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>89.679245</td>\n",
              "      <td>95.619048</td>\n",
              "      <td>85.78125</td>\n",
              "      <td>21</td>\n",
              "      <td>2008</td>\n",
              "      <td>32</td>\n",
              "      <td>2745</td>\n",
              "      <td>29200</td>\n",
              "      <td>247</td>\n",
              "      <td>16</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SSH-Patator</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>55040</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>735051</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>11607</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>8.666667</td>\n",
              "      <td>10.263203</td>\n",
              "      <td>10135</td>\n",
              "      <td>0</td>\n",
              "      <td>2321.40000</td>\n",
              "      <td>4413.201989</td>\n",
              "      <td>15826.112750</td>\n",
              "      <td>10.883599</td>\n",
              "      <td>1.050073e+05</td>\n",
              "      <td>2.773929e+05</td>\n",
              "      <td>734075</td>\n",
              "      <td>5</td>\n",
              "      <td>713</td>\n",
              "      <td>356.5</td>\n",
              "      <td>406.586399</td>\n",
              "      <td>644</td>\n",
              "      <td>69</td>\n",
              "      <td>735016</td>\n",
              "      <td>183754.0</td>\n",
              "      <td>366880.8013</td>\n",
              "      <td>734075</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>112</td>\n",
              "      <td>4.081349</td>\n",
              "      <td>6.802249</td>\n",
              "      <td>0</td>\n",
              "      <td>10135</td>\n",
              "      <td>1292.555556</td>\n",
              "      <td>3350.634907</td>\n",
              "      <td>1.120000e+07</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1454.125000</td>\n",
              "      <td>8.666667</td>\n",
              "      <td>2321.40000</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>5</td>\n",
              "      <td>11607</td>\n",
              "      <td>8192</td>\n",
              "      <td>229</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DDoS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53980</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>105009131</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.152368</td>\n",
              "      <td>0.038092</td>\n",
              "      <td>3.500000e+07</td>\n",
              "      <td>6.060000e+07</td>\n",
              "      <td>105000000</td>\n",
              "      <td>42</td>\n",
              "      <td>105000000</td>\n",
              "      <td>105000000.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>105000000</td>\n",
              "      <td>105000000</td>\n",
              "      <td>105000000</td>\n",
              "      <td>105000000.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>105000000</td>\n",
              "      <td>105000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>0.019046</td>\n",
              "      <td>0.019046</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>4.800000</td>\n",
              "      <td>4.381780</td>\n",
              "      <td>1.920000e+01</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>229</td>\n",
              "      <td>235</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>105000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>105000000</td>\n",
              "      <td>105000000</td>\n",
              "      <td>DoS slowloris</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Source Port  Destination Port  Protocol  ...   Idle Max   Idle Min          Label\n",
              "0        38444              1035         6  ...          0          0       PortScan\n",
              "1          443             51111         6  ...          0          0         BENIGN\n",
              "2        51284                22         6  ...          0          0    SSH-Patator\n",
              "3        55040                80         6  ...          0          0           DDoS\n",
              "4        53980                80         6  ...  105000000  105000000  DoS slowloris\n",
              "\n",
              "[5 rows x 72 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miDxhmNyZ04N"
      },
      "source": [
        "Count the number of attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpKy7b9fZ04O",
        "scrolled": true,
        "outputId": "e61bbbcf-68e1-4bec-89eb-33bf606707d7"
      },
      "source": [
        "df_train['Label'].value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        393818\n",
              "DoS Hulk                      129511\n",
              "PortScan                       89473\n",
              "DDoS                           71714\n",
              "DoS GoldenEye                   5795\n",
              "FTP-Patator                     4481\n",
              "SSH-Patator                     3331\n",
              "DoS slowloris                   3241\n",
              "DoS Slowhttptest                3056\n",
              "Bot                             1094\n",
              "Web Attack � Brute Force         892\n",
              "Web Attack � XSS                 370\n",
              "Infiltration                      19\n",
              "Web Attack � Sql Injection        12\n",
              "Heartbleed                         5\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2X3KG4zZ04P"
      },
      "source": [
        "Read test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5hWm9Q-Z04Q",
        "outputId": "3a06b36f-99b3-4f59-9b66-5058cf31a589"
      },
      "source": [
        "print('Test set: ')\n",
        "df_test['Label'].value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        175063\n",
              "DoS Hulk                       57443\n",
              "PortScan                       39575\n",
              "DDoS                           32261\n",
              "DoS GoldenEye                   2536\n",
              "FTP-Patator                     1918\n",
              "DoS slowloris                   1488\n",
              "SSH-Patator                     1416\n",
              "DoS Slowhttptest                1390\n",
              "Bot                              492\n",
              "Web Attack � Brute Force         369\n",
              "Web Attack � XSS                 165\n",
              "Infiltration                      13\n",
              "Web Attack � Sql Injection         6\n",
              "Heartbleed                         4\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0-lHNicZ04Q",
        "outputId": "6bd98e06-eb87-4e07-cb94-cdc934fb0eff"
      },
      "source": [
        "print('Validation set: ')\n",
        "df_val['Label'].value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        131119\n",
              "DoS Hulk                       43170\n",
              "PortScan                       29756\n",
              "DDoS                           24050\n",
              "DoS GoldenEye                   1962\n",
              "FTP-Patator                     1536\n",
              "SSH-Patator                     1150\n",
              "DoS slowloris                   1067\n",
              "DoS Slowhttptest                1053\n",
              "Bot                              370\n",
              "Web Attack � Brute Force         246\n",
              "Web Attack � XSS                 117\n",
              "Infiltration                       4\n",
              "Web Attack � Sql Injection         3\n",
              "Heartbleed                         2\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0O9ZuKoXhv3"
      },
      "source": [
        "## Step 3. Encode Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwXKhZOjXhv3"
      },
      "source": [
        "Encoding the labels, and generate numpy array. Note that the label has not been encoded as one-hot coding. We will use one-hot code later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyAnny9yZ04R"
      },
      "source": [
        "### Step 3.1 Encoding train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsR6_hJDZ04R"
      },
      "source": [
        "df_label = df_train['Label']\n",
        "data = df_train.drop(columns=['Label'])\n",
        "Xtrain = data.values\n",
        "y_train = encode_label(df_label.values)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XlV2AK3Z04S"
      },
      "source": [
        "### Step 3.2. Encoding test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVSrGExFZ04S",
        "scrolled": true
      },
      "source": [
        "df_label = df_test['Label']\n",
        "data = df_test.drop(columns=['Label'])\n",
        "Xtest = data.values\n",
        "y_test = encode_label(df_label.values)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO3PgredZ04T"
      },
      "source": [
        "### Step 3.3 Encoding validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ2wDKpZZ04T"
      },
      "source": [
        "df_label = df_val['Label']\n",
        "data = df_val.drop(columns=['Label'])\n",
        "Xval = data.values\n",
        "y_val = encode_label(df_label.values)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viMki-R0Z04Q"
      },
      "source": [
        "## Step 4. Normalization or Standardization\n",
        "\n",
        "The continuous feature values are normalized into the same feature space. This is important when using features that have different measurements, and is a general requirement of many machine learning algorithms. We implement the two methods to see the impact on the final classifications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8ERJidxXhv5"
      },
      "source": [
        "## Option 1. Normalization\n",
        "\n",
        "The values of the datasets are normalized using the Min-Max scaling technique, bringing them all within a range of [0,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c523vLd-Z04R"
      },
      "source": [
        "### Step 4.1 Normalizing train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5izaj07Z04R"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbYQFmfgZ04S",
        "scrolled": true,
        "outputId": "cb2dd996-fd00-4148-d2d6-ac9623f15e48"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(Xtrain)\n",
        "X_train"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.86617838e-01, 1.57935697e-02, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [6.75974670e-03, 7.79927670e-01, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [7.82543679e-01, 3.35708727e-04, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       ...,\n",
              "       [5.42641337e-01, 6.75995300e-03, 3.52941176e-01, ...,\n",
              "        7.32253146e-03, 4.90833333e-01, 4.85000000e-01],\n",
              "       [7.28374151e-01, 3.35708727e-04, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [6.86686503e-01, 5.00175484e-01, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ5x1QxAXhv5"
      },
      "source": [
        "### Step 4.2. Normalizing validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-_BpWSsXhv6",
        "outputId": "11ca584c-83f4-4d4f-eb9b-5de280c65959"
      },
      "source": [
        "X_val = scaler.fit_transform(Xval)\n",
        "X_val"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.75974670e-03, 7.97680098e-01, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [6.75974670e-03, 9.46901709e-01, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [8.78599222e-01, 8.08913309e-04, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       ...,\n",
              "       [8.66163119e-01, 1.22100122e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 8.30833333e-01, 8.30833333e-01],\n",
              "       [9.72289616e-01, 8.08913309e-04, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [9.17128252e-01, 1.22100122e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 7.83333333e-01, 7.83333333e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TItkmTF1Z04S"
      },
      "source": [
        "### Step 4.3. Normalizing test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXmp2w2bZ04T",
        "outputId": "a42daf25-5a51-49ce-9ef2-d66befd20bd3"
      },
      "source": [
        "X_test = scaler.fit_transform(Xtest)\n",
        "X_test"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.42946517e-01, 8.08777525e-04, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [6.75974670e-03, 8.43722818e-01, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [9.47631037e-01, 8.08777525e-04, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       ...,\n",
              "       [6.89799344e-01, 1.22079626e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [8.91096361e-01, 1.22079626e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [9.11772335e-01, 8.08777525e-04, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge2XVkhTXhv6"
      },
      "source": [
        "## Option 2. Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu459dh3Xhv7"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOJud1h5Xhv7",
        "outputId": "f589cabe-5de6-4901-b487-d725f4955c11"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(Xtrain)\n",
        "X_val = scaler.fit_transform(Xval)\n",
        "X_test = scaler.fit_transform(Xtest)\n",
        "\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.44038783, -0.39965161,  1.91238948, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [ 0.32397456, -0.41782911, -0.5219767 , ..., -0.16609924,\n",
              "         2.01162882,  2.14691177],\n",
              "       [ 0.50382029, -0.39647496, -0.5219767 , ..., -0.07444588,\n",
              "         1.22697701,  1.30316625],\n",
              "       ...,\n",
              "       [-1.17899436, -0.41782911, -0.5219767 , ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [ 0.50213454, -0.41941743,  1.91238948, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [ 0.63049256, -0.41782911, -0.5219767 , ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reoNDQZhZ04T"
      },
      "source": [
        "## Step 5 One-hot encoding for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So8gvIF8Z04T"
      },
      "source": [
        "y_train, y_test and y_val have to be one-hot-encoded. That means they must have dimension (number_of_samples, 15), where 15 denotes number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc97u4oZZ04U"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfeM_ZzsXhv8"
      },
      "source": [
        "Save the labels for AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N0GfC_zXhv8"
      },
      "source": [
        "y_train_ada = y_train\n",
        "y_test_ada = y_test\n",
        "y_val_ada = y_val"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQVqV19KZ04U"
      },
      "source": [
        "y_train = to_categorical(y_train, 15)\n",
        "y_test = to_categorical(y_test, 15)\n",
        "y_val = to_categorical(y_val, 15)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd9_XX_5Xhv8"
      },
      "source": [
        "## Step 6. Define the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOSi1KcXhv8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#importing confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#importing accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUup6sodXhv9"
      },
      "source": [
        "METRICS = [\n",
        "      tf.keras.metrics.TruePositives(name='tp'),\n",
        "      tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.AUC(name='auc'),\n",
        "]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJywwX9iXhv9"
      },
      "source": [
        "#  Model 1: PCA  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTYHuZtxXhv9"
      },
      "source": [
        "X_pca = df_train.drop('Label',axis=1)\n",
        "y_pca = df_train['Label']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XWs2_kXhv9"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_pca = scaler.fit_transform(X_pca)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGVxxHw2Xhv9"
      },
      "source": [
        "dfx = pd.DataFrame(data=X_pca,columns=df_train.columns[1:])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-gTg4_uXhv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "548c003f-cd20-4dca-fada-47af0d80efa5"
      },
      "source": [
        "dfx.head(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.586618</td>\n",
              "      <td>0.015794</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.583333e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>8.235825e-07</td>\n",
              "      <td>9.529852e-09</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000872</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.002072</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.410256</td>\n",
              "      <td>3.583333e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.583333e-07</td>\n",
              "      <td>4.333333e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>8.547009e-03</td>\n",
              "      <td>1.282051e-02</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>2.380952e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.235825e-07</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>9.529852e-09</td>\n",
              "      <td>0.015640</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.779928</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.950000e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400851</td>\n",
              "      <td>3.950000e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.950000e-06</td>\n",
              "      <td>4.025000e-06</td>\n",
              "      <td>3.916667e-06</td>\n",
              "      <td>3.916667e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.916667e-06</td>\n",
              "      <td>4.016666e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>1.418440e-03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.217133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.782544</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.010963e-01</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>8.268768e-04</td>\n",
              "      <td>4.359907e-06</td>\n",
              "      <td>0.025786</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016848</td>\n",
              "      <td>0.019654</td>\n",
              "      <td>0.056169</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019626</td>\n",
              "      <td>0.032795</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400001</td>\n",
              "      <td>1.944192e-03</td>\n",
              "      <td>0.007468</td>\n",
              "      <td>1.951187e-02</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>8.333333e-02</td>\n",
              "      <td>4.166853e-03</td>\n",
              "      <td>0.010538</td>\n",
              "      <td>2.001377e-02</td>\n",
              "      <td>2.733333e-06</td>\n",
              "      <td>1.008333e-01</td>\n",
              "      <td>3.261133e-03</td>\n",
              "      <td>0.009313</td>\n",
              "      <td>1.951183e-02</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995805</td>\n",
              "      <td>0.994741</td>\n",
              "      <td>5.770077e-07</td>\n",
              "      <td>1.318875e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039323</td>\n",
              "      <td>0.030143</td>\n",
              "      <td>0.040070</td>\n",
              "      <td>1.604664e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.026873</td>\n",
              "      <td>0.016848</td>\n",
              "      <td>0.019626</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>8.268768e-04</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>4.359907e-06</td>\n",
              "      <td>0.445572</td>\n",
              "      <td>0.003784</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.839857</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>6.125458e-03</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>1.070657e-05</td>\n",
              "      <td>1.843550e-05</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>0.001440</td>\n",
              "      <td>0.583276</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.531129</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.400002</td>\n",
              "      <td>8.750940e-04</td>\n",
              "      <td>0.003271</td>\n",
              "      <td>6.117325e-03</td>\n",
              "      <td>1.500000e-07</td>\n",
              "      <td>5.941667e-06</td>\n",
              "      <td>2.970833e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>5.366667e-06</td>\n",
              "      <td>6.749999e-07</td>\n",
              "      <td>6.125133e-03</td>\n",
              "      <td>1.531283e-03</td>\n",
              "      <td>0.004346</td>\n",
              "      <td>6.117292e-03</td>\n",
              "      <td>4.166667e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>1.360450e-06</td>\n",
              "      <td>3.401125e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.408340</td>\n",
              "      <td>0.442656</td>\n",
              "      <td>0.708152</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.435740</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>0.531129</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>1.070657e-05</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>1.843550e-05</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.823682</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>8.750761e-01</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>6.588660e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.003489</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>2.916667e-01</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>4.583333e-07</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>6.348654e-09</td>\n",
              "      <td>9.522981e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.001644</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>8.571429e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>6.588660e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.117647e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.117647e-07</td>\n",
              "      <td>4.117647e-07</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.798535</td>\n",
              "      <td>0.010758</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.416667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>8.235825e-07</td>\n",
              "      <td>9.529852e-09</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000872</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.002072</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005865</td>\n",
              "      <td>0.410811</td>\n",
              "      <td>3.416667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.416667e-07</td>\n",
              "      <td>4.166666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>9.009009e-03</td>\n",
              "      <td>1.351351e-02</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>2.380952e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.235825e-07</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>9.529852e-09</td>\n",
              "      <td>0.015640</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.415656</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.297167e-04</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>7.412242e-05</td>\n",
              "      <td>4.669627e-07</td>\n",
              "      <td>0.003626</td>\n",
              "      <td>0.039250</td>\n",
              "      <td>0.015858</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008460</td>\n",
              "      <td>0.050760</td>\n",
              "      <td>0.033633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005765</td>\n",
              "      <td>0.400016</td>\n",
              "      <td>1.432611e-04</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>4.296833e-04</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>1.083333e-07</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>1.292942e-05</td>\n",
              "      <td>1.939413e-05</td>\n",
              "      <td>0.062155</td>\n",
              "      <td>0.005923</td>\n",
              "      <td>0.038630</td>\n",
              "      <td>0.006598</td>\n",
              "      <td>4.351339e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.042252</td>\n",
              "      <td>0.015858</td>\n",
              "      <td>0.033633</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>7.412242e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>4.669627e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.864637</td>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.026617e-03</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>3.755536e-04</td>\n",
              "      <td>4.020486e-05</td>\n",
              "      <td>0.009790</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008927</td>\n",
              "      <td>0.010881</td>\n",
              "      <td>0.198607</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.386103</td>\n",
              "      <td>0.180096</td>\n",
              "      <td>0.005863</td>\n",
              "      <td>0.400054</td>\n",
              "      <td>3.211406e-05</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>2.024417e-04</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.026583e-03</td>\n",
              "      <td>6.038725e-05</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>2.251417e-04</td>\n",
              "      <td>5.083333e-07</td>\n",
              "      <td>7.794917e-04</td>\n",
              "      <td>5.567798e-05</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>2.915667e-04</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995805</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>4.870525e-05</td>\n",
              "      <td>6.088157e-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.139041</td>\n",
              "      <td>0.264152</td>\n",
              "      <td>0.241582</td>\n",
              "      <td>5.832889e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.238137</td>\n",
              "      <td>0.008927</td>\n",
              "      <td>0.386103</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>3.755536e-04</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>4.020486e-05</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.015472</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.876799</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>6.774011e-02</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.235374e-05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.002617</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.693505e-02</td>\n",
              "      <td>0.047914</td>\n",
              "      <td>6.772345e-02</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>6.774007e-02</td>\n",
              "      <td>1.693502e-02</td>\n",
              "      <td>0.048130</td>\n",
              "      <td>6.772342e-02</td>\n",
              "      <td>1.083333e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>2.050321e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.004144</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.002055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002158</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>1.235374e-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.959804e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.959804e-05</td>\n",
              "      <td>1.959804e-05</td>\n",
              "      <td>0.067723</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.067723</td>\n",
              "      <td>0.067723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.753765</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.550000e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>2.800180e-05</td>\n",
              "      <td>3.970772e-07</td>\n",
              "      <td>0.001370</td>\n",
              "      <td>0.014828</td>\n",
              "      <td>0.005991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.043163</td>\n",
              "      <td>0.028600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006600</td>\n",
              "      <td>0.404396</td>\n",
              "      <td>5.388889e-07</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>1.491667e-06</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>3.663004e-03</td>\n",
              "      <td>5.494505e-03</td>\n",
              "      <td>0.023481</td>\n",
              "      <td>0.005036</td>\n",
              "      <td>0.024110</td>\n",
              "      <td>0.010534</td>\n",
              "      <td>1.109063e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.026370</td>\n",
              "      <td>0.005991</td>\n",
              "      <td>0.028600</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>2.800180e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>3.970772e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Destination Port  Protocol  Flow Duration  ...  Idle Max  Idle Min     Label\n",
              "0          0.586618  0.015794       0.352941  ...       0.0  0.000000  0.000000\n",
              "1          0.006760  0.779928       0.352941  ...       0.0  0.000000  0.000000\n",
              "2          0.782544  0.000336       0.352941  ...       0.0  0.000000  0.000000\n",
              "3          0.839857  0.001221       0.352941  ...       0.0  0.000000  0.000000\n",
              "4          0.823682  0.001221       0.352941  ...       0.0  0.875000  0.875000\n",
              "5          0.798535  0.010758       0.352941  ...       0.0  0.000000  0.000000\n",
              "6          0.415656  0.000809       1.000000  ...       0.0  0.000000  0.000000\n",
              "7          0.864637  0.006760       0.352941  ...       0.0  0.000000  0.000000\n",
              "8          0.876799  0.001221       0.352941  ...       0.0  0.067723  0.067723\n",
              "9          0.753765  0.000809       1.000000  ...       0.0  0.000000  0.000000\n",
              "\n",
              "[10 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzLYPxy-Xhv-"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIE01O4yXhv-"
      },
      "source": [
        "pca = PCA(n_components=None)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd2crc5zXhv-"
      },
      "source": [
        "dfx_pca = pca.fit(dfx)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPwtI_6RXhv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "fc15d037-8b92-4146-ed11-1ce5115745a7"
      },
      "source": [
        "plt.figure(figsize=(24,5))\n",
        "plt.scatter(x=[i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],\n",
        "            y=dfx_pca.explained_variance_ratio_,\n",
        "           s=200, alpha=0.75,c='orange',edgecolor='k')\n",
        "plt.grid(True)\n",
        "plt.title(\"Explained variance ratio of the \\nfitted principal component vector\\n\",fontsize=25)\n",
        "plt.xlabel(\"Principal components\",fontsize=15)\n",
        "plt.xticks([i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.ylabel(\"Explained variance ratio\",fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeVjWmuZXhv_"
      },
      "source": [
        "The above two plots means that the $1^{st}$ principal component explains about 38% of the total variance in the data and the $2^{nd}$ component explians further 20%. Therefore, if we just consider first two components, they together explain 58% of the total variance. Using the first 10 features should give very hight detection rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8xM86XhXhv_"
      },
      "source": [
        "Transform the scaled data set using the fitted PCA object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srySEJvRXhv_"
      },
      "source": [
        "dfx_trans = pca.transform(dfx)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COMFpsuyXhv_"
      },
      "source": [
        "Put it in a data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rtptL8AMXhwA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "468b468d-6459-4323-c675-617594393f81"
      },
      "source": [
        "dfx_trans = pd.DataFrame(data=dfx_trans)\n",
        "dfx_trans.head(10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.572368</td>\n",
              "      <td>-0.515194</td>\n",
              "      <td>0.239465</td>\n",
              "      <td>0.141834</td>\n",
              "      <td>-0.077080</td>\n",
              "      <td>-0.050958</td>\n",
              "      <td>-0.011967</td>\n",
              "      <td>-0.216864</td>\n",
              "      <td>-0.009903</td>\n",
              "      <td>-0.219252</td>\n",
              "      <td>-0.196395</td>\n",
              "      <td>-0.014758</td>\n",
              "      <td>-0.013319</td>\n",
              "      <td>0.024277</td>\n",
              "      <td>0.034420</td>\n",
              "      <td>-0.010215</td>\n",
              "      <td>-0.023689</td>\n",
              "      <td>0.004559</td>\n",
              "      <td>-0.000519</td>\n",
              "      <td>-0.004628</td>\n",
              "      <td>0.005523</td>\n",
              "      <td>-0.001776</td>\n",
              "      <td>-0.002512</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>0.001419</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>-0.003415</td>\n",
              "      <td>0.004227</td>\n",
              "      <td>0.003843</td>\n",
              "      <td>-0.001942</td>\n",
              "      <td>-0.001313</td>\n",
              "      <td>-0.000683</td>\n",
              "      <td>-0.003356</td>\n",
              "      <td>0.002489</td>\n",
              "      <td>0.002650</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>-0.000088</td>\n",
              "      <td>-0.001708</td>\n",
              "      <td>-0.001003</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>-0.000660</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>-0.000603</td>\n",
              "      <td>-0.000332</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>-0.000057</td>\n",
              "      <td>-5.302619e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.000030</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>4.349300e-07</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>4.646552e-06</td>\n",
              "      <td>-2.717723e-07</td>\n",
              "      <td>-3.075949e-06</td>\n",
              "      <td>4.950278e-06</td>\n",
              "      <td>3.532324e-08</td>\n",
              "      <td>-3.291907e-08</td>\n",
              "      <td>-4.486828e-16</td>\n",
              "      <td>5.423567e-17</td>\n",
              "      <td>2.211322e-16</td>\n",
              "      <td>-2.427291e-17</td>\n",
              "      <td>-8.961834e-17</td>\n",
              "      <td>8.585935e-19</td>\n",
              "      <td>7.210748e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.305131</td>\n",
              "      <td>0.950823</td>\n",
              "      <td>0.467819</td>\n",
              "      <td>0.225980</td>\n",
              "      <td>0.258204</td>\n",
              "      <td>-0.051387</td>\n",
              "      <td>-0.086205</td>\n",
              "      <td>-0.272661</td>\n",
              "      <td>-0.290464</td>\n",
              "      <td>0.162609</td>\n",
              "      <td>0.350706</td>\n",
              "      <td>0.094880</td>\n",
              "      <td>-0.033500</td>\n",
              "      <td>0.034032</td>\n",
              "      <td>-0.010461</td>\n",
              "      <td>-0.154857</td>\n",
              "      <td>-0.029818</td>\n",
              "      <td>-0.030401</td>\n",
              "      <td>0.011193</td>\n",
              "      <td>0.007789</td>\n",
              "      <td>-0.002062</td>\n",
              "      <td>0.008666</td>\n",
              "      <td>0.005130</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>-0.000478</td>\n",
              "      <td>0.000827</td>\n",
              "      <td>-0.003551</td>\n",
              "      <td>-0.000285</td>\n",
              "      <td>-0.006190</td>\n",
              "      <td>-0.006429</td>\n",
              "      <td>0.001214</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>-0.001215</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>0.002226</td>\n",
              "      <td>0.001281</td>\n",
              "      <td>-0.001659</td>\n",
              "      <td>-0.000082</td>\n",
              "      <td>-0.000776</td>\n",
              "      <td>0.002390</td>\n",
              "      <td>-0.001495</td>\n",
              "      <td>-0.000480</td>\n",
              "      <td>0.001111</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-0.000173</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>-0.000154</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>-7.082754e-06</td>\n",
              "      <td>-0.000116</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>-0.000036</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>9.588135e-06</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-7.153991e-06</td>\n",
              "      <td>-6.668797e-07</td>\n",
              "      <td>-3.991366e-06</td>\n",
              "      <td>5.167831e-06</td>\n",
              "      <td>-4.143020e-08</td>\n",
              "      <td>-3.237400e-08</td>\n",
              "      <td>-6.511537e-16</td>\n",
              "      <td>-2.377190e-16</td>\n",
              "      <td>-1.392935e-16</td>\n",
              "      <td>3.587767e-17</td>\n",
              "      <td>1.069630e-16</td>\n",
              "      <td>-5.185134e-18</td>\n",
              "      <td>4.446547e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.457527</td>\n",
              "      <td>-0.671902</td>\n",
              "      <td>0.271425</td>\n",
              "      <td>0.142068</td>\n",
              "      <td>-0.229850</td>\n",
              "      <td>-0.052231</td>\n",
              "      <td>0.034908</td>\n",
              "      <td>-0.036686</td>\n",
              "      <td>0.061558</td>\n",
              "      <td>0.127822</td>\n",
              "      <td>-0.002849</td>\n",
              "      <td>-0.020738</td>\n",
              "      <td>-0.028766</td>\n",
              "      <td>0.023998</td>\n",
              "      <td>0.012764</td>\n",
              "      <td>-0.010203</td>\n",
              "      <td>-0.020757</td>\n",
              "      <td>0.040361</td>\n",
              "      <td>-0.008223</td>\n",
              "      <td>-0.006021</td>\n",
              "      <td>-0.009339</td>\n",
              "      <td>-0.013464</td>\n",
              "      <td>0.019853</td>\n",
              "      <td>-0.001255</td>\n",
              "      <td>-0.021537</td>\n",
              "      <td>-0.001811</td>\n",
              "      <td>-0.003854</td>\n",
              "      <td>-0.002420</td>\n",
              "      <td>-0.005401</td>\n",
              "      <td>0.013051</td>\n",
              "      <td>-0.000660</td>\n",
              "      <td>-0.005515</td>\n",
              "      <td>0.001904</td>\n",
              "      <td>-0.000327</td>\n",
              "      <td>0.002756</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.003733</td>\n",
              "      <td>0.002667</td>\n",
              "      <td>-0.001669</td>\n",
              "      <td>-0.000352</td>\n",
              "      <td>-0.003252</td>\n",
              "      <td>-0.001969</td>\n",
              "      <td>0.003717</td>\n",
              "      <td>-0.000450</td>\n",
              "      <td>-0.000219</td>\n",
              "      <td>0.002499</td>\n",
              "      <td>0.002053</td>\n",
              "      <td>0.006643</td>\n",
              "      <td>-0.005783</td>\n",
              "      <td>0.004277</td>\n",
              "      <td>-9.780799e-06</td>\n",
              "      <td>-0.000079</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>-0.000215</td>\n",
              "      <td>-1.213181e-04</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-0.000374</td>\n",
              "      <td>5.414121e-05</td>\n",
              "      <td>2.725307e-07</td>\n",
              "      <td>-2.279033e-05</td>\n",
              "      <td>-8.580023e-05</td>\n",
              "      <td>1.319279e-07</td>\n",
              "      <td>4.864361e-07</td>\n",
              "      <td>-4.154067e-15</td>\n",
              "      <td>4.085211e-17</td>\n",
              "      <td>2.849225e-16</td>\n",
              "      <td>1.304679e-17</td>\n",
              "      <td>-6.230528e-17</td>\n",
              "      <td>5.439341e-20</td>\n",
              "      <td>-1.199222e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.018007</td>\n",
              "      <td>-0.958150</td>\n",
              "      <td>0.544710</td>\n",
              "      <td>-0.928886</td>\n",
              "      <td>0.546009</td>\n",
              "      <td>0.235393</td>\n",
              "      <td>0.080858</td>\n",
              "      <td>0.321608</td>\n",
              "      <td>-0.037069</td>\n",
              "      <td>-0.111911</td>\n",
              "      <td>0.049312</td>\n",
              "      <td>0.173081</td>\n",
              "      <td>0.028986</td>\n",
              "      <td>-0.003775</td>\n",
              "      <td>-0.046737</td>\n",
              "      <td>-0.004579</td>\n",
              "      <td>-0.127537</td>\n",
              "      <td>0.156488</td>\n",
              "      <td>0.002298</td>\n",
              "      <td>0.010313</td>\n",
              "      <td>0.037278</td>\n",
              "      <td>0.026612</td>\n",
              "      <td>-0.001613</td>\n",
              "      <td>0.004435</td>\n",
              "      <td>-0.052494</td>\n",
              "      <td>-0.001214</td>\n",
              "      <td>-0.008234</td>\n",
              "      <td>-0.004858</td>\n",
              "      <td>0.015100</td>\n",
              "      <td>-0.006544</td>\n",
              "      <td>0.017663</td>\n",
              "      <td>-0.005532</td>\n",
              "      <td>-0.006401</td>\n",
              "      <td>-0.000172</td>\n",
              "      <td>0.007121</td>\n",
              "      <td>-0.005822</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.001474</td>\n",
              "      <td>-0.001792</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.001301</td>\n",
              "      <td>0.005366</td>\n",
              "      <td>-0.006038</td>\n",
              "      <td>0.001407</td>\n",
              "      <td>0.001369</td>\n",
              "      <td>-0.000327</td>\n",
              "      <td>0.003557</td>\n",
              "      <td>0.001126</td>\n",
              "      <td>-0.006974</td>\n",
              "      <td>-0.000703</td>\n",
              "      <td>1.463586e-05</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>-0.000249</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>-0.000118</td>\n",
              "      <td>-1.248034e-05</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.000053</td>\n",
              "      <td>1.497349e-05</td>\n",
              "      <td>8.673254e-07</td>\n",
              "      <td>-6.342471e-06</td>\n",
              "      <td>7.616001e-06</td>\n",
              "      <td>-4.334090e-08</td>\n",
              "      <td>-6.327728e-08</td>\n",
              "      <td>-3.594834e-15</td>\n",
              "      <td>1.575744e-16</td>\n",
              "      <td>1.170033e-16</td>\n",
              "      <td>-4.834644e-17</td>\n",
              "      <td>3.982068e-17</td>\n",
              "      <td>-1.982548e-17</td>\n",
              "      <td>7.966276e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.111311</td>\n",
              "      <td>0.487699</td>\n",
              "      <td>-0.186206</td>\n",
              "      <td>1.179229</td>\n",
              "      <td>-0.910365</td>\n",
              "      <td>1.072180</td>\n",
              "      <td>-0.410163</td>\n",
              "      <td>0.123038</td>\n",
              "      <td>0.621350</td>\n",
              "      <td>-0.077362</td>\n",
              "      <td>-0.406697</td>\n",
              "      <td>1.290645</td>\n",
              "      <td>0.530215</td>\n",
              "      <td>0.341514</td>\n",
              "      <td>-0.501684</td>\n",
              "      <td>-0.156619</td>\n",
              "      <td>0.022990</td>\n",
              "      <td>-0.038226</td>\n",
              "      <td>-0.179310</td>\n",
              "      <td>-0.026010</td>\n",
              "      <td>-0.007122</td>\n",
              "      <td>-0.014347</td>\n",
              "      <td>-0.009021</td>\n",
              "      <td>-0.029360</td>\n",
              "      <td>-0.009615</td>\n",
              "      <td>0.023943</td>\n",
              "      <td>0.009341</td>\n",
              "      <td>0.003127</td>\n",
              "      <td>0.002834</td>\n",
              "      <td>0.016215</td>\n",
              "      <td>0.004983</td>\n",
              "      <td>-0.003414</td>\n",
              "      <td>-0.004694</td>\n",
              "      <td>-0.005702</td>\n",
              "      <td>-0.016135</td>\n",
              "      <td>-0.000598</td>\n",
              "      <td>-0.001549</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>-0.000578</td>\n",
              "      <td>0.002353</td>\n",
              "      <td>-0.010897</td>\n",
              "      <td>0.003042</td>\n",
              "      <td>0.001811</td>\n",
              "      <td>0.002117</td>\n",
              "      <td>0.002084</td>\n",
              "      <td>-0.000799</td>\n",
              "      <td>-0.000184</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>-0.001130</td>\n",
              "      <td>0.000364</td>\n",
              "      <td>1.198177e-05</td>\n",
              "      <td>-0.000062</td>\n",
              "      <td>-0.000412</td>\n",
              "      <td>-0.000157</td>\n",
              "      <td>-0.000107</td>\n",
              "      <td>-4.740993e-05</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>-0.001432</td>\n",
              "      <td>-1.112166e-04</td>\n",
              "      <td>6.155011e-07</td>\n",
              "      <td>-1.099461e-06</td>\n",
              "      <td>8.574106e-07</td>\n",
              "      <td>1.613155e-07</td>\n",
              "      <td>-5.471749e-09</td>\n",
              "      <td>1.628206e-15</td>\n",
              "      <td>4.923747e-17</td>\n",
              "      <td>-2.601401e-16</td>\n",
              "      <td>-3.353459e-19</td>\n",
              "      <td>1.095716e-16</td>\n",
              "      <td>-1.659759e-17</td>\n",
              "      <td>1.389950e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.566619</td>\n",
              "      <td>-0.561331</td>\n",
              "      <td>0.179699</td>\n",
              "      <td>0.080278</td>\n",
              "      <td>-0.189534</td>\n",
              "      <td>-0.053044</td>\n",
              "      <td>0.059086</td>\n",
              "      <td>-0.157664</td>\n",
              "      <td>0.066040</td>\n",
              "      <td>-0.265877</td>\n",
              "      <td>-0.118775</td>\n",
              "      <td>-0.005903</td>\n",
              "      <td>-0.014660</td>\n",
              "      <td>0.014353</td>\n",
              "      <td>0.027434</td>\n",
              "      <td>-0.009793</td>\n",
              "      <td>-0.020754</td>\n",
              "      <td>0.005853</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>-0.004427</td>\n",
              "      <td>0.005853</td>\n",
              "      <td>-0.000817</td>\n",
              "      <td>-0.002353</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.001393</td>\n",
              "      <td>0.004471</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>-0.002510</td>\n",
              "      <td>0.004281</td>\n",
              "      <td>0.002905</td>\n",
              "      <td>-0.001585</td>\n",
              "      <td>-0.001811</td>\n",
              "      <td>-0.000773</td>\n",
              "      <td>-0.003214</td>\n",
              "      <td>0.002142</td>\n",
              "      <td>0.002458</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>-0.001649</td>\n",
              "      <td>-0.001020</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>-0.000619</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>-0.000505</td>\n",
              "      <td>-0.000293</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>-0.000172</td>\n",
              "      <td>7.073086e-07</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>-0.000036</td>\n",
              "      <td>8.670846e-07</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>5.732114e-06</td>\n",
              "      <td>2.226505e-07</td>\n",
              "      <td>-2.203180e-06</td>\n",
              "      <td>4.545460e-06</td>\n",
              "      <td>3.551714e-08</td>\n",
              "      <td>-2.942126e-08</td>\n",
              "      <td>-2.543938e-16</td>\n",
              "      <td>6.464401e-17</td>\n",
              "      <td>2.215659e-16</td>\n",
              "      <td>-2.514027e-17</td>\n",
              "      <td>-9.007913e-17</td>\n",
              "      <td>8.382647e-19</td>\n",
              "      <td>7.210748e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.464064</td>\n",
              "      <td>0.122514</td>\n",
              "      <td>-0.586668</td>\n",
              "      <td>-0.002387</td>\n",
              "      <td>0.286871</td>\n",
              "      <td>-0.002938</td>\n",
              "      <td>-0.253218</td>\n",
              "      <td>-0.058549</td>\n",
              "      <td>-0.118597</td>\n",
              "      <td>0.097333</td>\n",
              "      <td>-0.118712</td>\n",
              "      <td>-0.036989</td>\n",
              "      <td>-0.012763</td>\n",
              "      <td>0.002316</td>\n",
              "      <td>0.025289</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.013406</td>\n",
              "      <td>-0.004041</td>\n",
              "      <td>0.001556</td>\n",
              "      <td>-0.001057</td>\n",
              "      <td>0.013461</td>\n",
              "      <td>-0.004332</td>\n",
              "      <td>0.016243</td>\n",
              "      <td>0.009225</td>\n",
              "      <td>0.008802</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>-0.004233</td>\n",
              "      <td>-0.000524</td>\n",
              "      <td>-0.006474</td>\n",
              "      <td>-0.003136</td>\n",
              "      <td>0.017316</td>\n",
              "      <td>-0.000328</td>\n",
              "      <td>0.009527</td>\n",
              "      <td>0.000862</td>\n",
              "      <td>-0.000028</td>\n",
              "      <td>-0.002590</td>\n",
              "      <td>0.022223</td>\n",
              "      <td>-0.009829</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>0.003014</td>\n",
              "      <td>-0.003109</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>-0.000134</td>\n",
              "      <td>-0.000491</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>-0.001489</td>\n",
              "      <td>-0.001703</td>\n",
              "      <td>-9.813821e-06</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>-1.061806e-06</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-4.863330e-06</td>\n",
              "      <td>-1.346707e-06</td>\n",
              "      <td>-1.961769e-06</td>\n",
              "      <td>2.105225e-06</td>\n",
              "      <td>1.531607e-08</td>\n",
              "      <td>-1.101341e-08</td>\n",
              "      <td>-2.828802e-15</td>\n",
              "      <td>-4.163353e-17</td>\n",
              "      <td>-6.535121e-17</td>\n",
              "      <td>7.988577e-17</td>\n",
              "      <td>-9.942355e-17</td>\n",
              "      <td>6.688523e-18</td>\n",
              "      <td>-3.352594e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.299181</td>\n",
              "      <td>-0.761575</td>\n",
              "      <td>0.338759</td>\n",
              "      <td>-0.389731</td>\n",
              "      <td>0.100875</td>\n",
              "      <td>0.068304</td>\n",
              "      <td>0.073608</td>\n",
              "      <td>0.062760</td>\n",
              "      <td>0.054579</td>\n",
              "      <td>-0.140628</td>\n",
              "      <td>-0.013684</td>\n",
              "      <td>0.046812</td>\n",
              "      <td>0.000621</td>\n",
              "      <td>-0.016003</td>\n",
              "      <td>-0.019317</td>\n",
              "      <td>-0.012399</td>\n",
              "      <td>0.109354</td>\n",
              "      <td>-0.155686</td>\n",
              "      <td>0.014108</td>\n",
              "      <td>-0.013875</td>\n",
              "      <td>-0.064228</td>\n",
              "      <td>-0.026725</td>\n",
              "      <td>0.006572</td>\n",
              "      <td>-0.011019</td>\n",
              "      <td>0.043754</td>\n",
              "      <td>-0.015619</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>-0.012115</td>\n",
              "      <td>-0.017149</td>\n",
              "      <td>0.019228</td>\n",
              "      <td>0.007125</td>\n",
              "      <td>-0.029558</td>\n",
              "      <td>-0.000899</td>\n",
              "      <td>0.010250</td>\n",
              "      <td>0.008090</td>\n",
              "      <td>-0.004170</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>-0.000261</td>\n",
              "      <td>-0.012533</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.007312</td>\n",
              "      <td>-0.011473</td>\n",
              "      <td>0.001507</td>\n",
              "      <td>-0.001886</td>\n",
              "      <td>-0.001944</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>-0.001703</td>\n",
              "      <td>-0.007496</td>\n",
              "      <td>-0.003088</td>\n",
              "      <td>4.663124e-05</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>-1.320668e-04</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-0.000062</td>\n",
              "      <td>9.486313e-05</td>\n",
              "      <td>3.526428e-06</td>\n",
              "      <td>1.461003e-05</td>\n",
              "      <td>-3.933754e-06</td>\n",
              "      <td>-1.011863e-08</td>\n",
              "      <td>3.571507e-09</td>\n",
              "      <td>-2.093373e-14</td>\n",
              "      <td>7.799841e-17</td>\n",
              "      <td>1.823783e-16</td>\n",
              "      <td>-8.297544e-17</td>\n",
              "      <td>-1.747531e-17</td>\n",
              "      <td>-1.492647e-17</td>\n",
              "      <td>3.952009e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.074207</td>\n",
              "      <td>0.640575</td>\n",
              "      <td>-0.000759</td>\n",
              "      <td>-0.186550</td>\n",
              "      <td>-0.441405</td>\n",
              "      <td>-0.073439</td>\n",
              "      <td>0.267851</td>\n",
              "      <td>0.021730</td>\n",
              "      <td>-0.118905</td>\n",
              "      <td>-0.061133</td>\n",
              "      <td>0.016603</td>\n",
              "      <td>0.077731</td>\n",
              "      <td>-0.006447</td>\n",
              "      <td>-0.034424</td>\n",
              "      <td>0.027327</td>\n",
              "      <td>-0.105071</td>\n",
              "      <td>0.007345</td>\n",
              "      <td>0.015605</td>\n",
              "      <td>-0.000101</td>\n",
              "      <td>0.017516</td>\n",
              "      <td>0.006303</td>\n",
              "      <td>0.003147</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>-0.007504</td>\n",
              "      <td>-0.002865</td>\n",
              "      <td>0.005296</td>\n",
              "      <td>-0.001291</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-0.001359</td>\n",
              "      <td>-0.005289</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.003394</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.002063</td>\n",
              "      <td>0.006076</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.000992</td>\n",
              "      <td>-0.001934</td>\n",
              "      <td>0.000952</td>\n",
              "      <td>-0.002008</td>\n",
              "      <td>0.008129</td>\n",
              "      <td>-0.001837</td>\n",
              "      <td>-0.001045</td>\n",
              "      <td>-0.000720</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.000373</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000628</td>\n",
              "      <td>-0.000341</td>\n",
              "      <td>-0.000517</td>\n",
              "      <td>9.237735e-06</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>-0.000537</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>-1.452788e-05</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>-0.000443</td>\n",
              "      <td>6.791579e-06</td>\n",
              "      <td>2.429141e-07</td>\n",
              "      <td>1.007812e-05</td>\n",
              "      <td>-7.329747e-06</td>\n",
              "      <td>-1.887453e-08</td>\n",
              "      <td>5.019608e-08</td>\n",
              "      <td>-5.836653e-16</td>\n",
              "      <td>-1.231283e-16</td>\n",
              "      <td>-1.242618e-16</td>\n",
              "      <td>-4.285920e-17</td>\n",
              "      <td>2.503460e-17</td>\n",
              "      <td>1.256940e-18</td>\n",
              "      <td>4.995642e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.459646</td>\n",
              "      <td>0.053432</td>\n",
              "      <td>-0.681130</td>\n",
              "      <td>-0.091546</td>\n",
              "      <td>0.103363</td>\n",
              "      <td>-0.008800</td>\n",
              "      <td>-0.138656</td>\n",
              "      <td>0.030395</td>\n",
              "      <td>0.003520</td>\n",
              "      <td>0.019821</td>\n",
              "      <td>0.010493</td>\n",
              "      <td>-0.023683</td>\n",
              "      <td>-0.013666</td>\n",
              "      <td>-0.011342</td>\n",
              "      <td>0.012080</td>\n",
              "      <td>0.005375</td>\n",
              "      <td>-0.003049</td>\n",
              "      <td>-0.001539</td>\n",
              "      <td>0.003005</td>\n",
              "      <td>-0.001438</td>\n",
              "      <td>-0.002084</td>\n",
              "      <td>-0.001514</td>\n",
              "      <td>-0.005197</td>\n",
              "      <td>0.000497</td>\n",
              "      <td>-0.001277</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.004747</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.003141</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.000416</td>\n",
              "      <td>0.002036</td>\n",
              "      <td>-0.000091</td>\n",
              "      <td>-0.001589</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>-0.005918</td>\n",
              "      <td>0.002954</td>\n",
              "      <td>-0.000127</td>\n",
              "      <td>-0.001256</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-0.000886</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>-0.000036</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>-0.000145</td>\n",
              "      <td>-0.000682</td>\n",
              "      <td>-9.700769e-06</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.000046</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>1.795497e-06</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>-6.686293e-07</td>\n",
              "      <td>-3.071040e-07</td>\n",
              "      <td>9.905799e-07</td>\n",
              "      <td>-2.398228e-07</td>\n",
              "      <td>1.396052e-08</td>\n",
              "      <td>4.000923e-09</td>\n",
              "      <td>-5.947836e-16</td>\n",
              "      <td>-1.741910e-17</td>\n",
              "      <td>-5.471909e-17</td>\n",
              "      <td>8.711357e-17</td>\n",
              "      <td>-1.000391e-16</td>\n",
              "      <td>1.190240e-17</td>\n",
              "      <td>-4.796719e-17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...            68            69            70\n",
              "0 -0.572368 -0.515194  0.239465  ... -8.961834e-17  8.585935e-19  7.210748e-17\n",
              "1 -0.305131  0.950823  0.467819  ...  1.069630e-16 -5.185134e-18  4.446547e-17\n",
              "2 -0.457527 -0.671902  0.271425  ... -6.230528e-17  5.439341e-20 -1.199222e-17\n",
              "3  0.018007 -0.958150  0.544710  ...  3.982068e-17 -1.982548e-17  7.966276e-18\n",
              "4  2.111311  0.487699 -0.186206  ...  1.095716e-16 -1.659759e-17  1.389950e-17\n",
              "5 -0.566619 -0.561331  0.179699  ... -9.007913e-17  8.382647e-19  7.210748e-17\n",
              "6 -0.464064  0.122514 -0.586668  ... -9.942355e-17  6.688523e-18 -3.352594e-17\n",
              "7 -0.299181 -0.761575  0.338759  ... -1.747531e-17 -1.492647e-17  3.952009e-17\n",
              "8 -0.074207  0.640575 -0.000759  ...  2.503460e-17  1.256940e-18  4.995642e-17\n",
              "9 -0.459646  0.053432 -0.681130  ... -1.000391e-16  1.190240e-17 -4.796719e-17\n",
              "\n",
              "[10 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIDYI_dXhwA"
      },
      "source": [
        "## Training and Making Predictions\n",
        "\n",
        "In this case we'll use random forest classification for making the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swXJ6_ydXhwA"
      },
      "source": [
        "pca = PCA(n_components=12)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Yt7PnkXhwA"
      },
      "source": [
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test_pca)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7X48ybRXhwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881537a6-3da9-4515-a7d7-79dd643cb51b"
      },
      "source": [
        "print('Accuracy:%f' %accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:%f\" %metrics.average_precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:%f\" %metrics.recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-score:%f\" %metrics.f1_score(y_test, y_pred,average='weighted'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.637495\n",
            "Precision:0.654541\n",
            "Recall:0.637495\n",
            "F1-score:0.632099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tfibn9gAXhwB",
        "outputId": "be99767f-51c7-4306-cdb3-c104d7f00ce7"
      },
      "source": [
        "#The confusion matrix takes a vector of labels (not the one-hot encoding). \n",
        "\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[138946      0      0      0      0      0      0      0      0      0\n",
            "     189      0      0      0      0]\n",
            " [   489      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [ 31990      0      0      0      0      0      0      0      0      0\n",
            "      16      0      0      0      0]\n",
            " [  2573      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [ 57531      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [  1373      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]\n",
            " [  1444      0      0      0      0      0      0      0      0      0\n",
            "       5      0      0      0      0]\n",
            " [  1983      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     2      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     9      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [  1667      0      0      0      0      0      0      0      0      0\n",
            "   38034      0      0      0      0]\n",
            " [  1474      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   376      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     5      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   162      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxRNywIXXhwC"
      },
      "source": [
        "Get the attacks' names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9AlIiTKXhwC"
      },
      "source": [
        "labels_d = make_value2index(df_test['Label'])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1vqAhE1XhwC",
        "outputId": "261ed0a8-7418-4664-f1b5-a195359dbd00"
      },
      "source": [
        "print(labels_d)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'BENIGN': 175062, 'Bot': 175554, 'DDoS': 207815, 'DoS GoldenEye': 210351, 'DoS Hulk': 267794, 'DoS Slowhttptest': 269184, 'DoS slowloris': 270672, 'FTP-Patator': 272590, 'Heartbleed': 272594, 'Infiltration': 272607, 'PortScan': 312182, 'SSH-Patator': 313598, 'Web Attack � Brute Force': 313967, 'Web Attack � Sql Injection': 313973, 'Web Attack � XSS': 314138}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO4EmilxXhwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c31171-3e0d-40f1-b2a5-47dbbd0e4e34"
      },
      "source": [
        "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1), target_names=labels_d))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.63      0.97      0.76    175063\n",
            "                       Bot       0.00      0.00      0.00       492\n",
            "                      DDoS       0.00      0.00      0.00     32261\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2536\n",
            "                  DoS Hulk       0.00      0.00      0.00     57443\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1390\n",
            "             DoS slowloris       0.00      0.00      0.00      1488\n",
            "               FTP-Patator       0.00      0.00      0.00      1918\n",
            "                Heartbleed       0.00      0.00      0.00         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.88      0.99      0.93     39575\n",
            "               SSH-Patator       0.00      0.00      0.00      1416\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.00      0.00      0.00       165\n",
            "\n",
            "                  accuracy                           0.67    314139\n",
            "                 macro avg       0.10      0.13      0.11    314139\n",
            "              weighted avg       0.46      0.67      0.54    314139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtGAy_3gY2n"
      },
      "source": [
        "# Model : Naive Bayes model (GaussianNB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxNz3yQch35o"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izw8QWLIgcTx",
        "outputId": "44230a94-7ee4-4f84-ae66-97ceee060d36"
      },
      "source": [
        "model_gaussian = GaussianNB()\r\n",
        "model_gaussian.fit(X_train, y_train_ada)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c15lYNIdgs9V"
      },
      "source": [
        "# make predictions\r\n",
        "y_pred = model_gaussian.predict(X_test)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ6JwTSDg6iE",
        "outputId": "399d289d-f126-49e8-ee24-6754f6eb703e"
      },
      "source": [
        "display_metrics(y_test_ada, y_pred, labels_d)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.56\n",
            "\n",
            "Micro Precision: 0.56\n",
            "Micro Recall: 0.56\n",
            "Micro F1-score: 0.56\n",
            "\n",
            "Macro Precision: 0.04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Recall: 0.07\n",
            "Macro F1-score: 0.05\n",
            "\n",
            "Weighted Precision: 0.31\n",
            "Weighted Recall: 0.56\n",
            "Weighted F1-score: 0.40\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.56      1.00      0.72    175063\n",
            "                       Bot       0.00      0.00      0.00       492\n",
            "                      DDoS       0.00      0.00      0.00     32261\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2536\n",
            "                  DoS Hulk       0.00      0.00      0.00     57443\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1390\n",
            "             DoS slowloris       0.00      0.00      0.00      1488\n",
            "               FTP-Patator       0.00      0.00      0.00      1918\n",
            "                Heartbleed       0.00      0.00      0.00         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.00      0.00      0.00     39575\n",
            "               SSH-Patator       0.00      0.00      0.00      1416\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.00      0.00      0.00       165\n",
            "\n",
            "                  accuracy                           0.56    314139\n",
            "                 macro avg       0.04      0.07      0.05    314139\n",
            "              weighted avg       0.31      0.56      0.40    314139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXea3TMVXhwD"
      },
      "source": [
        "# Model 3: Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOUaeJwiXhwD"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRMY85VyXhwE"
      },
      "source": [
        "model_dec = DecisionTreeClassifier()\n",
        "model_dec.fit(X_train, y_train_ada)\n",
        "y_pred = model_dec.predict(X_test)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFQ6iCJjXhwE",
        "outputId": "73aae469-3102-47c2-a37c-78d4e931e00e"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\n",
        " \n",
        "print('\\nClassification Report\\n')\n",
        "\n",
        "print(classification_report(y_test_ada , y_pred, target_names=labels_d))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.94\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.94\n",
            "\n",
            "Macro Precision: 0.83\n",
            "Macro Recall: 0.76\n",
            "Macro F1-score: 0.76\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.94\n",
            "\n",
            "Classification Report\n",
            "\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.92      1.00      0.96    175063\n",
            "                       Bot       0.93      0.98      0.95       492\n",
            "                      DDoS       0.99      0.64      0.78     32261\n",
            "             DoS GoldenEye       0.85      0.69      0.76      2536\n",
            "                  DoS Hulk       1.00      0.93      0.96     57443\n",
            "          DoS Slowhttptest       0.71      0.93      0.80      1390\n",
            "             DoS slowloris       0.50      0.50      0.50      1488\n",
            "               FTP-Patator       1.00      0.50      0.66      1918\n",
            "                Heartbleed       1.00      1.00      1.00         4\n",
            "              Infiltration       1.00      0.46      0.63        13\n",
            "                  PortScan       1.00      1.00      1.00     39575\n",
            "               SSH-Patator       1.00      1.00      1.00      1416\n",
            "  Web Attack � Brute Force       0.15      0.80      0.25       369\n",
            "Web Attack � Sql Injection       0.67      0.33      0.44         6\n",
            "          Web Attack � XSS       0.75      0.67      0.71       165\n",
            "\n",
            "                  accuracy                           0.94    314139\n",
            "                 macro avg       0.83      0.76      0.76    314139\n",
            "              weighted avg       0.95      0.94      0.94    314139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABYQjuBCXhwE"
      },
      "source": [
        "# Model 4: Random Foresty with DecisionTree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI6QGw9LXhwF"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "clf.fit(X_train,y_train)\n",
        "    \n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFh6H0d3lNmz",
        "outputId": "38350d81-1a18-48c3-a9c2-a9f9a4980ed3"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, np.argmax(y_pred, axis = 1))))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')\r\n",
        "\r\n",
        "print(classification_report(y_test_ada ,np.argmax(y_pred, axis = 1), target_names=labels_d))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.91\n",
            "\n",
            "Micro Precision: 0.91\n",
            "Micro Recall: 0.91\n",
            "Micro F1-score: 0.91\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.91\n",
            "Macro Recall: 0.66\n",
            "Macro F1-score: 0.74\n",
            "\n",
            "Weighted Precision: 0.92\n",
            "Weighted Recall: 0.91\n",
            "Weighted F1-score: 0.90\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.86      1.00      0.93    175063\n",
            "                       Bot       0.94      0.55      0.69       492\n",
            "                      DDoS       1.00      0.99      1.00     32261\n",
            "             DoS GoldenEye       1.00      0.45      0.62      2536\n",
            "                  DoS Hulk       1.00      0.90      0.95     57443\n",
            "          DoS Slowhttptest       0.99      0.98      0.99      1390\n",
            "             DoS slowloris       1.00      0.96      0.98      1488\n",
            "               FTP-Patator       1.00      1.00      1.00      1918\n",
            "                Heartbleed       1.00      0.75      0.86         4\n",
            "              Infiltration       1.00      0.46      0.63        13\n",
            "                  PortScan       1.00      0.51      0.68     39575\n",
            "               SSH-Patator       1.00      0.55      0.71      1416\n",
            "  Web Attack � Brute Force       0.94      0.60      0.73       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.85      0.28      0.43       165\n",
            "\n",
            "                  accuracy                           0.91    314139\n",
            "                 macro avg       0.91      0.66      0.74    314139\n",
            "              weighted avg       0.92      0.91      0.90    314139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xi2INVUk9p9"
      },
      "source": [
        "# Model 5: Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_AoQZpOnxuD"
      },
      "source": [
        "from sklearn import linear_model"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqJDnChnmxJ"
      },
      "source": [
        "attack_classifier = linear_model.LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=10000)\r\n",
        "attack_classifier.fit(X_train, y_train_ada)\r\n",
        "\r\n",
        "y_pred = attack_classifier.predict(X_test)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3x76HdDorw7",
        "outputId": "2983b825-e3ee-495d-f5ca-390b1530dc01"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')\r\n",
        "\r\n",
        "print(classification_report(y_test_ada , y_pred, target_names=labels_d))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n",
            "Macro Precision: 0.40\n",
            "Macro Recall: 0.81\n",
            "Macro F1-score: 0.46\n",
            "\n",
            "Weighted Precision: 0.93\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.85\n",
            "\n",
            "Classification Report\n",
            "\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       1.00      0.70      0.82    175063\n",
            "                       Bot       0.03      0.73      0.05       492\n",
            "                      DDoS       0.71      0.99      0.83     32261\n",
            "             DoS GoldenEye       0.63      0.96      0.76      2536\n",
            "                  DoS Hulk       0.94      0.89      0.91     57443\n",
            "          DoS Slowhttptest       0.41      0.99      0.58      1390\n",
            "             DoS slowloris       0.21      0.94      0.35      1488\n",
            "               FTP-Patator       0.57      0.94      0.71      1918\n",
            "                Heartbleed       0.38      0.75      0.50         4\n",
            "              Infiltration       0.00      0.77      0.01        13\n",
            "                  PortScan       0.89      0.99      0.94     39575\n",
            "               SSH-Patator       0.22      0.99      0.36      1416\n",
            "  Web Attack � Brute Force       0.06      0.18      0.09       369\n",
            "Web Attack � Sql Injection       0.00      0.67      0.00         6\n",
            "          Web Attack � XSS       0.02      0.70      0.04       165\n",
            "\n",
            "                  accuracy                           0.81    314139\n",
            "                 macro avg       0.40      0.81      0.46    314139\n",
            "              weighted avg       0.93      0.81      0.85    314139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXJKq1zXhwF"
      },
      "source": [
        "# Model 6: AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqqrLskIXhwF"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT8egNOEXhwG"
      },
      "source": [
        "model_ada = AdaBoostClassifier(n_estimators=100)\n",
        "model_ada.fit(X_train, y_train_ada)\n",
        "\n",
        "# make predictions\n",
        "expected = y_test_ada\n",
        "predicted = model_ada.predict(X_test)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJCcrCS4XhwG",
        "outputId": "26d7c797-39a0-4fcb-af9f-fabf5f8543fe"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\n",
        " \n",
        "print('\\nClassification Report\\n')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n",
            "Macro Precision: 0.40\n",
            "Macro Recall: 0.81\n",
            "Macro F1-score: 0.46\n",
            "\n",
            "Weighted Precision: 0.93\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.85\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_lvcZVXXhwG",
        "outputId": "1b7a5046-5439-4d0c-8924-b0be82605ca4"
      },
      "source": [
        "print(classification_report(y_test_ada, predicted, target_names=labels_d))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.70      0.99      0.82    175063\n",
            "                       Bot       0.00      0.00      0.00       492\n",
            "                      DDoS       0.00      0.00      0.00     32261\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2536\n",
            "                  DoS Hulk       0.63      0.67      0.65     57443\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1390\n",
            "             DoS slowloris       0.00      0.00      0.00      1488\n",
            "               FTP-Patator       0.36      1.00      0.53      1918\n",
            "                Heartbleed       0.00      0.00      0.00         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.00      0.00      0.00     39575\n",
            "               SSH-Patator       0.00      0.00      0.00      1416\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.00      0.00      0.00       165\n",
            "\n",
            "                  accuracy                           0.68    314139\n",
            "                 macro avg       0.11      0.18      0.13    314139\n",
            "              weighted avg       0.51      0.68      0.58    314139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn7bQX_Gvusc"
      },
      "source": [
        "# Model 6: XGBClassifier (very slow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ttK187BvuLn"
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "53QHIzuAv5z9",
        "outputId": "0e392ce3-2300-4f4f-a1e0-44bdc7723ac0"
      },
      "source": [
        "xgboostc = XGBClassifier(learning_rate = 0.1, max_depth = 5,n_estimators = 1165, subsample=0.8,colsample_bytree=0.8,seed=27)\r\n",
        "xgboostc.fit(X_train,y_train)\r\n",
        "    \r\n",
        "y_pred = xgboostc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-07eedb02b743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgboostc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1165\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgboostc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboostc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz8IN3Jfwav0"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KznBGxa2yiLk"
      },
      "source": [
        "# Model 7: Voting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfa2lmvytnh"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIYUdP7iyuRG",
        "outputId": "24183e0f-1f98-42ed-c028-4897d2361b57"
      },
      "source": [
        "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=35, criterion=\"entropy\")\r\n",
        "ada = AdaBoostClassifier(n_estimators=75, learning_rate=1.5)\r\n",
        "etc = ExtraTreesClassifier(n_jobs=-1, criterion=\"entropy\", n_estimators=5)\r\n",
        "eclf = VotingClassifier(estimators=[('ada', ada), ('rfc', rfc), ('etc', etc)], voting='soft', weights=[2, 1, 3],n_jobs=1)\r\n",
        "eclf.fit(X_train,y_train_ada)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('ada',\n",
              "                              AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                                                 base_estimator=None,\n",
              "                                                 learning_rate=1.5,\n",
              "                                                 n_estimators=75,\n",
              "                                                 random_state=None)),\n",
              "                             ('rfc',\n",
              "                              RandomForestClassifier(bootstrap=True,\n",
              "                                                     ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     criterion='entropy',\n",
              "                                                     max_depth=None,\n",
              "                                                     max_features='auto',\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     max_samples=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_im...\n",
              "                                                   criterion='entropy',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   max_samples=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=5, n_jobs=-1,\n",
              "                                                   oob_score=False,\n",
              "                                                   random_state=None, verbose=0,\n",
              "                                                   warm_start=False))],\n",
              "                 flatten_transform=True, n_jobs=1, voting='soft',\n",
              "                 weights=[2, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rig3c_bqzpim"
      },
      "source": [
        "y_pred = eclf.predict(X_test)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuI-C6lZzqeV",
        "outputId": "81aa726e-37ce-4a95-db4a-82b6af662b53"
      },
      "source": [
        "display_metrics(y_test_ada, y_pred,labels_d)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.99\n",
            "\n",
            "Micro Precision: 0.99\n",
            "Micro Recall: 0.99\n",
            "Micro F1-score: 0.99\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.93\n",
            "Macro Recall: 0.72\n",
            "Macro F1-score: 0.75\n",
            "\n",
            "Weighted Precision: 0.99\n",
            "Weighted Recall: 0.99\n",
            "Weighted F1-score: 0.99\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.99      1.00      0.99    175063\n",
            "                       Bot       0.97      0.90      0.93       492\n",
            "                      DDoS       1.00      1.00      1.00     32261\n",
            "             DoS GoldenEye       1.00      0.93      0.96      2536\n",
            "                  DoS Hulk       1.00      1.00      1.00     57443\n",
            "          DoS Slowhttptest       0.99      0.96      0.98      1390\n",
            "             DoS slowloris       1.00      0.98      0.99      1488\n",
            "               FTP-Patator       1.00      0.49      0.66      1918\n",
            "                Heartbleed       1.00      1.00      1.00         4\n",
            "              Infiltration       1.00      0.46      0.63        13\n",
            "                  PortScan       1.00      1.00      1.00     39575\n",
            "               SSH-Patator       1.00      0.95      0.97      1416\n",
            "  Web Attack � Brute Force       0.96      0.06      0.12       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       1.00      0.02      0.04       165\n",
            "\n",
            "                  accuracy                           0.99    314139\n",
            "                 macro avg       0.93      0.72      0.75    314139\n",
            "              weighted avg       0.99      0.99      0.99    314139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAuDnGKYXhwH"
      },
      "source": [
        "# Model 8: KNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7fba3ZuXhwH"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgx8g7vuXhwH"
      },
      "source": [
        "features_order = ['Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min']"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyhoOfFgXhwH"
      },
      "source": [
        "features=[\"Fwd Packet Length Max\",\"Flow IAT Std\",\"Fwd Packet Length Std\" ,\"Fwd IAT Total\",'Flow Packets/s', \"Fwd Packet Length Mean\",  \"Flow Bytes/s\",  \"Flow IAT Mean\", \"Bwd Packet Length Mean\",  \"Flow IAT Max\", \"Bwd Packet Length Std\", ]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "GlhN1BDdXhwI",
        "outputId": "b84bfaeb-f2a8-418f-f9f4-8e40055569f7"
      },
      "source": [
        "df_knn_train = pd.DataFrame(X_train, columns = features_order)\n",
        "df_knn_train.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.586618</td>\n",
              "      <td>0.015794</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.583333e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>8.235825e-07</td>\n",
              "      <td>9.529852e-09</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000872</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.002072</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.410256</td>\n",
              "      <td>3.583333e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.583333e-07</td>\n",
              "      <td>4.333333e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>8.547009e-03</td>\n",
              "      <td>1.282051e-02</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>2.380952e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.235825e-07</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>9.529852e-09</td>\n",
              "      <td>0.015640</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.779928</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.950000e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400851</td>\n",
              "      <td>3.950000e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.950000e-06</td>\n",
              "      <td>4.025000e-06</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>4.016666e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>1.418440e-03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.217133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.782544</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.010963e-01</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>8.268768e-04</td>\n",
              "      <td>4.359907e-06</td>\n",
              "      <td>0.025786</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016848</td>\n",
              "      <td>0.019654</td>\n",
              "      <td>0.056169</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019626</td>\n",
              "      <td>0.032795</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400001</td>\n",
              "      <td>1.944192e-03</td>\n",
              "      <td>0.007468</td>\n",
              "      <td>1.951187e-02</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.010538</td>\n",
              "      <td>0.020014</td>\n",
              "      <td>2.733333e-06</td>\n",
              "      <td>0.100833</td>\n",
              "      <td>0.003261</td>\n",
              "      <td>0.009313</td>\n",
              "      <td>0.019512</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995805</td>\n",
              "      <td>0.994741</td>\n",
              "      <td>5.770077e-07</td>\n",
              "      <td>1.318875e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039323</td>\n",
              "      <td>0.030143</td>\n",
              "      <td>0.040070</td>\n",
              "      <td>1.604664e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.026873</td>\n",
              "      <td>0.016848</td>\n",
              "      <td>0.019626</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>8.268768e-04</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>4.359907e-06</td>\n",
              "      <td>0.445572</td>\n",
              "      <td>0.003784</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.839857</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>6.125458e-03</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>1.070657e-05</td>\n",
              "      <td>1.843550e-05</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>0.001440</td>\n",
              "      <td>0.583276</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.531129</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.400002</td>\n",
              "      <td>8.750940e-04</td>\n",
              "      <td>0.003271</td>\n",
              "      <td>6.117325e-03</td>\n",
              "      <td>1.500000e-07</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>6.749999e-07</td>\n",
              "      <td>0.006125</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.004346</td>\n",
              "      <td>0.006117</td>\n",
              "      <td>4.166667e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>1.360450e-06</td>\n",
              "      <td>3.401125e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.408340</td>\n",
              "      <td>0.442656</td>\n",
              "      <td>0.708152</td>\n",
              "      <td>5.000000e-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.435740</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>0.531129</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>1.070657e-05</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>1.843550e-05</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.823682</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>8.750761e-01</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>6.588660e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.003489</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>2.916667e-01</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>4.583333e-07</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995804</td>\n",
              "      <td>0.994740</td>\n",
              "      <td>6.348654e-09</td>\n",
              "      <td>9.522981e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.001644</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>8.571429e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>6.588660e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.117647e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.117647e-07</td>\n",
              "      <td>4.117647e-07</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Source Port  Destination Port  Protocol  ...  Idle Std  Idle Max  Idle Min\n",
              "0     0.586618          0.015794  0.352941  ...       0.0     0.000     0.000\n",
              "1     0.006760          0.779928  0.352941  ...       0.0     0.000     0.000\n",
              "2     0.782544          0.000336  0.352941  ...       0.0     0.000     0.000\n",
              "3     0.839857          0.001221  0.352941  ...       0.0     0.000     0.000\n",
              "4     0.823682          0.001221  0.352941  ...       0.0     0.875     0.875\n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eda-FM60XhwI"
      },
      "source": [
        "df_knn_test = pd.DataFrame(X_test, columns = features_order)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "iv6nljzO5_Wd",
        "outputId": "739936a4-89e8-4aca-f0dd-b9cec79e354c"
      },
      "source": [
        "df_knn_test.head()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.942947</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.141668e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>2.093430e-05</td>\n",
              "      <td>6.055046e-07</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>0.015129</td>\n",
              "      <td>0.005050</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007597</td>\n",
              "      <td>0.077556</td>\n",
              "      <td>0.033977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006396</td>\n",
              "      <td>0.336054</td>\n",
              "      <td>7.805555e-07</td>\n",
              "      <td>6.876438e-07</td>\n",
              "      <td>1.341667e-06</td>\n",
              "      <td>5.083333e-07</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>4.666666e-07</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.964949</td>\n",
              "      <td>0.955302</td>\n",
              "      <td>2.721088e-03</td>\n",
              "      <td>4.081633e-03</td>\n",
              "      <td>0.020718</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>0.029096</td>\n",
              "      <td>0.011808</td>\n",
              "      <td>1.393393e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.034854</td>\n",
              "      <td>0.005050</td>\n",
              "      <td>0.033977</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>2.093430e-05</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>6.058538e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.843723</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.166669e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.346154</td>\n",
              "      <td>3.166666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.166666e-07</td>\n",
              "      <td>3.250000e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.666666e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.964948</td>\n",
              "      <td>0.955302</td>\n",
              "      <td>1.282051e-02</td>\n",
              "      <td>1.923077e-02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>0.006912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.947631</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.577585e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.639853e-05</td>\n",
              "      <td>1.811927e-07</td>\n",
              "      <td>0.002012</td>\n",
              "      <td>0.023701</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004547</td>\n",
              "      <td>0.046416</td>\n",
              "      <td>0.020335</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005763</td>\n",
              "      <td>0.333344</td>\n",
              "      <td>2.577583e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.577583e-04</td>\n",
              "      <td>2.577666e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.666666e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.964948</td>\n",
              "      <td>0.955302</td>\n",
              "      <td>1.078086e-05</td>\n",
              "      <td>1.617129e-05</td>\n",
              "      <td>0.032459</td>\n",
              "      <td>0.003382</td>\n",
              "      <td>0.023699</td>\n",
              "      <td>0.003905</td>\n",
              "      <td>1.523810e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.034067</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.020335</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.639853e-05</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.812972e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.072313</td>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>5.078489e-01</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>2.867999e-04</td>\n",
              "      <td>1.197936e-05</td>\n",
              "      <td>0.018664</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009224</td>\n",
              "      <td>0.016911</td>\n",
              "      <td>0.125173</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.103418</td>\n",
              "      <td>0.113723</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.880930e-02</td>\n",
              "      <td>4.951908e-02</td>\n",
              "      <td>8.333342e-02</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>5.075000e-01</td>\n",
              "      <td>3.627489e-02</td>\n",
              "      <td>0.061537</td>\n",
              "      <td>8.416667e-02</td>\n",
              "      <td>1.608333e-06</td>\n",
              "      <td>5.066667e-01</td>\n",
              "      <td>4.225104e-02</td>\n",
              "      <td>0.063159</td>\n",
              "      <td>8.416667e-02</td>\n",
              "      <td>1.816667e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.964952</td>\n",
              "      <td>0.955305</td>\n",
              "      <td>8.204547e-08</td>\n",
              "      <td>1.066591e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093108</td>\n",
              "      <td>0.085664</td>\n",
              "      <td>0.113547</td>\n",
              "      <td>1.288557e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085026</td>\n",
              "      <td>0.009224</td>\n",
              "      <td>0.103418</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>2.867999e-04</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>1.198627e-05</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.001755</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.001716</td>\n",
              "      <td>0.002756</td>\n",
              "      <td>0.005617</td>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.083046</td>\n",
              "      <td>0.001052</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.081671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.670771</td>\n",
              "      <td>0.088508</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>5.333337e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>6.978099e-07</td>\n",
              "      <td>1.376147e-08</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.001009</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.003525</td>\n",
              "      <td>0.001544</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005835</td>\n",
              "      <td>0.339744</td>\n",
              "      <td>5.333333e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.333333e-07</td>\n",
              "      <td>5.416666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.666666e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.964948</td>\n",
              "      <td>0.955302</td>\n",
              "      <td>6.410256e-03</td>\n",
              "      <td>9.615385e-03</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.001370</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>2.380952e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.001969</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.001544</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.978099e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.376941e-08</td>\n",
              "      <td>0.015640</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Source Port  Destination Port  Protocol  ...  Idle Std  Idle Max  Idle Min\n",
              "0     0.942947          0.000809  1.000000  ...  0.000000  0.000000  0.000000\n",
              "1     0.006760          0.843723  0.352941  ...  0.000000  0.000000  0.000000\n",
              "2     0.947631          0.000809  1.000000  ...  0.000000  0.000000  0.000000\n",
              "3     0.072313          0.006760  0.352941  ...  0.001052  0.083333  0.081671\n",
              "4     0.670771          0.088508  0.352941  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T8Abk2XhwI"
      },
      "source": [
        "Select a subset of features from the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "LFABov84XhwK",
        "outputId": "d22f4300-46d1-4450-c9b1-3cdf3db2caf3"
      },
      "source": [
        "df_knn_sub=df_knn_train.loc[:, features]\n",
        "df_knn_sub.head()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.410256</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>3.583333e-07</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>3.583333e-07</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.400851</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>3.950000e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.950000e-06</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.025786</td>\n",
              "      <td>0.007468</td>\n",
              "      <td>0.019654</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.400001</td>\n",
              "      <td>0.016848</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>1.944192e-03</td>\n",
              "      <td>0.019626</td>\n",
              "      <td>1.951187e-02</td>\n",
              "      <td>0.032795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.003271</td>\n",
              "      <td>0.001440</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.400002</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>8.750940e-04</td>\n",
              "      <td>0.531129</td>\n",
              "      <td>6.117325e-03</td>\n",
              "      <td>0.657143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>2.916667e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.750000e-01</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Fwd Packet Length Max  Flow IAT Std  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0               0.000081      0.000000  ...  3.583333e-07               0.000000\n",
              "1               0.000000      0.000000  ...  3.950000e-06               0.000000\n",
              "2               0.025786      0.007468  ...  1.951187e-02               0.032795\n",
              "3               0.000806      0.003271  ...  6.117325e-03               0.657143\n",
              "4               0.000322      0.714623  ...  8.750000e-01               0.000000\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "zhVGlgN5XhwK",
        "outputId": "cd1b0954-b96a-4437-cdf1-7c2c0c3b86ed"
      },
      "source": [
        "df_knn_test_sub=df_knn_test.loc[:, features]\n",
        "df_knn_test_sub.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001284</td>\n",
              "      <td>6.876438e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.336054</td>\n",
              "      <td>0.005050</td>\n",
              "      <td>0.006396</td>\n",
              "      <td>7.805555e-07</td>\n",
              "      <td>0.033977</td>\n",
              "      <td>1.341667e-06</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.346154</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>3.166666e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.166666e-07</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.002012</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.333344</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.005763</td>\n",
              "      <td>2.577583e-04</td>\n",
              "      <td>0.020335</td>\n",
              "      <td>2.577583e-04</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.018664</td>\n",
              "      <td>4.951908e-02</td>\n",
              "      <td>0.016911</td>\n",
              "      <td>5.075000e-01</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.009224</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>1.880930e-02</td>\n",
              "      <td>0.103418</td>\n",
              "      <td>8.333342e-02</td>\n",
              "      <td>0.113723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.339744</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.005835</td>\n",
              "      <td>5.333333e-07</td>\n",
              "      <td>0.001544</td>\n",
              "      <td>5.333333e-07</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Fwd Packet Length Max  Flow IAT Std  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0               0.001284  6.876438e-07  ...  1.341667e-06               0.000000\n",
              "1               0.000000  0.000000e+00  ...  3.166666e-07               0.000000\n",
              "2               0.002012  0.000000e+00  ...  2.577583e-04               0.000000\n",
              "3               0.018664  4.951908e-02  ...  8.333342e-02               0.113723\n",
              "4               0.000086  0.000000e+00  ...  5.333333e-07               0.000000\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59G_b79XhwK"
      },
      "source": [
        "Convert dataframes to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJHMos8rXhwK"
      },
      "source": [
        "X_train_knn = df_knn_sub.to_numpy()\n",
        "X_test_knn = df_knn_test_sub.to_numpy()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hAzqyPsXhwK",
        "outputId": "5113d089-d4ae-4c1c-d463-89e47b3465e8"
      },
      "source": [
        "for i in range(5,X_train_knn.shape[1]+1):\n",
        "    knn=KNeighborsClassifier(n_neighbors=i)\n",
        "    model_knn=knn.fit(X_train_knn,y_train)\n",
        "    y_pred=model_knn.predict(X_test_knn)\n",
        "    print(\"for \" , i,  \" as K, accuracy is : \", accuracy_score(y_test, y_pred))\n",
        "    display_metrics(y_test, y_pred, labels_d)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for  5  as K, accuracy is :  0.8095556425658705\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.84\n",
            "Macro Recall: 0.58\n",
            "Macro F1-score: 0.63\n",
            "\n",
            "Weighted Precision: 0.84\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.76\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.75      1.00      0.85    175063\n",
            "                       Bot       0.87      0.43      0.58       492\n",
            "                      DDoS       0.99      0.97      0.98     32261\n",
            "             DoS GoldenEye       0.95      0.83      0.89      2536\n",
            "                  DoS Hulk       0.99      0.72      0.83     57443\n",
            "          DoS Slowhttptest       0.97      0.92      0.95      1390\n",
            "             DoS slowloris       0.98      0.90      0.94      1488\n",
            "               FTP-Patator       1.00      0.55      0.71      1918\n",
            "                Heartbleed       1.00      0.75      0.86         4\n",
            "              Infiltration       1.00      0.08      0.14        13\n",
            "                  PortScan       0.86      0.01      0.01     39575\n",
            "               SSH-Patator       1.00      0.51      0.67      1416\n",
            "  Web Attack � Brute Force       0.71      0.74      0.72       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.48      0.28      0.36       165\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    314139\n",
            "                 macro avg       0.84      0.58      0.63    314139\n",
            "              weighted avg       0.84      0.81      0.76    314139\n",
            "               samples avg       0.81      0.81      0.81    314139\n",
            "\n",
            "for  6  as K, accuracy is :  0.808549718436743\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.77\n",
            "Macro Recall: 0.55\n",
            "Macro F1-score: 0.61\n",
            "\n",
            "Weighted Precision: 0.84\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.76\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.75      1.00      0.85    175063\n",
            "                       Bot       0.89      0.41      0.57       492\n",
            "                      DDoS       0.99      0.96      0.98     32261\n",
            "             DoS GoldenEye       0.95      0.82      0.88      2536\n",
            "                  DoS Hulk       0.99      0.72      0.84     57443\n",
            "          DoS Slowhttptest       0.98      0.92      0.95      1390\n",
            "             DoS slowloris       0.98      0.90      0.94      1488\n",
            "               FTP-Patator       1.00      0.51      0.67      1918\n",
            "                Heartbleed       1.00      0.75      0.86         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.87      0.01      0.01     39575\n",
            "               SSH-Patator       1.00      0.51      0.67      1416\n",
            "  Web Attack � Brute Force       0.72      0.65      0.69       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.48      0.15      0.22       165\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    314139\n",
            "                 macro avg       0.77      0.55      0.61    314139\n",
            "              weighted avg       0.84      0.81      0.76    314139\n",
            "               samples avg       0.81      0.81      0.81    314139\n",
            "\n",
            "for  7  as K, accuracy is :  0.8094092105723899\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.76\n",
            "Macro Recall: 0.57\n",
            "Macro F1-score: 0.62\n",
            "\n",
            "Weighted Precision: 0.84\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.76\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.75      1.00      0.85    175063\n",
            "                       Bot       0.86      0.46      0.60       492\n",
            "                      DDoS       0.99      0.97      0.98     32261\n",
            "             DoS GoldenEye       0.95      0.84      0.89      2536\n",
            "                  DoS Hulk       0.99      0.72      0.84     57443\n",
            "          DoS Slowhttptest       0.97      0.92      0.95      1390\n",
            "             DoS slowloris       0.98      0.90      0.94      1488\n",
            "               FTP-Patator       1.00      0.51      0.67      1918\n",
            "                Heartbleed       1.00      0.75      0.86         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.87      0.01      0.02     39575\n",
            "               SSH-Patator       1.00      0.51      0.67      1416\n",
            "  Web Attack � Brute Force       0.68      0.72      0.70       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.42      0.24      0.30       165\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    314139\n",
            "                 macro avg       0.76      0.57      0.62    314139\n",
            "              weighted avg       0.84      0.81      0.76    314139\n",
            "               samples avg       0.81      0.81      0.81    314139\n",
            "\n",
            "for  8  as K, accuracy is :  0.8080117400259121\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.77\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.61\n",
            "\n",
            "Weighted Precision: 0.84\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.76\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.75      0.99      0.85    175063\n",
            "                       Bot       0.88      0.46      0.60       492\n",
            "                      DDoS       0.99      0.96      0.98     32261\n",
            "             DoS GoldenEye       0.95      0.81      0.87      2536\n",
            "                  DoS Hulk       0.99      0.72      0.83     57443\n",
            "          DoS Slowhttptest       0.97      0.92      0.95      1390\n",
            "             DoS slowloris       0.98      0.89      0.93      1488\n",
            "               FTP-Patator       1.00      0.51      0.67      1918\n",
            "                Heartbleed       1.00      0.75      0.86         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.89      0.01      0.02     39575\n",
            "               SSH-Patator       1.00      0.51      0.67      1416\n",
            "  Web Attack � Brute Force       0.69      0.68      0.69       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.44      0.14      0.21       165\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    314139\n",
            "                 macro avg       0.77      0.56      0.61    314139\n",
            "              weighted avg       0.84      0.81      0.76    314139\n",
            "               samples avg       0.81      0.81      0.81    314139\n",
            "\n",
            "for  9  as K, accuracy is :  0.8100395048051977\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.76\n",
            "Macro Recall: 0.57\n",
            "Macro F1-score: 0.62\n",
            "\n",
            "Weighted Precision: 0.84\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.76\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.75      0.99      0.85    175063\n",
            "                       Bot       0.85      0.49      0.62       492\n",
            "                      DDoS       0.99      0.98      0.99     32261\n",
            "             DoS GoldenEye       0.94      0.83      0.88      2536\n",
            "                  DoS Hulk       0.98      0.72      0.83     57443\n",
            "          DoS Slowhttptest       0.97      0.92      0.94      1390\n",
            "             DoS slowloris       0.98      0.89      0.93      1488\n",
            "               FTP-Patator       1.00      0.51      0.67      1918\n",
            "                Heartbleed       1.00      0.75      0.86         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.89      0.01      0.02     39575\n",
            "               SSH-Patator       1.00      0.51      0.67      1416\n",
            "  Web Attack � Brute Force       0.68      0.74      0.71       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.43      0.22      0.29       165\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    314139\n",
            "                 macro avg       0.76      0.57      0.62    314139\n",
            "              weighted avg       0.84      0.81      0.76    314139\n",
            "               samples avg       0.81      0.81      0.81    314139\n",
            "\n",
            "for  10  as K, accuracy is :  0.8094028439639778\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.70\n",
            "Macro Recall: 0.51\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.84\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.76\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.75      0.99      0.85    175063\n",
            "                       Bot       0.86      0.47      0.61       492\n",
            "                      DDoS       0.99      0.98      0.99     32261\n",
            "             DoS GoldenEye       0.95      0.82      0.88      2536\n",
            "                  DoS Hulk       0.98      0.72      0.83     57443\n",
            "          DoS Slowhttptest       0.97      0.92      0.94      1390\n",
            "             DoS slowloris       0.98      0.89      0.93      1488\n",
            "               FTP-Patator       1.00      0.51      0.67      1918\n",
            "                Heartbleed       0.00      0.00      0.00         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.89      0.01      0.02     39575\n",
            "               SSH-Patator       1.00      0.51      0.67      1416\n",
            "  Web Attack � Brute Force       0.69      0.71      0.70       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.45      0.15      0.23       165\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    314139\n",
            "                 macro avg       0.70      0.51      0.55    314139\n",
            "              weighted avg       0.84      0.81      0.76    314139\n",
            "               samples avg       0.81      0.81      0.81    314139\n",
            "\n",
            "for  11  as K, accuracy is :  0.8098803395948927\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.70\n",
            "Macro Recall: 0.52\n",
            "Macro F1-score: 0.56\n",
            "\n",
            "Weighted Precision: 0.84\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.76\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.75      0.99      0.85    175063\n",
            "                       Bot       0.84      0.49      0.62       492\n",
            "                      DDoS       0.99      0.98      0.99     32261\n",
            "             DoS GoldenEye       0.94      0.83      0.88      2536\n",
            "                  DoS Hulk       0.98      0.72      0.83     57443\n",
            "          DoS Slowhttptest       0.97      0.92      0.94      1390\n",
            "             DoS slowloris       0.97      0.89      0.93      1488\n",
            "               FTP-Patator       1.00      0.51      0.67      1918\n",
            "                Heartbleed       0.00      0.00      0.00         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.89      0.01      0.02     39575\n",
            "               SSH-Patator       1.00      0.51      0.67      1416\n",
            "  Web Attack � Brute Force       0.68      0.76      0.72       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.45      0.19      0.27       165\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    314139\n",
            "                 macro avg       0.70      0.52      0.56    314139\n",
            "              weighted avg       0.84      0.81      0.76    314139\n",
            "               samples avg       0.81      0.81      0.81    314139\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQvcVlbmXhwL"
      },
      "source": [
        "# Model 9: DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tL-xBt_XhwL"
      },
      "source": [
        "def make_model(metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          64, activation='relu',\n",
        "          input_shape=(X_train.shape[-1],)),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(64, activation ='relu'),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(64, activation ='relu'),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(64, activation ='relu'),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(64, activation ='relu'),\n",
        "      tf.keras.layers.Dense(y_train.shape[-1], activation='softmax',\n",
        "                         bias_initializer=output_bias),\n",
        "  ])\n",
        " \n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAzpGMEoXhwL"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 7000\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWmjTb9kXhwL",
        "outputId": "6abdc222-7075-4b24-f407-0cf763f9f7d7"
      },
      "source": [
        "model_dnn = make_model()\n",
        "model_dnn.summary()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_33 (Dense)             (None, 64)                4608      \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 15)                975       \n",
            "=================================================================\n",
            "Total params: 22,223\n",
            "Trainable params: 22,223\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajWocDr4_30a"
      },
      "source": [
        "If loading the validation dataset has an issue, please load the csv files again, and encode it again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeRlOQj6XhwL",
        "outputId": "9c18a686-9fb2-4585-e9c0-decbb37e37e3"
      },
      "source": [
        "baseline_history = model_dnn.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(X_val, y_val))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "101/101 [==============================] - 5s 30ms/step - loss: 0.3988 - tp: 392539.1176 - fp: 91456.4510 - tn: 9352477.1569 - fn: 282027.5686 - accuracy: 0.9655 - precision: 0.8342 - recall: 0.6016 - auc: 0.9398 - val_loss: 0.0842 - val_tp: 158578.0000 - val_fp: 52496.0000 - val_tn: 3245974.0000 - val_fn: 77027.0000 - val_accuracy: 0.9634 - val_precision: 0.7513 - val_recall: 0.6731 - val_auc: 0.9740\n",
            "Epoch 2/100\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.0816 - tp: 250933.5686 - fp: 71463.9412 - tn: 4974523.6667 - fn: 109494.1176 - accuracy: 0.9648 - precision: 0.7641 - recall: 0.6835 - auc: 0.9742 - val_loss: 0.0455 - val_tp: 198983.0000 - val_fp: 19679.0000 - val_tn: 3278791.0000 - val_fn: 36622.0000 - val_accuracy: 0.9841 - val_precision: 0.9100 - val_recall: 0.8446 - val_auc: 0.9874\n",
            "Epoch 3/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0504 - tp: 305150.3235 - fp: 40530.4118 - tn: 5005457.1961 - fn: 55277.3627 - accuracy: 0.9817 - precision: 0.8788 - recall: 0.8406 - auc: 0.9844 - val_loss: 0.0334 - val_tp: 215003.0000 - val_fp: 17156.0000 - val_tn: 3281314.0000 - val_fn: 20602.0000 - val_accuracy: 0.9893 - val_precision: 0.9261 - val_recall: 0.9126 - val_auc: 0.9879\n",
            "Epoch 4/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0384 - tp: 321699.6667 - fp: 31083.4216 - tn: 5014904.1863 - fn: 38728.0196 - accuracy: 0.9868 - precision: 0.9102 - recall: 0.8903 - auc: 0.9883 - val_loss: 0.0275 - val_tp: 216692.0000 - val_fp: 14739.0000 - val_tn: 3283731.0000 - val_fn: 18913.0000 - val_accuracy: 0.9905 - val_precision: 0.9363 - val_recall: 0.9197 - val_auc: 0.9900\n",
            "Epoch 5/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0310 - tp: 328791.2353 - fp: 25120.4902 - tn: 5020867.1176 - fn: 31636.4510 - accuracy: 0.9894 - precision: 0.9280 - recall: 0.9113 - auc: 0.9911 - val_loss: 0.0241 - val_tp: 218251.0000 - val_fp: 13107.0000 - val_tn: 3285363.0000 - val_fn: 17354.0000 - val_accuracy: 0.9914 - val_precision: 0.9433 - val_recall: 0.9263 - val_auc: 0.9904\n",
            "Epoch 6/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0261 - tp: 332500.5392 - fp: 21363.0882 - tn: 5024624.5196 - fn: 27927.1471 - accuracy: 0.9908 - precision: 0.9390 - recall: 0.9220 - auc: 0.9931 - val_loss: 0.0209 - val_tp: 220658.0000 - val_fp: 11707.0000 - val_tn: 3286763.0000 - val_fn: 14947.0000 - val_accuracy: 0.9925 - val_precision: 0.9496 - val_recall: 0.9366 - val_auc: 0.9910\n",
            "Epoch 7/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0223 - tp: 336637.2745 - fp: 18255.3922 - tn: 5027732.2157 - fn: 23790.4118 - accuracy: 0.9921 - precision: 0.9480 - recall: 0.9330 - auc: 0.9942 - val_loss: 0.0188 - val_tp: 222677.0000 - val_fp: 10453.0000 - val_tn: 3288017.0000 - val_fn: 12928.0000 - val_accuracy: 0.9934 - val_precision: 0.9552 - val_recall: 0.9451 - val_auc: 0.9914\n",
            "Epoch 8/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0202 - tp: 339179.3137 - fp: 16942.2451 - tn: 5029045.3627 - fn: 21248.3725 - accuracy: 0.9929 - precision: 0.9520 - recall: 0.9403 - auc: 0.9949 - val_loss: 0.0177 - val_tp: 224220.0000 - val_fp: 9944.0000 - val_tn: 3288526.0000 - val_fn: 11385.0000 - val_accuracy: 0.9940 - val_precision: 0.9575 - val_recall: 0.9517 - val_auc: 0.9915\n",
            "Epoch 9/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0186 - tp: 341169.4608 - fp: 15828.9706 - tn: 5030158.6373 - fn: 19258.2255 - accuracy: 0.9935 - precision: 0.9554 - recall: 0.9462 - auc: 0.9952 - val_loss: 0.0167 - val_tp: 225308.0000 - val_fp: 9297.0000 - val_tn: 3289173.0000 - val_fn: 10297.0000 - val_accuracy: 0.9945 - val_precision: 0.9604 - val_recall: 0.9563 - val_auc: 0.9920\n",
            "Epoch 10/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0174 - tp: 342655.2549 - fp: 14946.7843 - tn: 5031040.8235 - fn: 17772.4314 - accuracy: 0.9939 - precision: 0.9582 - recall: 0.9506 - auc: 0.9956 - val_loss: 0.0156 - val_tp: 225976.0000 - val_fp: 8440.0000 - val_tn: 3290030.0000 - val_fn: 9629.0000 - val_accuracy: 0.9949 - val_precision: 0.9640 - val_recall: 0.9591 - val_auc: 0.9919\n",
            "Epoch 11/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0165 - tp: 344046.9020 - fp: 14068.9510 - tn: 5031918.6569 - fn: 16380.7843 - accuracy: 0.9943 - precision: 0.9604 - recall: 0.9542 - auc: 0.9957 - val_loss: 0.0157 - val_tp: 226548.0000 - val_fp: 8453.0000 - val_tn: 3290017.0000 - val_fn: 9057.0000 - val_accuracy: 0.9950 - val_precision: 0.9640 - val_recall: 0.9616 - val_auc: 0.9923\n",
            "Epoch 12/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0156 - tp: 345188.4412 - fp: 13335.5588 - tn: 5032652.0490 - fn: 15239.2451 - accuracy: 0.9947 - precision: 0.9628 - recall: 0.9577 - auc: 0.9959 - val_loss: 0.0142 - val_tp: 227474.0000 - val_fp: 7710.0000 - val_tn: 3290760.0000 - val_fn: 8131.0000 - val_accuracy: 0.9955 - val_precision: 0.9672 - val_recall: 0.9655 - val_auc: 0.9931\n",
            "Epoch 13/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0149 - tp: 346197.1275 - fp: 12617.9608 - tn: 5033369.6471 - fn: 14230.5588 - accuracy: 0.9950 - precision: 0.9648 - recall: 0.9604 - auc: 0.9961 - val_loss: 0.0140 - val_tp: 227945.0000 - val_fp: 7534.0000 - val_tn: 3290936.0000 - val_fn: 7660.0000 - val_accuracy: 0.9957 - val_precision: 0.9680 - val_recall: 0.9675 - val_auc: 0.9930\n",
            "Epoch 14/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0141 - tp: 347181.9902 - fp: 11931.5196 - tn: 5034056.0882 - fn: 13245.6961 - accuracy: 0.9953 - precision: 0.9665 - recall: 0.9629 - auc: 0.9963 - val_loss: 0.0137 - val_tp: 228627.0000 - val_fp: 6850.0000 - val_tn: 3291620.0000 - val_fn: 6978.0000 - val_accuracy: 0.9961 - val_precision: 0.9709 - val_recall: 0.9704 - val_auc: 0.9926\n",
            "Epoch 15/100\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.0136 - tp: 347932.4314 - fp: 11312.9314 - tn: 5034674.6765 - fn: 12495.2549 - accuracy: 0.9956 - precision: 0.9684 - recall: 0.9652 - auc: 0.9963 - val_loss: 0.0127 - val_tp: 229159.0000 - val_fp: 6343.0000 - val_tn: 3292127.0000 - val_fn: 6446.0000 - val_accuracy: 0.9964 - val_precision: 0.9731 - val_recall: 0.9726 - val_auc: 0.9945\n",
            "Epoch 16/100\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.0132 - tp: 348331.9412 - fp: 11062.4314 - tn: 5034925.1765 - fn: 12095.7451 - accuracy: 0.9957 - precision: 0.9691 - recall: 0.9663 - auc: 0.9965 - val_loss: 0.0137 - val_tp: 228497.0000 - val_fp: 7028.0000 - val_tn: 3291442.0000 - val_fn: 7108.0000 - val_accuracy: 0.9960 - val_precision: 0.9702 - val_recall: 0.9698 - val_auc: 0.9936\n",
            "Epoch 17/100\n",
            "101/101 [==============================] - 2s 24ms/step - loss: 0.0128 - tp: 348964.6667 - fp: 10536.3824 - tn: 5035451.2255 - fn: 11463.0196 - accuracy: 0.9959 - precision: 0.9706 - recall: 0.9680 - auc: 0.9966 - val_loss: 0.0131 - val_tp: 228678.0000 - val_fp: 6841.0000 - val_tn: 3291629.0000 - val_fn: 6927.0000 - val_accuracy: 0.9961 - val_precision: 0.9710 - val_recall: 0.9706 - val_auc: 0.9928\n",
            "Epoch 18/100\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.0122 - tp: 349700.0098 - fp: 9892.6863 - tn: 5036094.9216 - fn: 10727.6765 - accuracy: 0.9962 - precision: 0.9724 - recall: 0.9701 - auc: 0.9966 - val_loss: 0.0122 - val_tp: 229170.0000 - val_fp: 6364.0000 - val_tn: 3292106.0000 - val_fn: 6435.0000 - val_accuracy: 0.9964 - val_precision: 0.9730 - val_recall: 0.9727 - val_auc: 0.9937\n",
            "Epoch 19/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0117 - tp: 350193.8137 - fp: 9510.8039 - tn: 5036476.8039 - fn: 10233.8725 - accuracy: 0.9964 - precision: 0.9737 - recall: 0.9717 - auc: 0.9967 - val_loss: 0.0123 - val_tp: 229069.0000 - val_fp: 6457.0000 - val_tn: 3292013.0000 - val_fn: 6536.0000 - val_accuracy: 0.9963 - val_precision: 0.9726 - val_recall: 0.9723 - val_auc: 0.9955\n",
            "Epoch 20/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0115 - tp: 350429.6078 - fp: 9321.9706 - tn: 5036665.6373 - fn: 9998.0784 - accuracy: 0.9964 - precision: 0.9739 - recall: 0.9721 - auc: 0.9969 - val_loss: 0.0121 - val_tp: 229320.0000 - val_fp: 6246.0000 - val_tn: 3292224.0000 - val_fn: 6285.0000 - val_accuracy: 0.9965 - val_precision: 0.9735 - val_recall: 0.9733 - val_auc: 0.9954\n",
            "Epoch 21/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0112 - tp: 350637.4412 - fp: 9149.1569 - tn: 5036838.4510 - fn: 9790.2451 - accuracy: 0.9965 - precision: 0.9746 - recall: 0.9729 - auc: 0.9970 - val_loss: 0.0120 - val_tp: 229228.0000 - val_fp: 6261.0000 - val_tn: 3292209.0000 - val_fn: 6377.0000 - val_accuracy: 0.9964 - val_precision: 0.9734 - val_recall: 0.9729 - val_auc: 0.9951\n",
            "Epoch 22/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0108 - tp: 350977.6471 - fp: 8817.7843 - tn: 5037169.8235 - fn: 9450.0392 - accuracy: 0.9966 - precision: 0.9754 - recall: 0.9737 - auc: 0.9972 - val_loss: 0.0121 - val_tp: 229188.0000 - val_fp: 6219.0000 - val_tn: 3292251.0000 - val_fn: 6417.0000 - val_accuracy: 0.9964 - val_precision: 0.9736 - val_recall: 0.9728 - val_auc: 0.9954\n",
            "Epoch 23/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0106 - tp: 351200.3039 - fp: 8598.0294 - tn: 5037389.5784 - fn: 9227.3824 - accuracy: 0.9967 - precision: 0.9762 - recall: 0.9745 - auc: 0.9972 - val_loss: 0.0121 - val_tp: 229141.0000 - val_fp: 6114.0000 - val_tn: 3292356.0000 - val_fn: 6464.0000 - val_accuracy: 0.9964 - val_precision: 0.9740 - val_recall: 0.9726 - val_auc: 0.9953\n",
            "Epoch 24/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0103 - tp: 351534.0098 - fp: 8281.9216 - tn: 5037705.6863 - fn: 8893.6765 - accuracy: 0.9968 - precision: 0.9768 - recall: 0.9752 - auc: 0.9972 - val_loss: 0.0112 - val_tp: 229506.0000 - val_fp: 5790.0000 - val_tn: 3292680.0000 - val_fn: 6099.0000 - val_accuracy: 0.9966 - val_precision: 0.9754 - val_recall: 0.9741 - val_auc: 0.9964\n",
            "Epoch 25/100\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.0101 - tp: 351666.0392 - fp: 8151.5490 - tn: 5037836.0588 - fn: 8761.6471 - accuracy: 0.9968 - precision: 0.9772 - recall: 0.9755 - auc: 0.9974 - val_loss: 0.0108 - val_tp: 229811.0000 - val_fp: 5748.0000 - val_tn: 3292722.0000 - val_fn: 5794.0000 - val_accuracy: 0.9967 - val_precision: 0.9756 - val_recall: 0.9754 - val_auc: 0.9968\n",
            "Epoch 26/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0098 - tp: 351897.3333 - fp: 7953.9804 - tn: 5038033.6275 - fn: 8530.3529 - accuracy: 0.9969 - precision: 0.9778 - recall: 0.9762 - auc: 0.9975 - val_loss: 0.0106 - val_tp: 230197.0000 - val_fp: 5349.0000 - val_tn: 3293121.0000 - val_fn: 5408.0000 - val_accuracy: 0.9970 - val_precision: 0.9773 - val_recall: 0.9770 - val_auc: 0.9970\n",
            "Epoch 27/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0099 - tp: 351773.3725 - fp: 8120.5490 - tn: 5037867.0588 - fn: 8654.3137 - accuracy: 0.9969 - precision: 0.9773 - recall: 0.9759 - auc: 0.9974 - val_loss: 0.0111 - val_tp: 229975.0000 - val_fp: 5584.0000 - val_tn: 3292886.0000 - val_fn: 5630.0000 - val_accuracy: 0.9968 - val_precision: 0.9763 - val_recall: 0.9761 - val_auc: 0.9964\n",
            "Epoch 28/100\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.0095 - tp: 352155.8725 - fp: 7776.1765 - tn: 5038211.4314 - fn: 8271.8137 - accuracy: 0.9970 - precision: 0.9783 - recall: 0.9769 - auc: 0.9977 - val_loss: 0.0105 - val_tp: 230066.0000 - val_fp: 5480.0000 - val_tn: 3292990.0000 - val_fn: 5539.0000 - val_accuracy: 0.9969 - val_precision: 0.9767 - val_recall: 0.9765 - val_auc: 0.9968\n",
            "Epoch 29/100\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.0092 - tp: 352336.7647 - fp: 7618.7843 - tn: 5038368.8235 - fn: 8090.9216 - accuracy: 0.9971 - precision: 0.9790 - recall: 0.9777 - auc: 0.9978 - val_loss: 0.0110 - val_tp: 230002.0000 - val_fp: 5578.0000 - val_tn: 3292892.0000 - val_fn: 5603.0000 - val_accuracy: 0.9968 - val_precision: 0.9763 - val_recall: 0.9762 - val_auc: 0.9964\n",
            "Epoch 30/100\n",
            "101/101 [==============================] - 2s 21ms/step - loss: 0.0090 - tp: 352511.5098 - fp: 7487.7745 - tn: 5038499.8333 - fn: 7916.1765 - accuracy: 0.9972 - precision: 0.9793 - recall: 0.9781 - auc: 0.9979 - val_loss: 0.0120 - val_tp: 229643.0000 - val_fp: 5927.0000 - val_tn: 3292543.0000 - val_fn: 5962.0000 - val_accuracy: 0.9966 - val_precision: 0.9748 - val_recall: 0.9747 - val_auc: 0.9958\n",
            "Epoch 31/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0091 - tp: 352410.3627 - fp: 7569.9314 - tn: 5038417.6765 - fn: 8017.3235 - accuracy: 0.9971 - precision: 0.9788 - recall: 0.9776 - auc: 0.9980 - val_loss: 0.0119 - val_tp: 229665.0000 - val_fp: 5844.0000 - val_tn: 3292626.0000 - val_fn: 5940.0000 - val_accuracy: 0.9967 - val_precision: 0.9752 - val_recall: 0.9748 - val_auc: 0.9960\n",
            "Epoch 32/100\n",
            "101/101 [==============================] - 2s 23ms/step - loss: 0.0089 - tp: 352585.7843 - fp: 7435.5392 - tn: 5038552.0686 - fn: 7841.9020 - accuracy: 0.9972 - precision: 0.9792 - recall: 0.9782 - auc: 0.9980 - val_loss: 0.0108 - val_tp: 229872.0000 - val_fp: 5679.0000 - val_tn: 3292791.0000 - val_fn: 5733.0000 - val_accuracy: 0.9968 - val_precision: 0.9759 - val_recall: 0.9757 - val_auc: 0.9965\n",
            "Epoch 33/100\n",
            "101/101 [==============================] - 2s 23ms/step - loss: 0.0087 - tp: 352691.1569 - fp: 7364.1373 - tn: 5038623.4706 - fn: 7736.5294 - accuracy: 0.9972 - precision: 0.9795 - recall: 0.9784 - auc: 0.9980 - val_loss: 0.0120 - val_tp: 229491.0000 - val_fp: 6075.0000 - val_tn: 3292395.0000 - val_fn: 6114.0000 - val_accuracy: 0.9966 - val_precision: 0.9742 - val_recall: 0.9740 - val_auc: 0.9960\n",
            "Epoch 34/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0087 - tp: 352865.8824 - fp: 7172.9510 - tn: 5038814.6569 - fn: 7561.8039 - accuracy: 0.9973 - precision: 0.9799 - recall: 0.9789 - auc: 0.9980 - val_loss: 0.0115 - val_tp: 229539.0000 - val_fp: 6005.0000 - val_tn: 3292465.0000 - val_fn: 6066.0000 - val_accuracy: 0.9966 - val_precision: 0.9745 - val_recall: 0.9743 - val_auc: 0.9963\n",
            "Epoch 35/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0086 - tp: 352831.2157 - fp: 7239.2647 - tn: 5038748.3431 - fn: 7596.4706 - accuracy: 0.9973 - precision: 0.9799 - recall: 0.9789 - auc: 0.9981 - val_loss: 0.0105 - val_tp: 230182.0000 - val_fp: 5384.0000 - val_tn: 3293086.0000 - val_fn: 5423.0000 - val_accuracy: 0.9969 - val_precision: 0.9771 - val_recall: 0.9770 - val_auc: 0.9969\n",
            "Epoch 36/100\n",
            "101/101 [==============================] - 2s 22ms/step - loss: 0.0085 - tp: 352868.5098 - fp: 7218.0000 - tn: 5038769.6078 - fn: 7559.1765 - accuracy: 0.9973 - precision: 0.9800 - recall: 0.9791 - auc: 0.9981 - val_loss: 0.0110 - val_tp: 229996.0000 - val_fp: 5579.0000 - val_tn: 3292891.0000 - val_fn: 5609.0000 - val_accuracy: 0.9968 - val_precision: 0.9763 - val_recall: 0.9762 - val_auc: 0.9966\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00036: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPDH0IA_8xee"
      },
      "source": [
        "y_pred=model_dnn.predict(X_test)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PdaWVhX9BvJ",
        "outputId": "b09d3a5a-9b08-43e5-b32f-c47bbae8d1bb"
      },
      "source": [
        "accuracy = model_dnn.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)\r\n",
        "print('Accuracy is: ', accuracy)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45/45 [==============================] - 1s 16ms/step - loss: 0.0128 - tp: 305952.0000 - fp: 7826.0000 - tn: 4390120.0000 - fn: 8187.0000 - accuracy: 0.9966 - precision: 0.9751 - recall: 0.9739 - auc: 0.9961\n",
            "Accuracy is:  [0.012817840091884136, 305952.0, 7826.0, 4390120.0, 8187.0, 0.9966016411781311, 0.9750587940216064, 0.9739382863044739, 0.9961137771606445]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZcyLQPqAlWS",
        "outputId": "8579ad19-d9f1-48ca-e0d4-735b4aebf096"
      },
      "source": [
        "display_metrics(y_test_ada, np.argmax(y_pred, axis = 1), labels_d)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.97\n",
            "\n",
            "Micro Precision: 0.97\n",
            "Micro Recall: 0.97\n",
            "Micro F1-score: 0.97\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.58\n",
            "Macro Recall: 0.51\n",
            "Macro F1-score: 0.53\n",
            "\n",
            "Weighted Precision: 0.97\n",
            "Weighted Recall: 0.97\n",
            "Weighted F1-score: 0.97\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.98      0.98      0.98    175063\n",
            "                       Bot       0.00      0.00      0.00       492\n",
            "                      DDoS       1.00      0.98      0.99     32261\n",
            "             DoS GoldenEye       0.99      0.94      0.96      2536\n",
            "                  DoS Hulk       0.98      1.00      0.99     57443\n",
            "          DoS Slowhttptest       0.92      0.89      0.90      1390\n",
            "             DoS slowloris       0.99      0.79      0.88      1488\n",
            "               FTP-Patator       0.96      0.87      0.91      1918\n",
            "                Heartbleed       0.00      0.00      0.00         4\n",
            "              Infiltration       0.00      0.00      0.00        13\n",
            "                  PortScan       0.93      0.99      0.96     39575\n",
            "               SSH-Patator       1.00      0.27      0.43      1416\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       369\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         6\n",
            "          Web Attack � XSS       0.00      0.00      0.00       165\n",
            "\n",
            "                  accuracy                           0.97    314139\n",
            "                 macro avg       0.58      0.51      0.53    314139\n",
            "              weighted avg       0.97      0.97      0.97    314139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erUa8gpIZ04U"
      },
      "source": [
        "# Model 10: CNN1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTmLBZDpZ04U"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Flatten, Dense, Activation,Dropout\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLXy3meMZ04U",
        "outputId": "f144a82f-a850-49c4-974e-1d2ae9ca3ca2"
      },
      "source": [
        "#hyper-params\n",
        "batch_size = 1024 # increasing batch size with more gpu added\n",
        "input_dim = X_train.shape[1]\n",
        "num_class = 15                   # 15 intrusion classes, including benign traffic class\n",
        "num_epochs = 30\n",
        "learning_rates = 1e-3\n",
        "regularizations = 1e-3\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "print(input_dim)\n",
        "print(num_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNRbsq6CZ04V",
        "outputId": "148ee859-e320-4a22-c118-cabe52ab7d56"
      },
      "source": [
        "#X_train_r = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_train_r = np.zeros((len(X_train), input_dim, 1))\n",
        "X_train_r[:, :, 0] = X_train[:, :input_dim]\n",
        "print(X_train_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjmIpn1HZ04W",
        "outputId": "2c1c2f93-014b-4c08-8435-e739318768fc"
      },
      "source": [
        "X_test_r = np.zeros((len(X_test), input_dim, 1))\n",
        "X_test_r[:, :, 0] = X_test[:, :input_dim]\n",
        "print(X_test_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_KlI1VeZ04W",
        "outputId": "68ffab00-7db7-42ca-d671-16330e9fa8f9"
      },
      "source": [
        "X_val_r = np.zeros((len(X_val), input_dim, 1))\n",
        "X_val_r[:, :, 0] = X_val[:, :input_dim]\n",
        "print(X_val_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkQlzQU0ajuD",
        "outputId": "71d58c31-8c34-435a-934b-9006668ffdc5"
      },
      "source": [
        "X_train_r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[7.66048676e-01],\n",
              "        [5.93603125e-03],\n",
              "        [1.00000000e+00],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[7.29167620e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [7.12500000e-01],\n",
              "        [7.12500000e-01]],\n",
              "\n",
              "       [[7.86144808e-01],\n",
              "        [6.76005616e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [8.96864890e-03],\n",
              "        [4.90833333e-01],\n",
              "        [4.83333333e-01]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[2.53009842e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[7.85610742e-01],\n",
              "        [8.08765183e-04],\n",
              "        [1.00000000e+00],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[8.26276036e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpTZU5OPZ04W",
        "outputId": "31aa9725-0fe7-4c0d-9ab7-1b4176ad7fcd"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', input_shape=(71,1)))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=3))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_class))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_8 (Conv1D)            (None, 71, 32)            128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 71, 32)            284       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 71, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 69, 128)           12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 69, 128)           276       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 69, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8832)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               883300    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 15)                1515      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 897,919\n",
            "Trainable params: 897,639\n",
            "Non-trainable params: 280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEpzDBNyC7P2"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_accuracy', \r\n",
        "    verbose=1,\r\n",
        "    patience=10,\r\n",
        "    mode='max',\r\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb52JhHSZ04W"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFJZ11t1Z04X"
      },
      "source": [
        "## Step 5. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq31n_0YZ04X",
        "outputId": "ad9b4e62-35a9-429f-9d10-4086725a878f"
      },
      "source": [
        "# fit network\n",
        "epochs = 50\n",
        "model.fit(X_train_r, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_r, y_test), verbose=1, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.9541 - val_accuracy: 0.8272\n",
            "Epoch 2/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.5939 - val_accuracy: 0.8978\n",
            "Epoch 3/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0106 - accuracy: 0.9963 - val_loss: 0.4867 - val_accuracy: 0.9373\n",
            "Epoch 4/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 1.2544 - val_accuracy: 0.7860\n",
            "Epoch 5/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.5194 - val_accuracy: 0.9147\n",
            "Epoch 6/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 0.7574 - val_accuracy: 0.8717\n",
            "Epoch 7/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.4678 - val_accuracy: 0.9551\n",
            "Epoch 8/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.3560 - val_accuracy: 0.9581\n",
            "Epoch 9/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 0.7849 - val_accuracy: 0.8832\n",
            "Epoch 10/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0091 - accuracy: 0.9967 - val_loss: 0.3110 - val_accuracy: 0.9654\n",
            "Epoch 11/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 0.7204 - val_accuracy: 0.8893\n",
            "Epoch 12/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 0.5198 - val_accuracy: 0.9458\n",
            "Epoch 13/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0090 - accuracy: 0.9967 - val_loss: 0.9469 - val_accuracy: 0.8524\n",
            "Epoch 14/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0085 - accuracy: 0.9968 - val_loss: 0.3824 - val_accuracy: 0.9637\n",
            "Epoch 15/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 0.3664 - val_accuracy: 0.9600\n",
            "Epoch 16/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0088 - accuracy: 0.9967 - val_loss: 0.5777 - val_accuracy: 0.9319\n",
            "Epoch 17/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0087 - accuracy: 0.9967 - val_loss: 0.3704 - val_accuracy: 0.9621\n",
            "Epoch 18/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0086 - accuracy: 0.9968 - val_loss: 0.4081 - val_accuracy: 0.9647\n",
            "Epoch 19/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.4933 - val_accuracy: 0.9559\n",
            "Epoch 20/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 1.0666 - val_accuracy: 0.8716\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00020: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7fd100bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSo0TpT5Z04X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07be9f6-400d-481c-8ee2-ebceae01716b"
      },
      "source": [
        "# evaluate model\n",
        "accuracy = model.evaluate(X_val_r, y_val, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "272/272 [==============================] - 2s 6ms/step - loss: 79.3116 - accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqGNmOrhH1CL"
      },
      "source": [
        "y_pred=model.predict(X_val_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltyd2KVLIBCK",
        "outputId": "aad22263-814e-4f9e-a3c2-621c490ef3df"
      },
      "source": [
        "display_metrics(y_val_ada, np.argmax(y_pred, axis = 1), labels_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.50\n",
            "\n",
            "Micro Precision: 0.50\n",
            "Micro Recall: 0.50\n",
            "Micro F1-score: 0.50\n",
            "\n",
            "Macro Precision: 0.03\n",
            "Macro Recall: 0.07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.04\n",
            "\n",
            "Weighted Precision: 0.25\n",
            "Weighted Recall: 0.50\n",
            "Weighted F1-score: 0.33\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.50      1.00      0.67    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.00      0.00      0.00     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         2\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.50    278270\n",
            "                 macro avg       0.03      0.07      0.04    278270\n",
            "              weighted avg       0.25      0.50      0.33    278270\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbhp6oW_Z04X"
      },
      "source": [
        ""
      ]
    }
  ]
}