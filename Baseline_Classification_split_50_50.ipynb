{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Classification_split_50_50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwangliberty/AIoTDesign-Frontend/blob/master/Baseline_Classification_split_50_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq5p9swvZ04G"
      },
      "source": [
        "# Baseline Models for CICIDS 2017 Data Set with 71 Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXg4HMXOZ04H"
      },
      "source": [
        "We use the pre-processing dataset from mlp4nids (Multi-layer perceptron for network intrusion detection) https://github.com/ArnaudRosay/mlp4nids. The dataset is splitted into 50:25:25.  We use the following classification methods: PCA+RF,Naive Bayes model, Decision Tree Classifier, Random Foresty with DecisionTree, Logistic Regression Classifier, Adaboost, Voting, kNN, and DNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOKT1sRZ04I"
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH9hOnpmzv6-"
      },
      "source": [
        "def display_metrics(y_test, y_pred, label_names):\r\n",
        "  print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\r\n",
        "\r\n",
        "  print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\r\n",
        "  print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\r\n",
        "  print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\r\n",
        "\r\n",
        "  print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\r\n",
        "  print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\r\n",
        "  print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\r\n",
        "\r\n",
        "  print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\r\n",
        "  print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\r\n",
        "  print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\r\n",
        "\r\n",
        "  print('\\nClassification Report\\n')\r\n",
        "  print(classification_report(y_test, y_pred, target_names=label_names))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCwZZFMTZ04J"
      },
      "source": [
        "def display_all(df):\n",
        "    with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 100): \n",
        "        print(df)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDogrv3Z04J"
      },
      "source": [
        "def make_value2index(attacks):\n",
        "    #make dictionary\n",
        "    attacks = sorted(attacks)\n",
        "    d = {}\n",
        "    counter=0\n",
        "    for attack in attacks:\n",
        "        d[attack] = counter\n",
        "        counter+=1\n",
        "    return d"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3b8hIQZ04K"
      },
      "source": [
        "# chganges label from string to integer/index\n",
        "def encode_label(Y_str):\n",
        "    labels_d = make_value2index(np.unique(Y_str))\n",
        "    Y = [labels_d[y_str] for y_str  in Y_str]\n",
        "    Y = np.array(Y)\n",
        "    return np.array(Y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMisXWZWZ04K"
      },
      "source": [
        "## Step 1. Loading csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwxZ7DrXZ04L"
      },
      "source": [
        "# All columns\n",
        "col_names = ['Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoCkMRAcaIA6"
      },
      "source": [
        "### Option 1. Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uaw_A5kaHSj",
        "outputId": "7e0d1c17-8599-4465-e67e-e6dba6ebfdd2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qUtBnilZ04M"
      },
      "source": [
        "# load three csv files generated by mlp4nids (Multi-layer perceptron for network intrusion detection )\n",
        "# first load the train set\n",
        "df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set.csv',names=col_names, skiprows=1)  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBn8DsjhX_U4",
        "outputId": "e7e0ea7b-04aa-4097-e616-8e4c26025fe0"
      },
      "source": [
        "print('Train set size: ', df_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size:  (556548, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYvaCAjZ04P",
        "outputId": "5ab53c11-8010-4c46-e5ae-25fd9d84ecda"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set.csv',names=col_names, skiprows=1)  \n",
        "print('Test set size: ', df_test.shape)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set.csv',names=col_names, skiprows=1)  \n",
        "print('Validation set size: ', df_val.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set size:  (278271, 72)\n",
            "Validation set size:  (278271, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cbbuInpXhvz"
      },
      "source": [
        "### Option 2. Load from local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLcmd-A8Xhv0"
      },
      "source": [
        "dataroot = '../data/cicids2017clean/train_set.csv'\n",
        "df_train = pd.read_csv(dataroot, names=col_names, skiprows=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9oqAbFyXhv0"
      },
      "source": [
        "dataroot = '../data/cicids2017clean/crossval_set.csv'\n",
        "df_val = pd.read_csv(dataroot, names=col_names, skiprows=1) \n",
        "dataroot = '../data/cicids2017clean/test_set.csv'\n",
        "df_test = pd.read_csv(dataroot, names=col_names, skiprows=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls46tMA9Xhv0"
      },
      "source": [
        "## Step 2. Exploring the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "q-oBvrtQXhv0",
        "outputId": "afc5d71f-389f-4f71-8a45-54ae57f28348"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>546863</th>\n",
              "      <td>50203</td>\n",
              "      <td>389</td>\n",
              "      <td>17</td>\n",
              "      <td>130</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>328</td>\n",
              "      <td>326</td>\n",
              "      <td>164</td>\n",
              "      <td>164</td>\n",
              "      <td>164.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>163</td>\n",
              "      <td>163</td>\n",
              "      <td>163.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.030769e+06</td>\n",
              "      <td>30769.230770</td>\n",
              "      <td>4.333333e+01</td>\n",
              "      <td>6.812733e+01</td>\n",
              "      <td>122</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4.000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>40</td>\n",
              "      <td>15384.615380</td>\n",
              "      <td>15384.615380</td>\n",
              "      <td>163</td>\n",
              "      <td>164</td>\n",
              "      <td>163.600000</td>\n",
              "      <td>0.547723</td>\n",
              "      <td>3.000000e-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>204.500000</td>\n",
              "      <td>164.0</td>\n",
              "      <td>163.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>328</td>\n",
              "      <td>2</td>\n",
              "      <td>326</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2284286</th>\n",
              "      <td>47786</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>85594954</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>360</td>\n",
              "      <td>11595</td>\n",
              "      <td>360</td>\n",
              "      <td>0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>146.969385</td>\n",
              "      <td>4344</td>\n",
              "      <td>0</td>\n",
              "      <td>1932.500000</td>\n",
              "      <td>1754.831473</td>\n",
              "      <td>1.396694e+02</td>\n",
              "      <td>0.140195</td>\n",
              "      <td>7.781359e+06</td>\n",
              "      <td>2.580000e+07</td>\n",
              "      <td>85500000</td>\n",
              "      <td>4</td>\n",
              "      <td>85500000</td>\n",
              "      <td>1.710000e+07</td>\n",
              "      <td>38200000.0</td>\n",
              "      <td>85500000</td>\n",
              "      <td>4</td>\n",
              "      <td>144398</td>\n",
              "      <td>28879.600</td>\n",
              "      <td>5.847893e+04</td>\n",
              "      <td>133148</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>200</td>\n",
              "      <td>200</td>\n",
              "      <td>0.070098</td>\n",
              "      <td>0.070098</td>\n",
              "      <td>0</td>\n",
              "      <td>4344</td>\n",
              "      <td>919.615385</td>\n",
              "      <td>1498.335273</td>\n",
              "      <td>2.245009e+06</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>996.250000</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1932.500000</td>\n",
              "      <td>6</td>\n",
              "      <td>360</td>\n",
              "      <td>6</td>\n",
              "      <td>11595</td>\n",
              "      <td>251</td>\n",
              "      <td>235</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>495.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>495</td>\n",
              "      <td>495</td>\n",
              "      <td>85500000.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>85500000</td>\n",
              "      <td>85500000</td>\n",
              "      <td>DoS Hulk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390020</th>\n",
              "      <td>51520</td>\n",
              "      <td>443</td>\n",
              "      <td>6</td>\n",
              "      <td>117915594</td>\n",
              "      <td>19</td>\n",
              "      <td>17</td>\n",
              "      <td>1406</td>\n",
              "      <td>4654</td>\n",
              "      <td>774</td>\n",
              "      <td>0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>179.202245</td>\n",
              "      <td>1418</td>\n",
              "      <td>0</td>\n",
              "      <td>273.764706</td>\n",
              "      <td>485.037567</td>\n",
              "      <td>5.139269e+01</td>\n",
              "      <td>0.305303</td>\n",
              "      <td>3.369017e+06</td>\n",
              "      <td>1.380000e+07</td>\n",
              "      <td>58900000</td>\n",
              "      <td>1</td>\n",
              "      <td>118000000</td>\n",
              "      <td>6.550866e+06</td>\n",
              "      <td>18900000.0</td>\n",
              "      <td>58900000</td>\n",
              "      <td>1</td>\n",
              "      <td>118000000</td>\n",
              "      <td>7366629.625</td>\n",
              "      <td>2.000000e+07</td>\n",
              "      <td>59000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>616</td>\n",
              "      <td>552</td>\n",
              "      <td>0.161132</td>\n",
              "      <td>0.144171</td>\n",
              "      <td>0</td>\n",
              "      <td>1418</td>\n",
              "      <td>163.783784</td>\n",
              "      <td>362.393290</td>\n",
              "      <td>1.313289e+05</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>168.333333</td>\n",
              "      <td>74.0</td>\n",
              "      <td>273.764706</td>\n",
              "      <td>19</td>\n",
              "      <td>1406</td>\n",
              "      <td>17</td>\n",
              "      <td>4654</td>\n",
              "      <td>29200</td>\n",
              "      <td>181</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>495327.0</td>\n",
              "      <td>572653.2552</td>\n",
              "      <td>900254</td>\n",
              "      <td>90400</td>\n",
              "      <td>58400000.0</td>\n",
              "      <td>665473.7481</td>\n",
              "      <td>58900000</td>\n",
              "      <td>58000000</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212934</th>\n",
              "      <td>61397</td>\n",
              "      <td>53</td>\n",
              "      <td>17</td>\n",
              "      <td>30731</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "      <td>93</td>\n",
              "      <td>46</td>\n",
              "      <td>46</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>93</td>\n",
              "      <td>93</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.523120e+03</td>\n",
              "      <td>65.080863</td>\n",
              "      <td>3.073100e+04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>30731</td>\n",
              "      <td>30731</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>32.540431</td>\n",
              "      <td>32.540431</td>\n",
              "      <td>46</td>\n",
              "      <td>93</td>\n",
              "      <td>61.666667</td>\n",
              "      <td>27.135463</td>\n",
              "      <td>7.363333e+02</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>92.500000</td>\n",
              "      <td>46.0</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306030</th>\n",
              "      <td>80</td>\n",
              "      <td>37209</td>\n",
              "      <td>6</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>40816.326530</td>\n",
              "      <td>4.900000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>20408.163270</td>\n",
              "      <td>20408.163270</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>332</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Source Port  Destination Port  Protocol  ...  Idle Max  Idle Min     Label\n",
              "546863         50203               389        17  ...         0         0    BENIGN\n",
              "2284286        47786                80         6  ...  85500000  85500000  DoS Hulk\n",
              "390020         51520               443         6  ...  58900000  58000000    BENIGN\n",
              "212934         61397                53        17  ...         0         0    BENIGN\n",
              "306030            80             37209         6  ...         0         0    BENIGN\n",
              "\n",
              "[5 rows x 72 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miDxhmNyZ04N"
      },
      "source": [
        "Count the number of attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpKy7b9fZ04O",
        "scrolled": true,
        "outputId": "ebb3ec0f-9a0a-4f2f-aaae-dedfcfc6bf00"
      },
      "source": [
        "df_train['Label'].value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        278274\n",
              "DoS Hulk                      115062\n",
              "PortScan                       79402\n",
              "DDoS                           64012\n",
              "DoS GoldenEye                   5146\n",
              "FTP-Patator                     3967\n",
              "SSH-Patator                     2948\n",
              "DoS slowloris                   2898\n",
              "DoS Slowhttptest                2749\n",
              "Bot                              978\n",
              "Web Attack � Brute Force         753\n",
              "Web Attack � XSS                 326\n",
              "Infiltration                      18\n",
              "Web Attack � Sql Injection        10\n",
              "Heartbleed                         5\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2X3KG4zZ04P"
      },
      "source": [
        "Read test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5hWm9Q-Z04Q",
        "outputId": "b2517e1f-9ad6-4649-e1a2-aea87155b7cd"
      },
      "source": [
        "print('Test set: ')\n",
        "df_test['Label'].value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack � Brute Force         376\n",
              "Web Attack � XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack � Sql Injection         5\n",
              "Heartbleed                         3\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0-lHNicZ04Q",
        "outputId": "6277f61d-df75-453a-aebd-1ee858e0ef8f"
      },
      "source": [
        "print('Validation set: ')\n",
        "df_val['Label'].value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack � Brute Force         376\n",
              "Web Attack � XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack � Sql Injection         5\n",
              "Heartbleed                         3\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0O9ZuKoXhv3"
      },
      "source": [
        "## Step 3. Encode Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwXKhZOjXhv3"
      },
      "source": [
        "Encoding the labels, and generate numpy array. Note that the label has not been encoded as one-hot coding. We will use one-hot code later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyAnny9yZ04R"
      },
      "source": [
        "### Step 3.1 Encoding train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsR6_hJDZ04R"
      },
      "source": [
        "df_label = df_train['Label']\n",
        "data = df_train.drop(columns=['Label'])\n",
        "Xtrain = data.values\n",
        "y_train = encode_label(df_label.values)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XlV2AK3Z04S"
      },
      "source": [
        "### Step 3.2. Encoding test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVSrGExFZ04S",
        "scrolled": true
      },
      "source": [
        "df_label = df_test['Label']\n",
        "data = df_test.drop(columns=['Label'])\n",
        "Xtest = data.values\n",
        "y_test = encode_label(df_label.values)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO3PgredZ04T"
      },
      "source": [
        "### Step 3.3 Encoding validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ2wDKpZZ04T"
      },
      "source": [
        "df_label = df_val['Label']\n",
        "data = df_val.drop(columns=['Label'])\n",
        "Xval = data.values\n",
        "y_val = encode_label(df_label.values)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viMki-R0Z04Q"
      },
      "source": [
        "## Step 4. Normalization or Standardization\n",
        "\n",
        "The continuous feature values are normalized into the same feature space. This is important when using features that have different measurements, and is a general requirement of many machine learning algorithms. We implement the two methods to see the impact on the final classifications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8ERJidxXhv5"
      },
      "source": [
        "## Option 1. Normalization\n",
        "\n",
        "The values of the datasets are normalized using the Min-Max scaling technique, bringing them all within a range of [0,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c523vLd-Z04R"
      },
      "source": [
        "### Step 4.1 Normalizing train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5izaj07Z04R"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbYQFmfgZ04S",
        "scrolled": true,
        "outputId": "b7133942-4143-4301-8fa4-ae3b6d33ca73"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(Xtrain)\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.66048676e-01, 5.93603125e-03, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [7.29167620e-01, 1.22077764e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 7.12500000e-01, 7.12500000e-01],\n",
              "       [7.86144808e-01, 6.76005616e-03, 3.52941176e-01, ...,\n",
              "        8.96864890e-03, 4.90833333e-01, 4.83333333e-01],\n",
              "       ...,\n",
              "       [2.53009842e-01, 1.22077764e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [7.85610742e-01, 8.08765183e-04, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [8.26276036e-01, 1.22077764e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ5x1QxAXhv5"
      },
      "source": [
        "### Step 4.2. Normalizing validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-_BpWSsXhv6",
        "outputId": "c6a67e91-b086-4a2f-d749-5f6de0ad79b5"
      },
      "source": [
        "X_val = scaler.fit_transform(Xval)\n",
        "X_val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.37994965e-01, 1.22344737e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [8.41290913e-01, 6.12488339e-02, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [2.87327382e-02, 6.77483980e-03, 3.52941176e-01, ...,\n",
              "        5.68691207e-05, 8.33333333e-02, 8.32947583e-02],\n",
              "       ...,\n",
              "       [9.30510414e-01, 8.10533882e-04, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [6.50492103e-01, 8.68494701e-02, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [4.24353399e-01, 1.22344737e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TItkmTF1Z04S"
      },
      "source": [
        "### Step 4.3. Normalizing test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXmp2w2bZ04T",
        "outputId": "773484d4-3b49-4ec0-816e-889cebcb36f8"
      },
      "source": [
        "X_test = scaler.fit_transform(Xtest)\n",
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.45487838e-01, 9.66383862e-01, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [9.17142247e-01, 1.22074038e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [7.92001099e-01, 7.97448653e-02, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       ...,\n",
              "       [9.04507584e-01, 1.22074038e-03, 3.52941176e-01, ...,\n",
              "        0.00000000e+00, 8.33370661e-01, 8.33370661e-01],\n",
              "       [9.22345653e-01, 8.08740501e-04, 1.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [5.45030061e-01, 6.75984985e-03, 3.52941176e-01, ...,\n",
              "        5.01804492e-03, 4.88761059e-01, 4.84412289e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge2XVkhTXhv6"
      },
      "source": [
        "## Option 2. Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu459dh3Xhv7"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOJud1h5Xhv7",
        "outputId": "f589cabe-5de6-4901-b487-d725f4955c11"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(Xtrain)\n",
        "X_val = scaler.fit_transform(Xval)\n",
        "X_test = scaler.fit_transform(Xtest)\n",
        "\n",
        "X_train"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.44038783, -0.39965161,  1.91238948, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [ 0.32397456, -0.41782911, -0.5219767 , ..., -0.16609924,\n",
              "         2.01162882,  2.14691177],\n",
              "       [ 0.50382029, -0.39647496, -0.5219767 , ..., -0.07444588,\n",
              "         1.22697701,  1.30316625],\n",
              "       ...,\n",
              "       [-1.17899436, -0.41782911, -0.5219767 , ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [ 0.50213454, -0.41941743,  1.91238948, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [ 0.63049256, -0.41782911, -0.5219767 , ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reoNDQZhZ04T"
      },
      "source": [
        "## Step 5 One-hot encoding for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So8gvIF8Z04T"
      },
      "source": [
        "y_train, y_test and y_val have to be one-hot-encoded. That means they must have dimension (number_of_samples, 15), where 15 denotes number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc97u4oZZ04U"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfeM_ZzsXhv8"
      },
      "source": [
        "Save the labels for AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N0GfC_zXhv8"
      },
      "source": [
        "y_train_ada = y_train\n",
        "y_test_ada = y_test\n",
        "y_val_ada = y_val"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQVqV19KZ04U"
      },
      "source": [
        "y_train = to_categorical(y_train, 15)\n",
        "y_test = to_categorical(y_test, 15)\n",
        "y_val = to_categorical(y_val, 15)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd9_XX_5Xhv8"
      },
      "source": [
        "## Step 6. Define the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOSi1KcXhv8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#importing confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#importing accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUup6sodXhv9"
      },
      "source": [
        "METRICS = [\n",
        "      tf.keras.metrics.TruePositives(name='tp'),\n",
        "      tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.AUC(name='auc'),\n",
        "]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJywwX9iXhv9"
      },
      "source": [
        "#  Model 1: PCA  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTYHuZtxXhv9"
      },
      "source": [
        "X_pca = df_train.drop('Label',axis=1)\n",
        "y_pca = df_train['Label']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XWs2_kXhv9"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_pca = scaler.fit_transform(X_pca)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGVxxHw2Xhv9"
      },
      "source": [
        "dfx = pd.DataFrame(data=X_pca,columns=df_train.columns[1:])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-gTg4_uXhv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "26035fa7-c301-40a6-9b5d-323aad12a197"
      },
      "source": [
        "dfx.head(10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.766049</td>\n",
              "      <td>0.005936</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.191667e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>5.199362e-07</td>\n",
              "      <td>0.006608</td>\n",
              "      <td>0.111413</td>\n",
              "      <td>0.027605</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009381</td>\n",
              "      <td>0.111644</td>\n",
              "      <td>0.037294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.087508</td>\n",
              "      <td>0.406154</td>\n",
              "      <td>4.694444e-07</td>\n",
              "      <td>8.033883e-07</td>\n",
              "      <td>1.125000e-06</td>\n",
              "      <td>1.416667e-07</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>5.128205e-03</td>\n",
              "      <td>7.692308e-03</td>\n",
              "      <td>0.119941</td>\n",
              "      <td>0.006608</td>\n",
              "      <td>0.084468</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>1.339286e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.080894</td>\n",
              "      <td>0.027605</td>\n",
              "      <td>0.037294</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>5.199035e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.729168</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>7.132916e-01</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>1.849282e-05</td>\n",
              "      <td>0.014504</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.020848</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.442150</td>\n",
              "      <td>0.262135</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>6.484476e-02</td>\n",
              "      <td>3.042453e-01</td>\n",
              "      <td>7.125000e-01</td>\n",
              "      <td>1.416667e-07</td>\n",
              "      <td>7.125000e-01</td>\n",
              "      <td>1.425000e-01</td>\n",
              "      <td>0.458034</td>\n",
              "      <td>7.125000e-01</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>1.203317e-03</td>\n",
              "      <td>2.406633e-04</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>1.109567e-03</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>2.336586e-08</td>\n",
              "      <td>3.504879e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.175020</td>\n",
              "      <td>0.474804</td>\n",
              "      <td>0.316671</td>\n",
              "      <td>1.002236e-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.394086</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.442150</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>1.849166e-05</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>4.500000e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.500000e-06</td>\n",
              "      <td>4.500000e-06</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.712500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.786145</td>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>9.826304e-01</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>7.422648e-06</td>\n",
              "      <td>0.031185</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012456</td>\n",
              "      <td>0.025421</td>\n",
              "      <td>0.081607</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062637</td>\n",
              "      <td>0.072454</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>2.807525e-02</td>\n",
              "      <td>1.627358e-01</td>\n",
              "      <td>4.908334e-01</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>9.833333e-01</td>\n",
              "      <td>5.459055e-02</td>\n",
              "      <td>0.226619</td>\n",
              "      <td>4.908333e-01</td>\n",
              "      <td>1.083333e-07</td>\n",
              "      <td>9.833333e-01</td>\n",
              "      <td>6.138858e-02</td>\n",
              "      <td>0.246002</td>\n",
              "      <td>4.916667e-01</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>5.371074e-08</td>\n",
              "      <td>7.208546e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057131</td>\n",
              "      <td>0.084563</td>\n",
              "      <td>0.076591</td>\n",
              "      <td>5.862897e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066588</td>\n",
              "      <td>0.012456</td>\n",
              "      <td>0.062637</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>7.422180e-06</td>\n",
              "      <td>0.445572</td>\n",
              "      <td>0.002777</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>4.502973e-03</td>\n",
              "      <td>0.008123</td>\n",
              "      <td>8.184127e-03</td>\n",
              "      <td>8.218182e-04</td>\n",
              "      <td>0.486667</td>\n",
              "      <td>0.008969</td>\n",
              "      <td>0.490833</td>\n",
              "      <td>0.483333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.936858</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.562001e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>1.483254e-07</td>\n",
              "      <td>0.001853</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.007743</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005352</td>\n",
              "      <td>0.063699</td>\n",
              "      <td>0.021278</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085287</td>\n",
              "      <td>0.400013</td>\n",
              "      <td>2.562000e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.562000e-04</td>\n",
              "      <td>2.562000e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>1.084681e-05</td>\n",
              "      <td>1.627022e-05</td>\n",
              "      <td>0.033848</td>\n",
              "      <td>0.003747</td>\n",
              "      <td>0.031839</td>\n",
              "      <td>0.005735</td>\n",
              "      <td>3.287202e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.036590</td>\n",
              "      <td>0.007743</td>\n",
              "      <td>0.021278</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>1.483160e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.567799</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>5.166668e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.408163</td>\n",
              "      <td>5.166666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.166666e-07</td>\n",
              "      <td>5.166666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>6.802721e-03</td>\n",
              "      <td>1.020408e-02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.005081</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.775952</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>7.117298e-01</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>1.849282e-05</td>\n",
              "      <td>0.015109</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009017</td>\n",
              "      <td>0.020106</td>\n",
              "      <td>0.667300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.663225</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>7.117304e-02</td>\n",
              "      <td>3.148585e-01</td>\n",
              "      <td>7.033334e-01</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>7.116667e-01</td>\n",
              "      <td>1.183333e-01</td>\n",
              "      <td>0.412470</td>\n",
              "      <td>7.033333e-01</td>\n",
              "      <td>1.083333e-07</td>\n",
              "      <td>2.166667e-05</td>\n",
              "      <td>7.222222e-06</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>1.665000e-05</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>2.732000e-08</td>\n",
              "      <td>2.341714e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.467164</td>\n",
              "      <td>0.515016</td>\n",
              "      <td>0.705711</td>\n",
              "      <td>4.955357e-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.430452</td>\n",
              "      <td>0.009017</td>\n",
              "      <td>0.663225</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>1.849166e-05</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>3.636364e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.636364e-08</td>\n",
              "      <td>3.636364e-08</td>\n",
              "      <td>0.703333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.703333</td>\n",
              "      <td>0.703333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.927825</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.485751e-04</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>1.851196e-05</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.336096</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.531129</td>\n",
              "      <td>0.474036</td>\n",
              "      <td>0.085381</td>\n",
              "      <td>0.400030</td>\n",
              "      <td>6.417499e-05</td>\n",
              "      <td>2.353907e-04</td>\n",
              "      <td>4.413833e-04</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>5.941667e-06</td>\n",
              "      <td>2.970833e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>5.591667e-06</td>\n",
              "      <td>4.500000e-07</td>\n",
              "      <td>4.480250e-04</td>\n",
              "      <td>1.120063e-04</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>4.412750e-04</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>1.858183e-05</td>\n",
              "      <td>4.645459e-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.667355</td>\n",
              "      <td>0.539818</td>\n",
              "      <td>2.912379e-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.575208</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.531129</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>1.851080e-05</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.933791</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.951084e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>1.866029e-07</td>\n",
              "      <td>0.002256</td>\n",
              "      <td>0.038043</td>\n",
              "      <td>0.009426</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006733</td>\n",
              "      <td>0.080137</td>\n",
              "      <td>0.026769</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085288</td>\n",
              "      <td>0.400017</td>\n",
              "      <td>1.951083e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.951083e-04</td>\n",
              "      <td>1.951083e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>1.424501e-05</td>\n",
              "      <td>2.136752e-05</td>\n",
              "      <td>0.041207</td>\n",
              "      <td>0.004714</td>\n",
              "      <td>0.039411</td>\n",
              "      <td>0.007443</td>\n",
              "      <td>5.537202e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.045293</td>\n",
              "      <td>0.009426</td>\n",
              "      <td>0.026769</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>1.865911e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.806385</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>8.893186e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>4.784689e-08</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.004110</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.400001</td>\n",
              "      <td>1.778723e-03</td>\n",
              "      <td>5.392574e-03</td>\n",
              "      <td>8.592682e-03</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>8.859475e-03</td>\n",
              "      <td>2.214869e-03</td>\n",
              "      <td>0.006278</td>\n",
              "      <td>8.592575e-03</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>3.123529e-07</td>\n",
              "      <td>2.342647e-06</td>\n",
              "      <td>0.004415</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.003098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040323</td>\n",
              "      <td>0.002769</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>4.784388e-08</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.884255</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>2.121253e-02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>4.784689e-08</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.004110</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>4.242591e-03</td>\n",
              "      <td>1.334258e-02</td>\n",
              "      <td>2.110916e-02</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>2.113341e-02</td>\n",
              "      <td>5.283352e-03</td>\n",
              "      <td>0.015573</td>\n",
              "      <td>2.110906e-02</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>1.309506e-07</td>\n",
              "      <td>9.821292e-07</td>\n",
              "      <td>0.004415</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.003098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.040323</td>\n",
              "      <td>0.002769</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>4.784388e-08</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Destination Port  Protocol  Flow Duration  ...  Idle Max  Idle Min     Label\n",
              "0          0.766049  0.005936       1.000000  ...  0.000000  0.000000  0.000000\n",
              "1          0.729168  0.001221       0.352941  ...  0.000000  0.712500  0.712500\n",
              "2          0.786145  0.006760       0.352941  ...  0.008969  0.490833  0.483333\n",
              "3          0.936858  0.000809       1.000000  ...  0.000000  0.000000  0.000000\n",
              "4          0.001221  0.567799       0.352941  ...  0.000000  0.000000  0.000000\n",
              "5          0.775952  0.001221       0.352941  ...  0.000000  0.703333  0.703333\n",
              "6          0.927825  0.001221       0.352941  ...  0.000000  0.000000  0.000000\n",
              "7          0.933791  0.000809       1.000000  ...  0.000000  0.000000  0.000000\n",
              "8          0.001221  0.806385       0.352941  ...  0.000000  0.000000  0.000000\n",
              "9          0.001221  0.884255       0.352941  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[10 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzLYPxy-Xhv-"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIE01O4yXhv-"
      },
      "source": [
        "pca = PCA(n_components=None)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd2crc5zXhv-"
      },
      "source": [
        "dfx_pca = pca.fit(dfx)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPwtI_6RXhv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "063a0cc3-f87a-4f6a-af5e-552224668d6b"
      },
      "source": [
        "plt.figure(figsize=(24,5))\n",
        "plt.scatter(x=[i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],\n",
        "            y=dfx_pca.explained_variance_ratio_,\n",
        "           s=200, alpha=0.75,c='orange',edgecolor='k')\n",
        "plt.grid(True)\n",
        "plt.title(\"Explained variance ratio of the \\nfitted principal component vector\\n\",fontsize=25)\n",
        "plt.xlabel(\"Principal components\",fontsize=15)\n",
        "plt.xticks([i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.ylabel(\"Explained variance ratio\",fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeVjWmuZXhv_"
      },
      "source": [
        "The above two plots means that the $1^{st}$ principal component explains about 38% of the total variance in the data and the $2^{nd}$ component explians further 20%. Therefore, if we just consider first two components, they together explain 58% of the total variance. Using the first 10 features should give very hight detection rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8xM86XhXhv_"
      },
      "source": [
        "Transform the scaled data set using the fitted PCA object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srySEJvRXhv_"
      },
      "source": [
        "dfx_trans = pca.transform(dfx)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COMFpsuyXhv_"
      },
      "source": [
        "Put it in a data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rtptL8AMXhwA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "0f7a8df3-1673-4356-da84-dee21c6f68dc"
      },
      "source": [
        "dfx_trans = pd.DataFrame(data=dfx_trans)\n",
        "dfx_trans.head(10)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.528423</td>\n",
              "      <td>-0.054268</td>\n",
              "      <td>-0.650975</td>\n",
              "      <td>-0.121440</td>\n",
              "      <td>0.312771</td>\n",
              "      <td>-0.020526</td>\n",
              "      <td>0.162312</td>\n",
              "      <td>0.045466</td>\n",
              "      <td>0.016224</td>\n",
              "      <td>0.018418</td>\n",
              "      <td>-0.016460</td>\n",
              "      <td>-0.009318</td>\n",
              "      <td>-0.029015</td>\n",
              "      <td>-0.037342</td>\n",
              "      <td>0.054113</td>\n",
              "      <td>0.022644</td>\n",
              "      <td>-0.034882</td>\n",
              "      <td>-0.064366</td>\n",
              "      <td>0.059290</td>\n",
              "      <td>0.067341</td>\n",
              "      <td>0.008456</td>\n",
              "      <td>0.005647</td>\n",
              "      <td>-0.016734</td>\n",
              "      <td>-0.009924</td>\n",
              "      <td>0.011498</td>\n",
              "      <td>0.040912</td>\n",
              "      <td>-0.002341</td>\n",
              "      <td>-0.006321</td>\n",
              "      <td>0.005452</td>\n",
              "      <td>-0.001471</td>\n",
              "      <td>-0.004027</td>\n",
              "      <td>-0.001112</td>\n",
              "      <td>-0.004714</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>-0.017931</td>\n",
              "      <td>0.061893</td>\n",
              "      <td>-0.026181</td>\n",
              "      <td>0.042581</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.003243</td>\n",
              "      <td>-0.012712</td>\n",
              "      <td>-0.004542</td>\n",
              "      <td>0.003541</td>\n",
              "      <td>0.003650</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>0.001483</td>\n",
              "      <td>-0.004438</td>\n",
              "      <td>-0.003796</td>\n",
              "      <td>0.000328</td>\n",
              "      <td>-0.003189</td>\n",
              "      <td>-0.000144</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>-2.176527e-06</td>\n",
              "      <td>1.611283e-05</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>4.041820e-06</td>\n",
              "      <td>-1.238171e-06</td>\n",
              "      <td>-2.748805e-08</td>\n",
              "      <td>-1.509605e-14</td>\n",
              "      <td>-2.228795e-17</td>\n",
              "      <td>-8.503432e-18</td>\n",
              "      <td>1.625856e-17</td>\n",
              "      <td>-1.218493e-17</td>\n",
              "      <td>-4.179102e-17</td>\n",
              "      <td>-2.199646e-17</td>\n",
              "      <td>9.685354e-17</td>\n",
              "      <td>-1.851814e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.739488</td>\n",
              "      <td>-0.212510</td>\n",
              "      <td>0.025645</td>\n",
              "      <td>-0.445235</td>\n",
              "      <td>-0.287995</td>\n",
              "      <td>-0.163435</td>\n",
              "      <td>0.206944</td>\n",
              "      <td>-0.119024</td>\n",
              "      <td>-0.104394</td>\n",
              "      <td>-0.022308</td>\n",
              "      <td>0.020001</td>\n",
              "      <td>-0.035603</td>\n",
              "      <td>-0.030896</td>\n",
              "      <td>-0.109088</td>\n",
              "      <td>-0.012269</td>\n",
              "      <td>-0.034573</td>\n",
              "      <td>0.009246</td>\n",
              "      <td>-0.110195</td>\n",
              "      <td>-0.106397</td>\n",
              "      <td>-0.012228</td>\n",
              "      <td>-0.019110</td>\n",
              "      <td>0.059648</td>\n",
              "      <td>-0.018026</td>\n",
              "      <td>0.005633</td>\n",
              "      <td>0.010983</td>\n",
              "      <td>-0.019082</td>\n",
              "      <td>-0.023557</td>\n",
              "      <td>-0.014568</td>\n",
              "      <td>0.003716</td>\n",
              "      <td>-0.009144</td>\n",
              "      <td>0.005187</td>\n",
              "      <td>0.002976</td>\n",
              "      <td>0.008936</td>\n",
              "      <td>0.004616</td>\n",
              "      <td>0.002021</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>-0.000896</td>\n",
              "      <td>-0.001479</td>\n",
              "      <td>-0.002188</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>0.003698</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.002848</td>\n",
              "      <td>-0.000120</td>\n",
              "      <td>0.000680</td>\n",
              "      <td>0.000603</td>\n",
              "      <td>-0.001401</td>\n",
              "      <td>-0.001029</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.001740</td>\n",
              "      <td>-0.001242</td>\n",
              "      <td>-0.000199</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>-4.146829e-05</td>\n",
              "      <td>1.448086e-05</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-2.261037e-06</td>\n",
              "      <td>3.311790e-07</td>\n",
              "      <td>3.997343e-08</td>\n",
              "      <td>1.356087e-14</td>\n",
              "      <td>9.996802e-17</td>\n",
              "      <td>-9.803726e-17</td>\n",
              "      <td>-1.711226e-16</td>\n",
              "      <td>5.436059e-18</td>\n",
              "      <td>-6.359054e-17</td>\n",
              "      <td>1.297594e-16</td>\n",
              "      <td>2.321866e-17</td>\n",
              "      <td>2.516472e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.045422</td>\n",
              "      <td>0.851608</td>\n",
              "      <td>-0.133830</td>\n",
              "      <td>1.096464</td>\n",
              "      <td>-0.612687</td>\n",
              "      <td>-0.005037</td>\n",
              "      <td>0.194180</td>\n",
              "      <td>0.308140</td>\n",
              "      <td>0.292880</td>\n",
              "      <td>-0.059210</td>\n",
              "      <td>0.134051</td>\n",
              "      <td>-0.069702</td>\n",
              "      <td>0.107466</td>\n",
              "      <td>-0.122517</td>\n",
              "      <td>-0.163518</td>\n",
              "      <td>0.133958</td>\n",
              "      <td>-0.021142</td>\n",
              "      <td>0.009501</td>\n",
              "      <td>0.041338</td>\n",
              "      <td>0.024245</td>\n",
              "      <td>0.041956</td>\n",
              "      <td>-0.049701</td>\n",
              "      <td>0.029794</td>\n",
              "      <td>-0.075642</td>\n",
              "      <td>0.011467</td>\n",
              "      <td>-0.001007</td>\n",
              "      <td>-0.017069</td>\n",
              "      <td>-0.045927</td>\n",
              "      <td>-0.012788</td>\n",
              "      <td>0.018669</td>\n",
              "      <td>-0.006433</td>\n",
              "      <td>-0.001649</td>\n",
              "      <td>-0.004328</td>\n",
              "      <td>-0.007064</td>\n",
              "      <td>-0.012518</td>\n",
              "      <td>-0.005144</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>0.008119</td>\n",
              "      <td>-0.001088</td>\n",
              "      <td>-0.006066</td>\n",
              "      <td>0.006021</td>\n",
              "      <td>0.005828</td>\n",
              "      <td>0.000993</td>\n",
              "      <td>-0.003935</td>\n",
              "      <td>0.007716</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>-0.002909</td>\n",
              "      <td>-0.002198</td>\n",
              "      <td>0.004929</td>\n",
              "      <td>-0.000091</td>\n",
              "      <td>0.005610</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>-0.003497</td>\n",
              "      <td>0.002445</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>-9.018644e-07</td>\n",
              "      <td>-4.730441e-07</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>-4.214114e-06</td>\n",
              "      <td>1.546327e-06</td>\n",
              "      <td>1.943857e-08</td>\n",
              "      <td>1.052508e-14</td>\n",
              "      <td>1.038516e-16</td>\n",
              "      <td>-3.633533e-17</td>\n",
              "      <td>2.367270e-17</td>\n",
              "      <td>3.136584e-17</td>\n",
              "      <td>2.701720e-17</td>\n",
              "      <td>6.242877e-17</td>\n",
              "      <td>-3.407114e-16</td>\n",
              "      <td>4.723279e-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.543423</td>\n",
              "      <td>-0.031737</td>\n",
              "      <td>-0.731471</td>\n",
              "      <td>-0.147769</td>\n",
              "      <td>0.220560</td>\n",
              "      <td>-0.020285</td>\n",
              "      <td>0.113929</td>\n",
              "      <td>0.128449</td>\n",
              "      <td>-0.049466</td>\n",
              "      <td>0.060202</td>\n",
              "      <td>-0.018380</td>\n",
              "      <td>-0.026028</td>\n",
              "      <td>-0.000234</td>\n",
              "      <td>-0.018275</td>\n",
              "      <td>0.051521</td>\n",
              "      <td>0.042006</td>\n",
              "      <td>-0.033595</td>\n",
              "      <td>-0.020154</td>\n",
              "      <td>0.068244</td>\n",
              "      <td>-0.009795</td>\n",
              "      <td>0.009078</td>\n",
              "      <td>0.001913</td>\n",
              "      <td>0.004309</td>\n",
              "      <td>-0.004630</td>\n",
              "      <td>-0.008054</td>\n",
              "      <td>-0.009773</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.001302</td>\n",
              "      <td>-0.002100</td>\n",
              "      <td>-0.002422</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>-0.000305</td>\n",
              "      <td>-0.001417</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>-0.000841</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>-0.001180</td>\n",
              "      <td>0.002067</td>\n",
              "      <td>-0.000559</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>-0.000283</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>-0.000678</td>\n",
              "      <td>-0.000625</td>\n",
              "      <td>-0.000242</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.001176</td>\n",
              "      <td>-0.000129</td>\n",
              "      <td>0.002343</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>-0.000033</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>-1.917587e-05</td>\n",
              "      <td>-5.732972e-06</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>2.466251e-06</td>\n",
              "      <td>-2.183444e-06</td>\n",
              "      <td>-2.336814e-09</td>\n",
              "      <td>3.831866e-15</td>\n",
              "      <td>-1.297701e-17</td>\n",
              "      <td>-1.395204e-17</td>\n",
              "      <td>-3.735473e-17</td>\n",
              "      <td>-1.202928e-17</td>\n",
              "      <td>-5.280530e-17</td>\n",
              "      <td>-2.172905e-17</td>\n",
              "      <td>7.176566e-17</td>\n",
              "      <td>-2.290972e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.339273</td>\n",
              "      <td>-1.189368</td>\n",
              "      <td>0.601774</td>\n",
              "      <td>0.283416</td>\n",
              "      <td>0.291701</td>\n",
              "      <td>-0.319752</td>\n",
              "      <td>-0.086461</td>\n",
              "      <td>0.136513</td>\n",
              "      <td>-0.095492</td>\n",
              "      <td>-0.262394</td>\n",
              "      <td>-0.048651</td>\n",
              "      <td>-0.068839</td>\n",
              "      <td>0.064652</td>\n",
              "      <td>0.020744</td>\n",
              "      <td>-0.102028</td>\n",
              "      <td>-0.081915</td>\n",
              "      <td>0.011500</td>\n",
              "      <td>0.032993</td>\n",
              "      <td>-0.088276</td>\n",
              "      <td>-0.042493</td>\n",
              "      <td>-0.013505</td>\n",
              "      <td>0.008438</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>-0.001501</td>\n",
              "      <td>-0.001269</td>\n",
              "      <td>0.001470</td>\n",
              "      <td>0.010144</td>\n",
              "      <td>-0.006001</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>-0.005995</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>-0.002780</td>\n",
              "      <td>-0.012747</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>-0.002630</td>\n",
              "      <td>-0.002152</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>-0.000360</td>\n",
              "      <td>0.000594</td>\n",
              "      <td>-0.001753</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>-0.000689</td>\n",
              "      <td>-0.010333</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.000538</td>\n",
              "      <td>-0.000046</td>\n",
              "      <td>-0.000515</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>-0.000179</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>2.666180e-05</td>\n",
              "      <td>5.584027e-06</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>3.646325e-07</td>\n",
              "      <td>1.228497e-06</td>\n",
              "      <td>-3.250384e-08</td>\n",
              "      <td>-5.747088e-16</td>\n",
              "      <td>-3.200169e-17</td>\n",
              "      <td>1.036441e-16</td>\n",
              "      <td>-3.193941e-17</td>\n",
              "      <td>1.613148e-17</td>\n",
              "      <td>-2.365744e-16</td>\n",
              "      <td>-2.654429e-17</td>\n",
              "      <td>6.107156e-17</td>\n",
              "      <td>-3.736141e-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.027563</td>\n",
              "      <td>0.074277</td>\n",
              "      <td>0.313869</td>\n",
              "      <td>-0.927671</td>\n",
              "      <td>0.064372</td>\n",
              "      <td>-0.080116</td>\n",
              "      <td>0.149591</td>\n",
              "      <td>0.047926</td>\n",
              "      <td>0.005484</td>\n",
              "      <td>-0.032374</td>\n",
              "      <td>-0.021931</td>\n",
              "      <td>-0.060036</td>\n",
              "      <td>0.162083</td>\n",
              "      <td>0.067017</td>\n",
              "      <td>-0.124509</td>\n",
              "      <td>0.060184</td>\n",
              "      <td>0.092839</td>\n",
              "      <td>0.373090</td>\n",
              "      <td>0.087042</td>\n",
              "      <td>0.046283</td>\n",
              "      <td>-0.022925</td>\n",
              "      <td>-0.000393</td>\n",
              "      <td>-0.022401</td>\n",
              "      <td>0.023295</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-0.004325</td>\n",
              "      <td>-0.003180</td>\n",
              "      <td>-0.012392</td>\n",
              "      <td>-0.006791</td>\n",
              "      <td>0.001316</td>\n",
              "      <td>-0.046596</td>\n",
              "      <td>0.003668</td>\n",
              "      <td>0.014998</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>-0.006541</td>\n",
              "      <td>0.017856</td>\n",
              "      <td>-0.002316</td>\n",
              "      <td>0.004102</td>\n",
              "      <td>-0.010374</td>\n",
              "      <td>-0.009482</td>\n",
              "      <td>0.021358</td>\n",
              "      <td>0.015693</td>\n",
              "      <td>-0.004578</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>-0.000621</td>\n",
              "      <td>-0.001203</td>\n",
              "      <td>-0.002519</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.003554</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>-0.009983</td>\n",
              "      <td>-0.001540</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>-3.920283e-05</td>\n",
              "      <td>-4.865488e-05</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-3.075872e-06</td>\n",
              "      <td>-4.961850e-07</td>\n",
              "      <td>-4.133798e-08</td>\n",
              "      <td>1.667893e-14</td>\n",
              "      <td>2.156031e-16</td>\n",
              "      <td>-2.756184e-17</td>\n",
              "      <td>-1.231978e-16</td>\n",
              "      <td>1.452452e-17</td>\n",
              "      <td>7.680812e-17</td>\n",
              "      <td>1.264552e-16</td>\n",
              "      <td>1.489382e-16</td>\n",
              "      <td>-6.247361e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.133226</td>\n",
              "      <td>1.057903</td>\n",
              "      <td>0.563300</td>\n",
              "      <td>-0.642817</td>\n",
              "      <td>0.326734</td>\n",
              "      <td>0.121149</td>\n",
              "      <td>-0.097008</td>\n",
              "      <td>0.239211</td>\n",
              "      <td>-0.048379</td>\n",
              "      <td>0.066934</td>\n",
              "      <td>-0.088637</td>\n",
              "      <td>0.068343</td>\n",
              "      <td>0.093091</td>\n",
              "      <td>-0.025788</td>\n",
              "      <td>0.032463</td>\n",
              "      <td>-0.008153</td>\n",
              "      <td>-0.009691</td>\n",
              "      <td>-0.090759</td>\n",
              "      <td>-0.026593</td>\n",
              "      <td>-0.019720</td>\n",
              "      <td>0.016730</td>\n",
              "      <td>0.032758</td>\n",
              "      <td>0.000538</td>\n",
              "      <td>-0.024243</td>\n",
              "      <td>0.035942</td>\n",
              "      <td>-0.034520</td>\n",
              "      <td>-0.004740</td>\n",
              "      <td>-0.008121</td>\n",
              "      <td>-0.005901</td>\n",
              "      <td>-0.001865</td>\n",
              "      <td>-0.038136</td>\n",
              "      <td>0.001607</td>\n",
              "      <td>-0.001499</td>\n",
              "      <td>-0.005495</td>\n",
              "      <td>-0.018810</td>\n",
              "      <td>-0.022658</td>\n",
              "      <td>0.005189</td>\n",
              "      <td>0.015983</td>\n",
              "      <td>0.006793</td>\n",
              "      <td>-0.000754</td>\n",
              "      <td>-0.000351</td>\n",
              "      <td>-0.005606</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>-0.000438</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.001275</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>-0.001840</td>\n",
              "      <td>-0.002094</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>-0.002213</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>-0.000175</td>\n",
              "      <td>-0.000433</td>\n",
              "      <td>3.655272e-05</td>\n",
              "      <td>-8.388841e-06</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>1.927599e-06</td>\n",
              "      <td>-1.467781e-06</td>\n",
              "      <td>-1.146983e-08</td>\n",
              "      <td>2.791725e-14</td>\n",
              "      <td>-1.347347e-16</td>\n",
              "      <td>-1.394735e-18</td>\n",
              "      <td>1.611766e-16</td>\n",
              "      <td>9.021057e-18</td>\n",
              "      <td>2.754720e-16</td>\n",
              "      <td>-1.010543e-16</td>\n",
              "      <td>1.335301e-16</td>\n",
              "      <td>-1.948899e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.542142</td>\n",
              "      <td>-0.027113</td>\n",
              "      <td>-0.730577</td>\n",
              "      <td>-0.157356</td>\n",
              "      <td>0.209539</td>\n",
              "      <td>-0.023888</td>\n",
              "      <td>0.131447</td>\n",
              "      <td>0.125423</td>\n",
              "      <td>-0.055098</td>\n",
              "      <td>0.038532</td>\n",
              "      <td>0.049978</td>\n",
              "      <td>-0.044176</td>\n",
              "      <td>0.007721</td>\n",
              "      <td>-0.021275</td>\n",
              "      <td>-0.046795</td>\n",
              "      <td>-0.008685</td>\n",
              "      <td>0.028828</td>\n",
              "      <td>0.014336</td>\n",
              "      <td>-0.058990</td>\n",
              "      <td>0.014035</td>\n",
              "      <td>0.004117</td>\n",
              "      <td>0.001508</td>\n",
              "      <td>-0.000467</td>\n",
              "      <td>-0.002735</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.003441</td>\n",
              "      <td>0.001342</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>-0.002098</td>\n",
              "      <td>0.001439</td>\n",
              "      <td>-0.000774</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.001606</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>-0.003040</td>\n",
              "      <td>0.006401</td>\n",
              "      <td>-0.002803</td>\n",
              "      <td>0.005003</td>\n",
              "      <td>-0.000030</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>-0.001766</td>\n",
              "      <td>-0.000409</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.001126</td>\n",
              "      <td>-0.000486</td>\n",
              "      <td>-0.000535</td>\n",
              "      <td>-0.000256</td>\n",
              "      <td>0.001680</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>-0.000160</td>\n",
              "      <td>0.002784</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000028</td>\n",
              "      <td>-0.000069</td>\n",
              "      <td>-5.148805e-06</td>\n",
              "      <td>-4.049300e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>1.896746e-06</td>\n",
              "      <td>5.031553e-07</td>\n",
              "      <td>-9.939018e-09</td>\n",
              "      <td>9.355678e-15</td>\n",
              "      <td>1.457957e-17</td>\n",
              "      <td>1.416309e-17</td>\n",
              "      <td>-3.609366e-17</td>\n",
              "      <td>-9.514891e-18</td>\n",
              "      <td>-5.117130e-17</td>\n",
              "      <td>-3.327911e-17</td>\n",
              "      <td>7.365212e-17</td>\n",
              "      <td>-3.337571e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.330862</td>\n",
              "      <td>-1.231548</td>\n",
              "      <td>0.670640</td>\n",
              "      <td>0.356405</td>\n",
              "      <td>0.367697</td>\n",
              "      <td>-0.328396</td>\n",
              "      <td>-0.068371</td>\n",
              "      <td>0.130929</td>\n",
              "      <td>-0.153988</td>\n",
              "      <td>-0.108983</td>\n",
              "      <td>-0.088796</td>\n",
              "      <td>-0.136223</td>\n",
              "      <td>0.155747</td>\n",
              "      <td>0.049398</td>\n",
              "      <td>-0.000454</td>\n",
              "      <td>-0.013008</td>\n",
              "      <td>-0.073254</td>\n",
              "      <td>-0.018238</td>\n",
              "      <td>0.021573</td>\n",
              "      <td>-0.049733</td>\n",
              "      <td>-0.004139</td>\n",
              "      <td>0.014088</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.001911</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.005498</td>\n",
              "      <td>0.008182</td>\n",
              "      <td>-0.003181</td>\n",
              "      <td>-0.010247</td>\n",
              "      <td>-0.003669</td>\n",
              "      <td>0.006725</td>\n",
              "      <td>-0.002321</td>\n",
              "      <td>-0.004819</td>\n",
              "      <td>-0.000864</td>\n",
              "      <td>-0.002742</td>\n",
              "      <td>-0.000551</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>-0.000147</td>\n",
              "      <td>0.001741</td>\n",
              "      <td>0.003789</td>\n",
              "      <td>0.004595</td>\n",
              "      <td>-0.002231</td>\n",
              "      <td>0.018362</td>\n",
              "      <td>-0.000771</td>\n",
              "      <td>0.001194</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>-0.000941</td>\n",
              "      <td>-0.003306</td>\n",
              "      <td>-0.000286</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>-0.000283</td>\n",
              "      <td>-5.458528e-05</td>\n",
              "      <td>-1.080140e-06</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>-3.745297e-06</td>\n",
              "      <td>-1.329458e-06</td>\n",
              "      <td>-1.779350e-08</td>\n",
              "      <td>-7.023010e-15</td>\n",
              "      <td>-7.994088e-17</td>\n",
              "      <td>9.888875e-17</td>\n",
              "      <td>-1.401204e-17</td>\n",
              "      <td>1.062894e-17</td>\n",
              "      <td>-2.266271e-16</td>\n",
              "      <td>-2.031842e-17</td>\n",
              "      <td>6.833888e-17</td>\n",
              "      <td>-3.416656e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.319516</td>\n",
              "      <td>-1.243466</td>\n",
              "      <td>0.689304</td>\n",
              "      <td>0.389183</td>\n",
              "      <td>0.385392</td>\n",
              "      <td>-0.330652</td>\n",
              "      <td>-0.060033</td>\n",
              "      <td>0.131780</td>\n",
              "      <td>-0.170965</td>\n",
              "      <td>-0.064192</td>\n",
              "      <td>-0.078393</td>\n",
              "      <td>-0.159791</td>\n",
              "      <td>0.190675</td>\n",
              "      <td>0.061107</td>\n",
              "      <td>-0.003329</td>\n",
              "      <td>-0.002713</td>\n",
              "      <td>-0.077876</td>\n",
              "      <td>-0.022830</td>\n",
              "      <td>0.018265</td>\n",
              "      <td>-0.046581</td>\n",
              "      <td>-0.002261</td>\n",
              "      <td>0.017873</td>\n",
              "      <td>0.000684</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.001870</td>\n",
              "      <td>0.004971</td>\n",
              "      <td>0.012179</td>\n",
              "      <td>-0.000723</td>\n",
              "      <td>-0.012564</td>\n",
              "      <td>0.005424</td>\n",
              "      <td>0.009088</td>\n",
              "      <td>-0.001701</td>\n",
              "      <td>0.005784</td>\n",
              "      <td>-0.004153</td>\n",
              "      <td>-0.001971</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>-0.000248</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>-0.000351</td>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>0.003990</td>\n",
              "      <td>-0.002068</td>\n",
              "      <td>0.016302</td>\n",
              "      <td>-0.000181</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>-0.000729</td>\n",
              "      <td>-0.002970</td>\n",
              "      <td>-0.000265</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>-0.000179</td>\n",
              "      <td>-4.141140e-05</td>\n",
              "      <td>-2.711977e-07</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>-3.053145e-06</td>\n",
              "      <td>-9.882648e-07</td>\n",
              "      <td>-4.903098e-09</td>\n",
              "      <td>-1.203289e-14</td>\n",
              "      <td>-8.861449e-17</td>\n",
              "      <td>1.040929e-16</td>\n",
              "      <td>-1.054260e-17</td>\n",
              "      <td>1.019526e-17</td>\n",
              "      <td>-2.266271e-16</td>\n",
              "      <td>-2.221577e-17</td>\n",
              "      <td>6.546574e-17</td>\n",
              "      <td>-4.067177e-18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...            68            69            70\n",
              "0 -0.528423 -0.054268 -0.650975  ... -2.199646e-17  9.685354e-17 -1.851814e-18\n",
              "1  1.739488 -0.212510  0.025645  ...  1.297594e-16  2.321866e-17  2.516472e-18\n",
              "2  1.045422  0.851608 -0.133830  ...  6.242877e-17 -3.407114e-16  4.723279e-20\n",
              "3 -0.543423 -0.031737 -0.731471  ... -2.172905e-17  7.176566e-17 -2.290972e-18\n",
              "4 -0.339273 -1.189368  0.601774  ... -2.654429e-17  6.107156e-17 -3.736141e-19\n",
              "5  2.027563  0.074277  0.313869  ...  1.264552e-16  1.489382e-16 -6.247361e-18\n",
              "6 -0.133226  1.057903  0.563300  ... -1.010543e-16  1.335301e-16 -1.948899e-18\n",
              "7 -0.542142 -0.027113 -0.730577  ... -3.327911e-17  7.365212e-17 -3.337571e-18\n",
              "8 -0.330862 -1.231548  0.670640  ... -2.031842e-17  6.833888e-17 -3.416656e-18\n",
              "9 -0.319516 -1.243466  0.689304  ... -2.221577e-17  6.546574e-17 -4.067177e-18\n",
              "\n",
              "[10 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIDYI_dXhwA"
      },
      "source": [
        "## Training and Making Predictions\n",
        "\n",
        "In this case we'll use random forest classification for making the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swXJ6_ydXhwA"
      },
      "source": [
        "pca = PCA(n_components=12)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Yt7PnkXhwA"
      },
      "source": [
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test_pca)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7X48ybRXhwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a450ee-c1ec-42da-a343-c4ff22068e9b"
      },
      "source": [
        "print('Accuracy:%f' %accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:%f\" %metrics.average_precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:%f\" %metrics.recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-score:%f\" %metrics.f1_score(y_test, y_pred,average='weighted'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.514412\n",
            "Precision:0.586086\n",
            "Recall:0.514412\n",
            "F1-score:0.607775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tfibn9gAXhwB",
        "outputId": "be99767f-51c7-4306-cdb3-c104d7f00ce7"
      },
      "source": [
        "#The confusion matrix takes a vector of labels (not the one-hot encoding). \n",
        "\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[138946      0      0      0      0      0      0      0      0      0\n",
            "     189      0      0      0      0]\n",
            " [   489      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [ 31990      0      0      0      0      0      0      0      0      0\n",
            "      16      0      0      0      0]\n",
            " [  2573      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [ 57531      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [  1373      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]\n",
            " [  1444      0      0      0      0      0      0      0      0      0\n",
            "       5      0      0      0      0]\n",
            " [  1983      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     2      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     9      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [  1667      0      0      0      0      0      0      0      0      0\n",
            "   38034      0      0      0      0]\n",
            " [  1474      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   376      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     5      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   162      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxRNywIXXhwC"
      },
      "source": [
        "Get the attacks' names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9AlIiTKXhwC"
      },
      "source": [
        "labels_d = make_value2index(df_test['Label'])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1vqAhE1XhwC",
        "outputId": "4e811ba0-909e-48b6-82c2-20271fe164c7"
      },
      "source": [
        "print(labels_d)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'BENIGN': 139134, 'Bot': 139623, 'DDoS': 171629, 'DoS GoldenEye': 174202, 'DoS Hulk': 231733, 'DoS Slowhttptest': 233107, 'DoS slowloris': 234556, 'FTP-Patator': 236539, 'Heartbleed': 236542, 'Infiltration': 236551, 'PortScan': 276252, 'SSH-Patator': 277726, 'Web Attack � Brute Force': 278102, 'Web Attack � Sql Injection': 278107, 'Web Attack � XSS': 278270}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO4EmilxXhwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc82f148-292c-4cc2-81a5-6bccbc849dcc"
      },
      "source": [
        "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1), target_names=labels_d))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.59      1.00      0.75    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.86      0.44      0.58     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.38      0.55     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.64    278271\n",
            "                 macro avg       0.16      0.12      0.13    278271\n",
            "              weighted avg       0.62      0.64      0.57    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtGAy_3gY2n"
      },
      "source": [
        "# Model : Naive Bayes model (GaussianNB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxNz3yQch35o"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izw8QWLIgcTx",
        "outputId": "8901ee0d-6144-4fa4-f991-3cf7ecf7302f"
      },
      "source": [
        "model_gaussian = GaussianNB()\r\n",
        "model_gaussian.fit(X_train, y_train_ada)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c15lYNIdgs9V"
      },
      "source": [
        "# make predictions\r\n",
        "y_pred = model_gaussian.predict(X_test)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ6JwTSDg6iE",
        "outputId": "e9571be8-1673-4b42-cc04-cb60d1a54f02"
      },
      "source": [
        "display_metrics(y_test_ada, y_pred, labels_d)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.50\n",
            "\n",
            "Micro Precision: 0.50\n",
            "Micro Recall: 0.50\n",
            "Micro F1-score: 0.50\n",
            "\n",
            "Macro Precision: 0.03\n",
            "Macro Recall: 0.07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.04\n",
            "\n",
            "Weighted Precision: 0.25\n",
            "Weighted Recall: 0.50\n",
            "Weighted F1-score: 0.33\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.50      1.00      0.67    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.00      0.00      0.00     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.50    278271\n",
            "                 macro avg       0.03      0.07      0.04    278271\n",
            "              weighted avg       0.25      0.50      0.33    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXea3TMVXhwD"
      },
      "source": [
        "# Model 3: Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOUaeJwiXhwD"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRMY85VyXhwE"
      },
      "source": [
        "model_dec = DecisionTreeClassifier()\n",
        "model_dec.fit(X_train, y_train_ada)\n",
        "y_pred = model_dec.predict(X_test)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFQ6iCJjXhwE",
        "outputId": "2774638f-59d0-487f-d178-2908b2ef9873"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\n",
        " \n",
        "print('\\nClassification Report\\n')\n",
        "\n",
        "print(classification_report(y_test_ada , y_pred, target_names=labels_d))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.48\n",
            "\n",
            "Micro Precision: 0.48\n",
            "Micro Recall: 0.48\n",
            "Micro F1-score: 0.48\n",
            "\n",
            "Macro Precision: 0.22\n",
            "Macro Recall: 0.16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.15\n",
            "\n",
            "Weighted Precision: 0.35\n",
            "Weighted Recall: 0.48\n",
            "Weighted F1-score: 0.39\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.60      0.93      0.73    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.21      0.23      0.22      2573\n",
            "                  DoS Hulk       0.00      0.00      0.00     57531\n",
            "          DoS Slowhttptest       0.55      0.15      0.23      1374\n",
            "             DoS slowloris       0.99      0.07      0.13      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.60      1.00      0.75         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.29      0.10      0.15     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.48    278271\n",
            "                 macro avg       0.22      0.16      0.15    278271\n",
            "              weighted avg       0.35      0.48      0.39    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABYQjuBCXhwE"
      },
      "source": [
        "# Model 4: Random Foresty with DecisionTree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI6QGw9LXhwF"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "clf.fit(X_train,y_train)\n",
        "    \n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFh6H0d3lNmz",
        "outputId": "ca0c7f25-93ab-4155-f095-2cabe02b2a81"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, np.argmax(y_pred, axis = 1))))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')\r\n",
        "\r\n",
        "print(classification_report(y_test_ada ,np.argmax(y_pred, axis = 1), target_names=labels_d))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.50\n",
            "\n",
            "Micro Precision: 0.50\n",
            "Micro Recall: 0.50\n",
            "Micro F1-score: 0.50\n",
            "\n",
            "Macro Precision: 0.10\n",
            "Macro Recall: 0.07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.05\n",
            "\n",
            "Weighted Precision: 0.46\n",
            "Weighted Recall: 0.50\n",
            "Weighted F1-score: 0.34\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.50      1.00      0.67    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       1.00      0.01      0.02     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.50    278271\n",
            "                 macro avg       0.10      0.07      0.05    278271\n",
            "              weighted avg       0.46      0.50      0.34    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xi2INVUk9p9"
      },
      "source": [
        "# Model 5: Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_AoQZpOnxuD"
      },
      "source": [
        "from sklearn import linear_model"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqJDnChnmxJ"
      },
      "source": [
        "attack_classifier = linear_model.LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=10000)\r\n",
        "attack_classifier.fit(X_train, y_train_ada)\r\n",
        "\r\n",
        "y_pred = attack_classifier.predict(X_test)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3x76HdDorw7",
        "outputId": "46adf99a-f5d7-43f7-8489-f07f0d6ebd2b"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')\r\n",
        "\r\n",
        "print(classification_report(y_test_ada , y_pred, target_names=labels_d))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.82\n",
            "\n",
            "Micro Precision: 0.82\n",
            "Micro Recall: 0.82\n",
            "Micro F1-score: 0.82\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.34\n",
            "Macro Recall: 0.39\n",
            "Macro F1-score: 0.32\n",
            "\n",
            "Weighted Precision: 0.86\n",
            "Weighted Recall: 0.82\n",
            "Weighted F1-score: 0.83\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.90      0.85      0.88    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.62      1.00      0.76     32006\n",
            "             DoS GoldenEye       0.33      0.20      0.25      2573\n",
            "                  DoS Hulk       0.92      0.63      0.75     57531\n",
            "          DoS Slowhttptest       0.13      0.07      0.09      1374\n",
            "             DoS slowloris       0.35      0.36      0.36      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.96      1.00      0.98     39701\n",
            "               SSH-Patator       0.79      0.50      0.62      1474\n",
            "  Web Attack � Brute Force       0.07      0.40      0.11       376\n",
            "Web Attack � Sql Injection       0.00      0.80      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.82    278271\n",
            "                 macro avg       0.34      0.39      0.32    278271\n",
            "              weighted avg       0.86      0.82      0.83    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXJKq1zXhwF"
      },
      "source": [
        "# Model 6: AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqqrLskIXhwF"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT8egNOEXhwG"
      },
      "source": [
        "model_ada = AdaBoostClassifier(n_estimators=100)\n",
        "model_ada.fit(X_train, y_train_ada)\n",
        "\n",
        "# make predictions\n",
        "expected = y_test_ada\n",
        "predicted = model_ada.predict(X_test)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJCcrCS4XhwG",
        "outputId": "afe4d6f3-be93-4d5a-e5ba-dd5ba5bc7e5b"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\n",
        " \n",
        "print('\\nClassification Report\\n')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.82\n",
            "\n",
            "Micro Precision: 0.82\n",
            "Micro Recall: 0.82\n",
            "Micro F1-score: 0.82\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.34\n",
            "Macro Recall: 0.39\n",
            "Macro F1-score: 0.32\n",
            "\n",
            "Weighted Precision: 0.86\n",
            "Weighted Recall: 0.82\n",
            "Weighted F1-score: 0.83\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_lvcZVXXhwG",
        "outputId": "cea0ed11-608a-4c22-fac9-f683ad2dd687"
      },
      "source": [
        "print(classification_report(y_test_ada, predicted, target_names=labels_d))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.60      0.99      0.75    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.58      0.48      0.52     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      1.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.59    278271\n",
            "                 macro avg       0.08      0.16      0.09    278271\n",
            "              weighted avg       0.42      0.59      0.48    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn7bQX_Gvusc"
      },
      "source": [
        "# Model 6: XGBClassifier (very slow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ttK187BvuLn"
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "53QHIzuAv5z9",
        "outputId": "0e392ce3-2300-4f4f-a1e0-44bdc7723ac0"
      },
      "source": [
        "xgboostc = XGBClassifier(learning_rate = 0.1, max_depth = 5,n_estimators = 1165, subsample=0.8,colsample_bytree=0.8,seed=27)\r\n",
        "xgboostc.fit(X_train,y_train)\r\n",
        "    \r\n",
        "y_pred = xgboostc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-07eedb02b743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgboostc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1165\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgboostc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboostc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz8IN3Jfwav0"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KznBGxa2yiLk"
      },
      "source": [
        "# Model 7: Voting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfa2lmvytnh"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIYUdP7iyuRG",
        "outputId": "0c441c66-dea8-4b7b-caec-d1126379805f"
      },
      "source": [
        "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=35, criterion=\"entropy\")\r\n",
        "ada = AdaBoostClassifier(n_estimators=75, learning_rate=1.5)\r\n",
        "etc = ExtraTreesClassifier(n_jobs=-1, criterion=\"entropy\", n_estimators=5)\r\n",
        "eclf = VotingClassifier(estimators=[('ada', ada), ('rfc', rfc), ('etc', etc)], voting='soft', weights=[2, 1, 3],n_jobs=1)\r\n",
        "eclf.fit(X_train,y_train_ada)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('ada',\n",
              "                              AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                                                 base_estimator=None,\n",
              "                                                 learning_rate=1.5,\n",
              "                                                 n_estimators=75,\n",
              "                                                 random_state=None)),\n",
              "                             ('rfc',\n",
              "                              RandomForestClassifier(bootstrap=True,\n",
              "                                                     ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     criterion='entropy',\n",
              "                                                     max_depth=None,\n",
              "                                                     max_features='auto',\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     max_samples=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_im...\n",
              "                                                   criterion='entropy',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   max_samples=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=5, n_jobs=-1,\n",
              "                                                   oob_score=False,\n",
              "                                                   random_state=None, verbose=0,\n",
              "                                                   warm_start=False))],\n",
              "                 flatten_transform=True, n_jobs=1, voting='soft',\n",
              "                 weights=[2, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rig3c_bqzpim"
      },
      "source": [
        "y_pred = eclf.predict(X_test)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuI-C6lZzqeV",
        "outputId": "66239401-17ee-42de-d926-a6e218fd7b08"
      },
      "source": [
        "display_metrics(y_test_ada, y_pred,labels_d)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.61\n",
            "\n",
            "Micro Precision: 0.61\n",
            "Micro Recall: 0.61\n",
            "Micro F1-score: 0.61\n",
            "\n",
            "Macro Precision: 0.36\n",
            "Macro Recall: 0.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.20\n",
            "\n",
            "Weighted Precision: 0.60\n",
            "Weighted Recall: 0.61\n",
            "Weighted F1-score: 0.52\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.56      1.00      0.72    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       1.00      0.42      0.59     32006\n",
            "             DoS GoldenEye       0.94      0.08      0.14      2573\n",
            "                  DoS Hulk       0.90      0.28      0.43     57531\n",
            "          DoS Slowhttptest       0.97      0.05      0.09      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       1.00      1.00      1.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.61    278271\n",
            "                 macro avg       0.36      0.19      0.20    278271\n",
            "              weighted avg       0.60      0.61      0.52    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAuDnGKYXhwH"
      },
      "source": [
        "# Model 8: KNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7fba3ZuXhwH"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgx8g7vuXhwH"
      },
      "source": [
        "features_order = ['Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min']"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyhoOfFgXhwH"
      },
      "source": [
        "features=[\"Fwd Packet Length Max\",\"Flow IAT Std\",\"Fwd Packet Length Std\" ,\"Fwd IAT Total\",'Flow Packets/s', \"Fwd Packet Length Mean\",  \"Flow Bytes/s\",  \"Flow IAT Mean\", \"Bwd Packet Length Mean\",  \"Flow IAT Max\", \"Bwd Packet Length Std\", ]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "GlhN1BDdXhwI",
        "outputId": "10debfb4-b67c-4e5c-be61-d2e414a8d67e"
      },
      "source": [
        "df_knn_train = pd.DataFrame(X_train, columns = features_order)\n",
        "df_knn_train.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.440388</td>\n",
              "      <td>-0.399652</td>\n",
              "      <td>1.912389</td>\n",
              "      <td>-0.564734</td>\n",
              "      <td>-0.008664</td>\n",
              "      <td>-0.007233</td>\n",
              "      <td>-0.043168</td>\n",
              "      <td>-0.006377</td>\n",
              "      <td>-0.138669</td>\n",
              "      <td>1.352672</td>\n",
              "      <td>0.202439</td>\n",
              "      <td>-0.246600</td>\n",
              "      <td>-0.539960</td>\n",
              "      <td>2.376337</td>\n",
              "      <td>-0.456260</td>\n",
              "      <td>-0.561373</td>\n",
              "      <td>0.184633</td>\n",
              "      <td>-0.147382</td>\n",
              "      <td>-0.381429</td>\n",
              "      <td>-0.520573</td>\n",
              "      <td>-0.523283</td>\n",
              "      <td>-0.055856</td>\n",
              "      <td>-0.551962</td>\n",
              "      <td>-0.397978</td>\n",
              "      <td>-0.490145</td>\n",
              "      <td>-0.512693</td>\n",
              "      <td>-0.103626</td>\n",
              "      <td>-0.366432</td>\n",
              "      <td>-0.238771</td>\n",
              "      <td>-0.299101</td>\n",
              "      <td>-0.321189</td>\n",
              "      <td>-0.098360</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.010990</td>\n",
              "      <td>-0.009186</td>\n",
              "      <td>-0.182966</td>\n",
              "      <td>0.179785</td>\n",
              "      <td>6.971585</td>\n",
              "      <td>-0.586699</td>\n",
              "      <td>-0.332459</td>\n",
              "      <td>-0.641775</td>\n",
              "      <td>-0.481515</td>\n",
              "      <td>-0.244129</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>-0.750640</td>\n",
              "      <td>-0.769272</td>\n",
              "      <td>-0.320901</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>0.184487</td>\n",
              "      <td>-0.284095</td>\n",
              "      <td>0.202439</td>\n",
              "      <td>-0.456260</td>\n",
              "      <td>-0.008664</td>\n",
              "      <td>-0.043168</td>\n",
              "      <td>-0.007233</td>\n",
              "      <td>-0.006376</td>\n",
              "      <td>-0.506005</td>\n",
              "      <td>-0.175389</td>\n",
              "      <td>-0.006338</td>\n",
              "      <td>-0.863266</td>\n",
              "      <td>-0.151069</td>\n",
              "      <td>-0.090907</td>\n",
              "      <td>-0.163927</td>\n",
              "      <td>-0.132944</td>\n",
              "      <td>-0.499660</td>\n",
              "      <td>-0.166099</td>\n",
              "      <td>-0.510466</td>\n",
              "      <td>-0.476370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.323975</td>\n",
              "      <td>-0.417829</td>\n",
              "      <td>-0.521977</td>\n",
              "      <td>1.673237</td>\n",
              "      <td>-0.002432</td>\n",
              "      <td>-0.002611</td>\n",
              "      <td>-0.039269</td>\n",
              "      <td>-0.000550</td>\n",
              "      <td>0.018977</td>\n",
              "      <td>-0.176083</td>\n",
              "      <td>-0.108088</td>\n",
              "      <td>0.033759</td>\n",
              "      <td>0.926443</td>\n",
              "      <td>-0.442726</td>\n",
              "      <td>1.569069</td>\n",
              "      <td>0.837688</td>\n",
              "      <td>-0.041403</td>\n",
              "      <td>-0.264267</td>\n",
              "      <td>1.094896</td>\n",
              "      <td>2.011935</td>\n",
              "      <td>2.006133</td>\n",
              "      <td>-0.055856</td>\n",
              "      <td>1.679926</td>\n",
              "      <td>1.342471</td>\n",
              "      <td>2.225140</td>\n",
              "      <td>1.996232</td>\n",
              "      <td>-0.103626</td>\n",
              "      <td>-0.361494</td>\n",
              "      <td>-0.235278</td>\n",
              "      <td>-0.292569</td>\n",
              "      <td>-0.315330</td>\n",
              "      <td>-0.098358</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>-0.243475</td>\n",
              "      <td>-0.184752</td>\n",
              "      <td>-0.483785</td>\n",
              "      <td>0.811228</td>\n",
              "      <td>1.406812</td>\n",
              "      <td>0.944032</td>\n",
              "      <td>0.376370</td>\n",
              "      <td>-0.244129</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>-0.750640</td>\n",
              "      <td>1.299931</td>\n",
              "      <td>-0.320901</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>0.184487</td>\n",
              "      <td>1.373365</td>\n",
              "      <td>-0.108088</td>\n",
              "      <td>1.569069</td>\n",
              "      <td>-0.002432</td>\n",
              "      <td>-0.039269</td>\n",
              "      <td>-0.002611</td>\n",
              "      <td>-0.000551</td>\n",
              "      <td>-0.484444</td>\n",
              "      <td>-0.138100</td>\n",
              "      <td>-0.006338</td>\n",
              "      <td>1.006365</td>\n",
              "      <td>-0.150378</td>\n",
              "      <td>-0.090907</td>\n",
              "      <td>-0.163410</td>\n",
              "      <td>-0.132202</td>\n",
              "      <td>2.104247</td>\n",
              "      <td>-0.166099</td>\n",
              "      <td>2.011629</td>\n",
              "      <td>2.146912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.503820</td>\n",
              "      <td>-0.396475</td>\n",
              "      <td>-0.521977</td>\n",
              "      <td>2.518296</td>\n",
              "      <td>0.017819</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.088178</td>\n",
              "      <td>-0.004139</td>\n",
              "      <td>0.351963</td>\n",
              "      <td>-0.176083</td>\n",
              "      <td>-0.066286</td>\n",
              "      <td>0.095246</td>\n",
              "      <td>-0.099794</td>\n",
              "      <td>-0.442726</td>\n",
              "      <td>-0.329481</td>\n",
              "      <td>-0.174671</td>\n",
              "      <td>-0.041407</td>\n",
              "      <td>-0.264266</td>\n",
              "      <td>0.257756</td>\n",
              "      <td>0.834021</td>\n",
              "      <td>1.219203</td>\n",
              "      <td>-0.055857</td>\n",
              "      <td>2.528305</td>\n",
              "      <td>0.268773</td>\n",
              "      <td>0.853281</td>\n",
              "      <td>1.215678</td>\n",
              "      <td>-0.103626</td>\n",
              "      <td>3.669165</td>\n",
              "      <td>0.652311</td>\n",
              "      <td>1.934804</td>\n",
              "      <td>2.275044</td>\n",
              "      <td>-0.098360</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.031658</td>\n",
              "      <td>0.020320</td>\n",
              "      <td>-0.243474</td>\n",
              "      <td>-0.184750</td>\n",
              "      <td>-0.483785</td>\n",
              "      <td>-0.167321</td>\n",
              "      <td>-0.332036</td>\n",
              "      <td>-0.258665</td>\n",
              "      <td>-0.431330</td>\n",
              "      <td>-0.244129</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>1.332197</td>\n",
              "      <td>-0.769272</td>\n",
              "      <td>-0.320901</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>-0.804627</td>\n",
              "      <td>-0.359807</td>\n",
              "      <td>-0.066286</td>\n",
              "      <td>-0.329481</td>\n",
              "      <td>0.017819</td>\n",
              "      <td>0.088178</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>-0.004139</td>\n",
              "      <td>1.992423</td>\n",
              "      <td>-0.146632</td>\n",
              "      <td>0.005086</td>\n",
              "      <td>1.006365</td>\n",
              "      <td>0.540276</td>\n",
              "      <td>1.514512</td>\n",
              "      <td>0.774940</td>\n",
              "      <td>0.002619</td>\n",
              "      <td>1.278915</td>\n",
              "      <td>-0.074446</td>\n",
              "      <td>1.226977</td>\n",
              "      <td>1.303166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.979540</td>\n",
              "      <td>-0.419417</td>\n",
              "      <td>1.912389</td>\n",
              "      <td>-0.563934</td>\n",
              "      <td>-0.010222</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.077527</td>\n",
              "      <td>-0.006497</td>\n",
              "      <td>-0.233578</td>\n",
              "      <td>0.252714</td>\n",
              "      <td>-0.149890</td>\n",
              "      <td>-0.246600</td>\n",
              "      <td>-0.564511</td>\n",
              "      <td>1.165697</td>\n",
              "      <td>-0.536380</td>\n",
              "      <td>-0.561373</td>\n",
              "      <td>-0.041206</td>\n",
              "      <td>-0.264020</td>\n",
              "      <td>-0.375607</td>\n",
              "      <td>-0.520580</td>\n",
              "      <td>-0.522377</td>\n",
              "      <td>-0.047936</td>\n",
              "      <td>-0.551962</td>\n",
              "      <td>-0.397978</td>\n",
              "      <td>-0.490145</td>\n",
              "      <td>-0.512694</td>\n",
              "      <td>-0.103626</td>\n",
              "      <td>-0.366433</td>\n",
              "      <td>-0.238772</td>\n",
              "      <td>-0.299101</td>\n",
              "      <td>-0.321189</td>\n",
              "      <td>-0.098360</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.012471</td>\n",
              "      <td>-0.010339</td>\n",
              "      <td>-0.243347</td>\n",
              "      <td>-0.183983</td>\n",
              "      <td>1.620185</td>\n",
              "      <td>-0.610444</td>\n",
              "      <td>-0.566964</td>\n",
              "      <td>-0.613625</td>\n",
              "      <td>-0.481233</td>\n",
              "      <td>-0.244129</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>-0.750640</td>\n",
              "      <td>-0.769272</td>\n",
              "      <td>-0.320901</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>0.184487</td>\n",
              "      <td>-0.518557</td>\n",
              "      <td>-0.149890</td>\n",
              "      <td>-0.536380</td>\n",
              "      <td>-0.010222</td>\n",
              "      <td>-0.077527</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.006497</td>\n",
              "      <td>-0.506005</td>\n",
              "      <td>-0.175389</td>\n",
              "      <td>-0.007970</td>\n",
              "      <td>-0.863266</td>\n",
              "      <td>-0.151069</td>\n",
              "      <td>-0.090907</td>\n",
              "      <td>-0.163927</td>\n",
              "      <td>-0.132944</td>\n",
              "      <td>-0.499660</td>\n",
              "      <td>-0.166099</td>\n",
              "      <td>-0.510466</td>\n",
              "      <td>-0.476370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.973754</td>\n",
              "      <td>1.766353</td>\n",
              "      <td>-0.521977</td>\n",
              "      <td>-0.564737</td>\n",
              "      <td>-0.010222</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.083132</td>\n",
              "      <td>-0.006545</td>\n",
              "      <td>-0.270577</td>\n",
              "      <td>-0.176083</td>\n",
              "      <td>-0.287238</td>\n",
              "      <td>-0.246600</td>\n",
              "      <td>-0.597129</td>\n",
              "      <td>-0.442726</td>\n",
              "      <td>-0.642826</td>\n",
              "      <td>-0.561373</td>\n",
              "      <td>-0.041410</td>\n",
              "      <td>-0.109216</td>\n",
              "      <td>-0.381428</td>\n",
              "      <td>-0.520580</td>\n",
              "      <td>-0.523285</td>\n",
              "      <td>-0.055845</td>\n",
              "      <td>-0.551962</td>\n",
              "      <td>-0.397978</td>\n",
              "      <td>-0.490145</td>\n",
              "      <td>-0.512694</td>\n",
              "      <td>-0.103626</td>\n",
              "      <td>-0.366433</td>\n",
              "      <td>-0.238772</td>\n",
              "      <td>-0.299101</td>\n",
              "      <td>-0.321189</td>\n",
              "      <td>-0.098360</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.011582</td>\n",
              "      <td>-0.009647</td>\n",
              "      <td>-0.163209</td>\n",
              "      <td>0.298818</td>\n",
              "      <td>-0.483785</td>\n",
              "      <td>-0.641546</td>\n",
              "      <td>-0.708833</td>\n",
              "      <td>-0.642355</td>\n",
              "      <td>-0.481515</td>\n",
              "      <td>-0.244129</td>\n",
              "      <td>-0.19009</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>-0.750640</td>\n",
              "      <td>1.299931</td>\n",
              "      <td>3.116228</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.01089</td>\n",
              "      <td>0.184487</td>\n",
              "      <td>-0.712198</td>\n",
              "      <td>-0.287238</td>\n",
              "      <td>-0.642826</td>\n",
              "      <td>-0.010222</td>\n",
              "      <td>-0.083132</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.006545</td>\n",
              "      <td>-0.477514</td>\n",
              "      <td>-0.139048</td>\n",
              "      <td>-0.007970</td>\n",
              "      <td>1.006365</td>\n",
              "      <td>-0.151069</td>\n",
              "      <td>-0.090907</td>\n",
              "      <td>-0.163927</td>\n",
              "      <td>-0.132944</td>\n",
              "      <td>-0.499660</td>\n",
              "      <td>-0.166099</td>\n",
              "      <td>-0.510466</td>\n",
              "      <td>-0.476370</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Source Port  Destination Port  Protocol  ...  Idle Std  Idle Max  Idle Min\n",
              "0     0.440388         -0.399652  1.912389  ... -0.166099 -0.510466 -0.476370\n",
              "1     0.323975         -0.417829 -0.521977  ... -0.166099  2.011629  2.146912\n",
              "2     0.503820         -0.396475 -0.521977  ... -0.074446  1.226977  1.303166\n",
              "3     0.979540         -0.419417  1.912389  ... -0.166099 -0.510466 -0.476370\n",
              "4    -1.973754          1.766353 -0.521977  ... -0.166099 -0.510466 -0.476370\n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eda-FM60XhwI"
      },
      "source": [
        "df_knn_test = pd.DataFrame(X_test, columns = features_order)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "iv6nljzO5_Wd",
        "outputId": "1bbee8c8-92ff-446f-92d0-9199b0f8f62a"
      },
      "source": [
        "df_knn_test.head()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.534689</td>\n",
              "      <td>3.654595</td>\n",
              "      <td>-0.393884</td>\n",
              "      <td>-0.538603</td>\n",
              "      <td>-0.010288</td>\n",
              "      <td>-0.008849</td>\n",
              "      <td>-0.063542</td>\n",
              "      <td>-0.007710</td>\n",
              "      <td>-0.471282</td>\n",
              "      <td>-0.160986</td>\n",
              "      <td>-0.426783</td>\n",
              "      <td>-0.472746</td>\n",
              "      <td>-0.553613</td>\n",
              "      <td>-0.227799</td>\n",
              "      <td>-0.585327</td>\n",
              "      <td>-0.523296</td>\n",
              "      <td>-0.049753</td>\n",
              "      <td>-0.149383</td>\n",
              "      <td>-0.383884</td>\n",
              "      <td>-0.470971</td>\n",
              "      <td>-0.488241</td>\n",
              "      <td>-0.069791</td>\n",
              "      <td>-0.529612</td>\n",
              "      <td>-0.354254</td>\n",
              "      <td>-0.440979</td>\n",
              "      <td>-0.481636</td>\n",
              "      <td>-0.151834</td>\n",
              "      <td>-0.363404</td>\n",
              "      <td>-0.231374</td>\n",
              "      <td>-0.262419</td>\n",
              "      <td>-0.308107</td>\n",
              "      <td>-0.139502</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.011039</td>\n",
              "      <td>-0.010012</td>\n",
              "      <td>-0.213164</td>\n",
              "      <td>0.502542</td>\n",
              "      <td>-0.429756</td>\n",
              "      <td>-0.567325</td>\n",
              "      <td>-0.624704</td>\n",
              "      <td>-0.562257</td>\n",
              "      <td>-0.391713</td>\n",
              "      <td>-0.249628</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>1.166932</td>\n",
              "      <td>-0.789440</td>\n",
              "      <td>-0.158387</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>0.879322</td>\n",
              "      <td>-0.625772</td>\n",
              "      <td>-0.426783</td>\n",
              "      <td>-0.585327</td>\n",
              "      <td>-0.010288</td>\n",
              "      <td>-0.063542</td>\n",
              "      <td>-0.008849</td>\n",
              "      <td>-0.007710</td>\n",
              "      <td>1.048939</td>\n",
              "      <td>-0.232876</td>\n",
              "      <td>-0.008737</td>\n",
              "      <td>1.973823</td>\n",
              "      <td>-0.134029</td>\n",
              "      <td>-0.099438</td>\n",
              "      <td>-0.150466</td>\n",
              "      <td>-0.109029</td>\n",
              "      <td>-0.454944</td>\n",
              "      <td>-0.141985</td>\n",
              "      <td>-0.462573</td>\n",
              "      <td>-0.436326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.819477</td>\n",
              "      <td>-0.366195</td>\n",
              "      <td>-0.393884</td>\n",
              "      <td>-0.538604</td>\n",
              "      <td>-0.009461</td>\n",
              "      <td>-0.009477</td>\n",
              "      <td>-0.063542</td>\n",
              "      <td>-0.007711</td>\n",
              "      <td>-0.471282</td>\n",
              "      <td>-0.160986</td>\n",
              "      <td>-0.426783</td>\n",
              "      <td>-0.472746</td>\n",
              "      <td>-0.555933</td>\n",
              "      <td>-0.355504</td>\n",
              "      <td>-0.592847</td>\n",
              "      <td>-0.523296</td>\n",
              "      <td>-0.055793</td>\n",
              "      <td>1.389574</td>\n",
              "      <td>-0.383891</td>\n",
              "      <td>-0.470971</td>\n",
              "      <td>-0.488242</td>\n",
              "      <td>-0.069804</td>\n",
              "      <td>-0.529612</td>\n",
              "      <td>-0.354253</td>\n",
              "      <td>-0.440979</td>\n",
              "      <td>-0.481636</td>\n",
              "      <td>-0.151833</td>\n",
              "      <td>-0.363404</td>\n",
              "      <td>-0.231374</td>\n",
              "      <td>-0.262419</td>\n",
              "      <td>-0.308107</td>\n",
              "      <td>-0.139502</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.010088</td>\n",
              "      <td>-0.010641</td>\n",
              "      <td>1.434625</td>\n",
              "      <td>-0.175489</td>\n",
              "      <td>-0.429756</td>\n",
              "      <td>-0.569646</td>\n",
              "      <td>-0.629924</td>\n",
              "      <td>-0.566460</td>\n",
              "      <td>-0.391718</td>\n",
              "      <td>-0.249628</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>-0.856948</td>\n",
              "      <td>1.266721</td>\n",
              "      <td>-0.158387</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>-0.935521</td>\n",
              "      <td>-0.632903</td>\n",
              "      <td>-0.426783</td>\n",
              "      <td>-0.592847</td>\n",
              "      <td>-0.009461</td>\n",
              "      <td>-0.063542</td>\n",
              "      <td>-0.009477</td>\n",
              "      <td>-0.007711</td>\n",
              "      <td>-0.581076</td>\n",
              "      <td>-0.233011</td>\n",
              "      <td>-0.008737</td>\n",
              "      <td>0.808415</td>\n",
              "      <td>-0.134029</td>\n",
              "      <td>-0.099438</td>\n",
              "      <td>-0.150466</td>\n",
              "      <td>-0.109029</td>\n",
              "      <td>-0.454944</td>\n",
              "      <td>-0.141985</td>\n",
              "      <td>-0.462573</td>\n",
              "      <td>-0.436326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.363511</td>\n",
              "      <td>-0.039070</td>\n",
              "      <td>-0.393884</td>\n",
              "      <td>-0.538603</td>\n",
              "      <td>-0.010288</td>\n",
              "      <td>-0.008849</td>\n",
              "      <td>-0.063289</td>\n",
              "      <td>-0.007710</td>\n",
              "      <td>-0.466190</td>\n",
              "      <td>-0.128325</td>\n",
              "      <td>-0.406699</td>\n",
              "      <td>-0.472746</td>\n",
              "      <td>-0.553613</td>\n",
              "      <td>-0.227799</td>\n",
              "      <td>-0.585327</td>\n",
              "      <td>-0.523296</td>\n",
              "      <td>-0.049411</td>\n",
              "      <td>-0.183005</td>\n",
              "      <td>-0.383882</td>\n",
              "      <td>-0.470971</td>\n",
              "      <td>-0.488240</td>\n",
              "      <td>-0.069787</td>\n",
              "      <td>-0.529612</td>\n",
              "      <td>-0.354254</td>\n",
              "      <td>-0.440979</td>\n",
              "      <td>-0.481636</td>\n",
              "      <td>-0.151834</td>\n",
              "      <td>-0.363404</td>\n",
              "      <td>-0.231374</td>\n",
              "      <td>-0.262419</td>\n",
              "      <td>-0.308107</td>\n",
              "      <td>-0.139502</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.011673</td>\n",
              "      <td>-0.010012</td>\n",
              "      <td>-0.230264</td>\n",
              "      <td>0.361819</td>\n",
              "      <td>-0.318232</td>\n",
              "      <td>-0.567325</td>\n",
              "      <td>-0.621223</td>\n",
              "      <td>-0.563658</td>\n",
              "      <td>-0.391715</td>\n",
              "      <td>-0.249628</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>1.166932</td>\n",
              "      <td>-0.789440</td>\n",
              "      <td>-0.158387</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>0.879322</td>\n",
              "      <td>-0.621017</td>\n",
              "      <td>-0.406699</td>\n",
              "      <td>-0.585327</td>\n",
              "      <td>-0.010288</td>\n",
              "      <td>-0.063289</td>\n",
              "      <td>-0.008849</td>\n",
              "      <td>-0.007710</td>\n",
              "      <td>-0.537551</td>\n",
              "      <td>-0.232876</td>\n",
              "      <td>-0.008737</td>\n",
              "      <td>-0.356993</td>\n",
              "      <td>-0.134029</td>\n",
              "      <td>-0.099438</td>\n",
              "      <td>-0.150466</td>\n",
              "      <td>-0.109029</td>\n",
              "      <td>-0.454944</td>\n",
              "      <td>-0.141985</td>\n",
              "      <td>-0.462573</td>\n",
              "      <td>-0.436326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.304732</td>\n",
              "      <td>-0.366195</td>\n",
              "      <td>-0.393884</td>\n",
              "      <td>2.574659</td>\n",
              "      <td>0.008740</td>\n",
              "      <td>0.001836</td>\n",
              "      <td>0.214372</td>\n",
              "      <td>-0.006607</td>\n",
              "      <td>0.646457</td>\n",
              "      <td>-0.160986</td>\n",
              "      <td>0.491677</td>\n",
              "      <td>0.903840</td>\n",
              "      <td>-0.248190</td>\n",
              "      <td>-0.355504</td>\n",
              "      <td>-0.315712</td>\n",
              "      <td>-0.202754</td>\n",
              "      <td>-0.055790</td>\n",
              "      <td>-0.311377</td>\n",
              "      <td>0.138555</td>\n",
              "      <td>-0.038456</td>\n",
              "      <td>-0.164580</td>\n",
              "      <td>-0.069796</td>\n",
              "      <td>2.588486</td>\n",
              "      <td>0.076364</td>\n",
              "      <td>-0.039888</td>\n",
              "      <td>-0.157693</td>\n",
              "      <td>-0.151263</td>\n",
              "      <td>3.527658</td>\n",
              "      <td>0.406635</td>\n",
              "      <td>0.376831</td>\n",
              "      <td>0.192254</td>\n",
              "      <td>-0.139422</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.018112</td>\n",
              "      <td>0.007710</td>\n",
              "      <td>-0.295553</td>\n",
              "      <td>-0.175484</td>\n",
              "      <td>-0.429756</td>\n",
              "      <td>-0.261647</td>\n",
              "      <td>-0.255096</td>\n",
              "      <td>-0.229182</td>\n",
              "      <td>-0.357980</td>\n",
              "      <td>-0.249628</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>1.166932</td>\n",
              "      <td>-0.789440</td>\n",
              "      <td>-0.158387</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>-0.935521</td>\n",
              "      <td>-0.283392</td>\n",
              "      <td>0.491677</td>\n",
              "      <td>-0.315712</td>\n",
              "      <td>0.008740</td>\n",
              "      <td>0.214372</td>\n",
              "      <td>0.001836</td>\n",
              "      <td>-0.006607</td>\n",
              "      <td>1.048939</td>\n",
              "      <td>-0.193330</td>\n",
              "      <td>-0.004402</td>\n",
              "      <td>0.808415</td>\n",
              "      <td>0.034301</td>\n",
              "      <td>0.667100</td>\n",
              "      <td>0.896073</td>\n",
              "      <td>-0.073758</td>\n",
              "      <td>-0.121727</td>\n",
              "      <td>-0.129635</td>\n",
              "      <td>-0.139144</td>\n",
              "      <td>-0.106241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.316363</td>\n",
              "      <td>-0.366195</td>\n",
              "      <td>-0.393884</td>\n",
              "      <td>1.088803</td>\n",
              "      <td>-0.002842</td>\n",
              "      <td>-0.003192</td>\n",
              "      <td>-0.005554</td>\n",
              "      <td>-0.007558</td>\n",
              "      <td>0.572620</td>\n",
              "      <td>-0.160986</td>\n",
              "      <td>0.033158</td>\n",
              "      <td>0.494621</td>\n",
              "      <td>-0.362240</td>\n",
              "      <td>-0.312936</td>\n",
              "      <td>-0.523786</td>\n",
              "      <td>-0.386402</td>\n",
              "      <td>-0.055792</td>\n",
              "      <td>-0.311377</td>\n",
              "      <td>0.205431</td>\n",
              "      <td>-0.020599</td>\n",
              "      <td>-0.171657</td>\n",
              "      <td>-0.069784</td>\n",
              "      <td>1.089329</td>\n",
              "      <td>0.217117</td>\n",
              "      <td>-0.048569</td>\n",
              "      <td>-0.162736</td>\n",
              "      <td>-0.151146</td>\n",
              "      <td>1.727036</td>\n",
              "      <td>0.416071</td>\n",
              "      <td>0.380530</td>\n",
              "      <td>0.184457</td>\n",
              "      <td>-0.139386</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.004227</td>\n",
              "      <td>-0.004231</td>\n",
              "      <td>-0.295553</td>\n",
              "      <td>-0.175484</td>\n",
              "      <td>-0.429756</td>\n",
              "      <td>-0.375792</td>\n",
              "      <td>-0.504513</td>\n",
              "      <td>-0.401150</td>\n",
              "      <td>-0.383613</td>\n",
              "      <td>-0.249628</td>\n",
              "      <td>-0.241266</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>1.166932</td>\n",
              "      <td>-0.789440</td>\n",
              "      <td>-0.158387</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.008892</td>\n",
              "      <td>0.879322</td>\n",
              "      <td>-0.512972</td>\n",
              "      <td>0.033158</td>\n",
              "      <td>-0.523786</td>\n",
              "      <td>-0.002842</td>\n",
              "      <td>-0.005554</td>\n",
              "      <td>-0.003192</td>\n",
              "      <td>-0.007558</td>\n",
              "      <td>-0.133946</td>\n",
              "      <td>3.804648</td>\n",
              "      <td>-0.000934</td>\n",
              "      <td>-0.939697</td>\n",
              "      <td>0.048620</td>\n",
              "      <td>0.168689</td>\n",
              "      <td>0.194866</td>\n",
              "      <td>0.022308</td>\n",
              "      <td>-0.128919</td>\n",
              "      <td>-0.130008</td>\n",
              "      <td>-0.146216</td>\n",
              "      <td>-0.112304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Source Port  Destination Port  Protocol  ...  Idle Std  Idle Max  Idle Min\n",
              "0    -0.534689          3.654595 -0.393884  ... -0.141985 -0.462573 -0.436326\n",
              "1     0.819477         -0.366195 -0.393884  ... -0.141985 -0.462573 -0.436326\n",
              "2     0.363511         -0.039070 -0.393884  ... -0.141985 -0.462573 -0.436326\n",
              "3    -0.304732         -0.366195 -0.393884  ... -0.129635 -0.139144 -0.106241\n",
              "4     0.316363         -0.366195 -0.393884  ... -0.130008 -0.146216 -0.112304\n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T8Abk2XhwI"
      },
      "source": [
        "Select a subset of features from the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "LFABov84XhwK",
        "outputId": "9e900442-8f78-4171-960e-4d22862813e9"
      },
      "source": [
        "df_knn_sub=df_knn_train.loc[:, features]\n",
        "df_knn_sub.head()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.138669</td>\n",
              "      <td>-0.520573</td>\n",
              "      <td>-0.246600</td>\n",
              "      <td>-0.551962</td>\n",
              "      <td>-0.147382</td>\n",
              "      <td>0.202439</td>\n",
              "      <td>0.184633</td>\n",
              "      <td>-0.381429</td>\n",
              "      <td>-0.456260</td>\n",
              "      <td>-0.523283</td>\n",
              "      <td>-0.561373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.018977</td>\n",
              "      <td>2.011935</td>\n",
              "      <td>0.033759</td>\n",
              "      <td>1.679926</td>\n",
              "      <td>-0.264267</td>\n",
              "      <td>-0.108088</td>\n",
              "      <td>-0.041403</td>\n",
              "      <td>1.094896</td>\n",
              "      <td>1.569069</td>\n",
              "      <td>2.006133</td>\n",
              "      <td>0.837688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.351963</td>\n",
              "      <td>0.834021</td>\n",
              "      <td>0.095246</td>\n",
              "      <td>2.528305</td>\n",
              "      <td>-0.264266</td>\n",
              "      <td>-0.066286</td>\n",
              "      <td>-0.041407</td>\n",
              "      <td>0.257756</td>\n",
              "      <td>-0.329481</td>\n",
              "      <td>1.219203</td>\n",
              "      <td>-0.174671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.233578</td>\n",
              "      <td>-0.520580</td>\n",
              "      <td>-0.246600</td>\n",
              "      <td>-0.551962</td>\n",
              "      <td>-0.264020</td>\n",
              "      <td>-0.149890</td>\n",
              "      <td>-0.041206</td>\n",
              "      <td>-0.375607</td>\n",
              "      <td>-0.536380</td>\n",
              "      <td>-0.522377</td>\n",
              "      <td>-0.561373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.270577</td>\n",
              "      <td>-0.520580</td>\n",
              "      <td>-0.246600</td>\n",
              "      <td>-0.551962</td>\n",
              "      <td>-0.109216</td>\n",
              "      <td>-0.287238</td>\n",
              "      <td>-0.041410</td>\n",
              "      <td>-0.381428</td>\n",
              "      <td>-0.642826</td>\n",
              "      <td>-0.523285</td>\n",
              "      <td>-0.561373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Fwd Packet Length Max  Flow IAT Std  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0              -0.138669     -0.520573  ...     -0.523283              -0.561373\n",
              "1               0.018977      2.011935  ...      2.006133               0.837688\n",
              "2               0.351963      0.834021  ...      1.219203              -0.174671\n",
              "3              -0.233578     -0.520580  ...     -0.522377              -0.561373\n",
              "4              -0.270577     -0.520580  ...     -0.523285              -0.561373\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "zhVGlgN5XhwK",
        "outputId": "44608e42-23d7-403c-beb7-ae259bdbf7cd"
      },
      "source": [
        "df_knn_test_sub=df_knn_test.loc[:, features]\n",
        "df_knn_test_sub.head()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.471282</td>\n",
              "      <td>-0.470971</td>\n",
              "      <td>-0.472746</td>\n",
              "      <td>-0.529612</td>\n",
              "      <td>-0.149383</td>\n",
              "      <td>-0.426783</td>\n",
              "      <td>-0.049753</td>\n",
              "      <td>-0.383884</td>\n",
              "      <td>-0.585327</td>\n",
              "      <td>-0.488241</td>\n",
              "      <td>-0.523296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.471282</td>\n",
              "      <td>-0.470971</td>\n",
              "      <td>-0.472746</td>\n",
              "      <td>-0.529612</td>\n",
              "      <td>1.389574</td>\n",
              "      <td>-0.426783</td>\n",
              "      <td>-0.055793</td>\n",
              "      <td>-0.383891</td>\n",
              "      <td>-0.592847</td>\n",
              "      <td>-0.488242</td>\n",
              "      <td>-0.523296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.466190</td>\n",
              "      <td>-0.470971</td>\n",
              "      <td>-0.472746</td>\n",
              "      <td>-0.529612</td>\n",
              "      <td>-0.183005</td>\n",
              "      <td>-0.406699</td>\n",
              "      <td>-0.049411</td>\n",
              "      <td>-0.383882</td>\n",
              "      <td>-0.585327</td>\n",
              "      <td>-0.488240</td>\n",
              "      <td>-0.523296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.646457</td>\n",
              "      <td>-0.038456</td>\n",
              "      <td>0.903840</td>\n",
              "      <td>2.588486</td>\n",
              "      <td>-0.311377</td>\n",
              "      <td>0.491677</td>\n",
              "      <td>-0.055790</td>\n",
              "      <td>0.138555</td>\n",
              "      <td>-0.315712</td>\n",
              "      <td>-0.164580</td>\n",
              "      <td>-0.202754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.572620</td>\n",
              "      <td>-0.020599</td>\n",
              "      <td>0.494621</td>\n",
              "      <td>1.089329</td>\n",
              "      <td>-0.311377</td>\n",
              "      <td>0.033158</td>\n",
              "      <td>-0.055792</td>\n",
              "      <td>0.205431</td>\n",
              "      <td>-0.523786</td>\n",
              "      <td>-0.171657</td>\n",
              "      <td>-0.386402</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Fwd Packet Length Max  Flow IAT Std  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0              -0.471282     -0.470971  ...     -0.488241              -0.523296\n",
              "1              -0.471282     -0.470971  ...     -0.488242              -0.523296\n",
              "2              -0.466190     -0.470971  ...     -0.488240              -0.523296\n",
              "3               0.646457     -0.038456  ...     -0.164580              -0.202754\n",
              "4               0.572620     -0.020599  ...     -0.171657              -0.386402\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59G_b79XhwK"
      },
      "source": [
        "Convert dataframes to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJHMos8rXhwK"
      },
      "source": [
        "X_train_knn = df_knn_sub.to_numpy()\n",
        "X_test_knn = df_knn_test_sub.to_numpy()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hAzqyPsXhwK",
        "outputId": "ef297c56-ecf5-49e8-f173-1a89c7369f90"
      },
      "source": [
        "for i in range(5,X_train_knn.shape[1]+1):\n",
        "    knn=KNeighborsClassifier(n_neighbors=i)\n",
        "    model_knn=knn.fit(X_train_knn,y_train)\n",
        "    y_pred=model_knn.predict(X_test_knn)\n",
        "    print(\"for \" , i,  \" as K, accuracy is : \", accuracy_score(y_test, y_pred))\n",
        "    display_metrics(y_test, y_pred, labels_d)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for  5  as K, accuracy is :  0.8002127422548523\n",
            "\n",
            "Accuracy: 0.80\n",
            "\n",
            "Micro Precision: 0.80\n",
            "Micro Recall: 0.80\n",
            "Micro F1-score: 0.80\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.38\n",
            "Macro Recall: 0.34\n",
            "Macro F1-score: 0.34\n",
            "\n",
            "Weighted Precision: 0.81\n",
            "Weighted Recall: 0.80\n",
            "Weighted F1-score: 0.79\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.76      0.93      0.84    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.97      0.63      0.77     32006\n",
            "             DoS GoldenEye       0.63      0.75      0.69      2573\n",
            "                  DoS Hulk       0.87      0.66      0.75     57531\n",
            "          DoS Slowhttptest       0.69      0.69      0.69      1374\n",
            "             DoS slowloris       0.74      0.34      0.46      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.86      0.79      0.83     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.05      0.20      0.08       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.09      0.02      0.04       163\n",
            "\n",
            "                 micro avg       0.80      0.80      0.80    278271\n",
            "                 macro avg       0.38      0.34      0.34    278271\n",
            "              weighted avg       0.81      0.80      0.79    278271\n",
            "               samples avg       0.80      0.80      0.80    278271\n",
            "\n",
            "for  6  as K, accuracy is :  0.7986423306776488\n",
            "\n",
            "Accuracy: 0.80\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.80\n",
            "Micro F1-score: 0.80\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.38\n",
            "Macro Recall: 0.33\n",
            "Macro F1-score: 0.34\n",
            "\n",
            "Weighted Precision: 0.81\n",
            "Weighted Recall: 0.80\n",
            "Weighted F1-score: 0.79\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.77      0.93      0.84    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.97      0.63      0.76     32006\n",
            "             DoS GoldenEye       0.64      0.75      0.69      2573\n",
            "                  DoS Hulk       0.87      0.66      0.75     57531\n",
            "          DoS Slowhttptest       0.69      0.69      0.69      1374\n",
            "             DoS slowloris       0.74      0.34      0.46      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.86      0.79      0.83     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.05      0.20      0.09       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.14      0.02      0.03       163\n",
            "\n",
            "                 micro avg       0.81      0.80      0.80    278271\n",
            "                 macro avg       0.38      0.33      0.34    278271\n",
            "              weighted avg       0.81      0.80      0.79    278271\n",
            "               samples avg       0.80      0.80      0.80    278271\n",
            "\n",
            "for  7  as K, accuracy is :  0.803630274085334\n",
            "\n",
            "Accuracy: 0.80\n",
            "\n",
            "Micro Precision: 0.80\n",
            "Micro Recall: 0.80\n",
            "Micro F1-score: 0.80\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.44\n",
            "Macro Recall: 0.36\n",
            "Macro F1-score: 0.38\n",
            "\n",
            "Weighted Precision: 0.82\n",
            "Weighted Recall: 0.80\n",
            "Weighted F1-score: 0.80\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.77      0.93      0.84    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.97      0.64      0.77     32006\n",
            "             DoS GoldenEye       0.63      0.75      0.69      2573\n",
            "                  DoS Hulk       0.87      0.66      0.75     57531\n",
            "          DoS Slowhttptest       0.69      0.69      0.69      1374\n",
            "             DoS slowloris       0.73      0.34      0.46      1449\n",
            "               FTP-Patator       1.00      0.34      0.51      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.86      0.79      0.83     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.05      0.20      0.08       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.06      0.02      0.03       163\n",
            "\n",
            "                 micro avg       0.80      0.80      0.80    278271\n",
            "                 macro avg       0.44      0.36      0.38    278271\n",
            "              weighted avg       0.82      0.80      0.80    278271\n",
            "               samples avg       0.80      0.80      0.80    278271\n",
            "\n",
            "for  8  as K, accuracy is :  0.7989621627837612\n",
            "\n",
            "Accuracy: 0.80\n",
            "\n",
            "Micro Precision: 0.82\n",
            "Micro Recall: 0.80\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.45\n",
            "Macro Recall: 0.35\n",
            "Macro F1-score: 0.38\n",
            "\n",
            "Weighted Precision: 0.83\n",
            "Weighted Recall: 0.80\n",
            "Weighted F1-score: 0.80\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.78      0.92      0.85    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.97      0.63      0.77     32006\n",
            "             DoS GoldenEye       0.64      0.75      0.69      2573\n",
            "                  DoS Hulk       0.87      0.66      0.75     57531\n",
            "          DoS Slowhttptest       0.69      0.69      0.69      1374\n",
            "             DoS slowloris       0.74      0.34      0.46      1449\n",
            "               FTP-Patator       1.00      0.33      0.49      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.86      0.79      0.83     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.06      0.18      0.09       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.07      0.02      0.03       163\n",
            "\n",
            "                 micro avg       0.82      0.80      0.81    278271\n",
            "                 macro avg       0.45      0.35      0.38    278271\n",
            "              weighted avg       0.83      0.80      0.80    278271\n",
            "               samples avg       0.80      0.80      0.80    278271\n",
            "\n",
            "for  9  as K, accuracy is :  0.8116188894998041\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.44\n",
            "Macro Recall: 0.38\n",
            "Macro F1-score: 0.39\n",
            "\n",
            "Weighted Precision: 0.83\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.81\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.78      0.93      0.85    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.97      0.66      0.79     32006\n",
            "             DoS GoldenEye       0.63      0.75      0.69      2573\n",
            "                  DoS Hulk       0.87      0.69      0.77     57531\n",
            "          DoS Slowhttptest       0.69      0.70      0.69      1374\n",
            "             DoS slowloris       0.73      0.34      0.46      1449\n",
            "               FTP-Patator       0.99      0.37      0.54      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.86      0.80      0.83     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.07      0.31      0.12       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.04      0.08      0.06       163\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    278271\n",
            "                 macro avg       0.44      0.38      0.39    278271\n",
            "              weighted avg       0.83      0.81      0.81    278271\n",
            "               samples avg       0.81      0.81      0.81    278271\n",
            "\n",
            "for  10  as K, accuracy is :  0.8021856391790736\n",
            "\n",
            "Accuracy: 0.80\n",
            "\n",
            "Micro Precision: 0.82\n",
            "Micro Recall: 0.80\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.44\n",
            "Macro Recall: 0.37\n",
            "Macro F1-score: 0.38\n",
            "\n",
            "Weighted Precision: 0.83\n",
            "Weighted Recall: 0.80\n",
            "Weighted F1-score: 0.80\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.79      0.92      0.85    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.97      0.66      0.78     32006\n",
            "             DoS GoldenEye       0.65      0.75      0.70      2573\n",
            "                  DoS Hulk       0.87      0.65      0.75     57531\n",
            "          DoS Slowhttptest       0.69      0.70      0.69      1374\n",
            "             DoS slowloris       0.73      0.34      0.46      1449\n",
            "               FTP-Patator       0.99      0.36      0.53      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.86      0.80      0.83     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.08      0.30      0.12       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                 micro avg       0.82      0.80      0.81    278271\n",
            "                 macro avg       0.44      0.37      0.38    278271\n",
            "              weighted avg       0.83      0.80      0.80    278271\n",
            "               samples avg       0.80      0.80      0.80    278271\n",
            "\n",
            "for  11  as K, accuracy is :  0.8061062776933278\n",
            "\n",
            "Accuracy: 0.81\n",
            "\n",
            "Micro Precision: 0.81\n",
            "Micro Recall: 0.81\n",
            "Micro F1-score: 0.81\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.44\n",
            "Macro Recall: 0.38\n",
            "Macro F1-score: 0.39\n",
            "\n",
            "Weighted Precision: 0.82\n",
            "Weighted Recall: 0.81\n",
            "Weighted F1-score: 0.80\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.78      0.93      0.84    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.97      0.67      0.80     32006\n",
            "             DoS GoldenEye       0.63      0.75      0.68      2573\n",
            "                  DoS Hulk       0.86      0.66      0.75     57531\n",
            "          DoS Slowhttptest       0.69      0.70      0.69      1374\n",
            "             DoS slowloris       0.73      0.34      0.46      1449\n",
            "               FTP-Patator       1.00      0.46      0.63      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.86      0.80      0.83     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.07      0.38      0.12       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                 micro avg       0.81      0.81      0.81    278271\n",
            "                 macro avg       0.44      0.38      0.39    278271\n",
            "              weighted avg       0.82      0.81      0.80    278271\n",
            "               samples avg       0.81      0.81      0.81    278271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQvcVlbmXhwL"
      },
      "source": [
        "# Model 9: DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tL-xBt_XhwL"
      },
      "source": [
        "def make_model(metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          16, activation='relu',\n",
        "          input_shape=(X_train.shape[-1],)),\n",
        "      tf.keras.layers.Dropout(0.1),\n",
        "      tf.keras.layers.Dense(32, activation ='relu'),\n",
        "      tf.keras.layers.Dropout(0.1),\n",
        "      tf.keras.layers.Dense(16, activation ='relu'),\n",
        "      tf.keras.layers.Dense(y_train.shape[-1], activation='softmax',\n",
        "                         bias_initializer=output_bias),\n",
        "  ])\n",
        " \n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAzpGMEoXhwL"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 7000\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWmjTb9kXhwL",
        "outputId": "127a1938-1cec-4bf3-f872-c861cb92092c"
      },
      "source": [
        "model_dnn = make_model()\n",
        "model_dnn.summary()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_33 (Dense)             (None, 16)                1152      \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 15)                255       \n",
            "=================================================================\n",
            "Total params: 2,479\n",
            "Trainable params: 2,479\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajWocDr4_30a"
      },
      "source": [
        "If loading the validation dataset has an issue, please load the csv files again, and encode it again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeRlOQj6XhwL",
        "outputId": "00729cb3-2cd9-45a9-dd74-6f651e6a2292"
      },
      "source": [
        "baseline_history = model_dnn.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(X_val, y_val))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "80/80 [==============================] - 4s 26ms/step - loss: 0.7211 - tp: 239921.7778 - fp: 16725.5185 - tn: 7894665.3210 - fn: 325177.5679 - accuracy: 0.9623 - precision: 0.9348 - recall: 0.4663 - auc: 0.8574 - val_loss: 0.6874 - val_tp: 790.0000 - val_fp: 589.0000 - val_tn: 3895205.0000 - val_fn: 277481.0000 - val_accuracy: 0.9334 - val_precision: 0.5729 - val_recall: 0.0028 - val_auc: 0.5982\n",
            "Epoch 2/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.6763 - tp: 1132.0617 - fp: 740.4198 - tn: 4014856.4198 - fn: 285696.2840 - accuracy: 0.9334 - precision: 0.5966 - recall: 0.0038 - auc: 0.6154 - val_loss: 0.6336 - val_tp: 1034.0000 - val_fp: 590.0000 - val_tn: 3895204.0000 - val_fn: 277237.0000 - val_accuracy: 0.9334 - val_precision: 0.6367 - val_recall: 0.0037 - val_auc: 0.7145\n",
            "Epoch 3/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.6188 - tp: 3227.6420 - fp: 899.9012 - tn: 4014696.9383 - fn: 283600.7037 - accuracy: 0.9338 - precision: 0.7629 - recall: 0.0102 - auc: 0.6967 - val_loss: 0.5617 - val_tp: 1944.0000 - val_fp: 660.0000 - val_tn: 3895134.0000 - val_fn: 276327.0000 - val_accuracy: 0.9336 - val_precision: 0.7465 - val_recall: 0.0070 - val_auc: 0.7494\n",
            "Epoch 4/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.5439 - tp: 8275.7778 - fp: 1701.4321 - tn: 4013895.4074 - fn: 278552.5679 - accuracy: 0.9347 - precision: 0.8293 - recall: 0.0264 - auc: 0.7389 - val_loss: 0.4754 - val_tp: 6489.0000 - val_fp: 873.0000 - val_tn: 3894921.0000 - val_fn: 271782.0000 - val_accuracy: 0.9347 - val_precision: 0.8814 - val_recall: 0.0233 - val_auc: 0.7742\n",
            "Epoch 5/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.4591 - tp: 34937.8025 - fp: 5313.3704 - tn: 4010283.4691 - fn: 251890.5432 - accuracy: 0.9394 - precision: 0.8652 - recall: 0.1081 - auc: 0.7704 - val_loss: 0.3867 - val_tp: 29218.0000 - val_fp: 6917.0000 - val_tn: 3888877.0000 - val_fn: 249053.0000 - val_accuracy: 0.9387 - val_precision: 0.8086 - val_recall: 0.1050 - val_auc: 0.8054\n",
            "Epoch 6/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3753 - tp: 63686.4074 - fp: 13423.3704 - tn: 4002173.4691 - fn: 223141.9383 - accuracy: 0.9449 - precision: 0.8324 - recall: 0.2170 - auc: 0.7978 - val_loss: 0.3054 - val_tp: 87735.0000 - val_fp: 14141.0000 - val_tn: 3881653.0000 - val_fn: 190536.0000 - val_accuracy: 0.9510 - val_precision: 0.8612 - val_recall: 0.3153 - val_auc: 0.8401\n",
            "Epoch 7/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.3007 - tp: 81061.7654 - fp: 22449.2099 - tn: 3993147.6296 - fn: 205766.5802 - accuracy: 0.9468 - precision: 0.7854 - recall: 0.2779 - auc: 0.8284 - val_loss: 0.2392 - val_tp: 101589.0000 - val_fp: 23900.0000 - val_tn: 3871894.0000 - val_fn: 176682.0000 - val_accuracy: 0.9519 - val_precision: 0.8095 - val_recall: 0.3651 - val_auc: 0.8752\n",
            "Epoch 8/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.2416 - tp: 94757.9753 - fp: 31577.0617 - tn: 3984019.7778 - fn: 192070.3704 - accuracy: 0.9479 - precision: 0.7524 - recall: 0.3254 - auc: 0.8614 - val_loss: 0.1899 - val_tp: 122986.0000 - val_fp: 27437.0000 - val_tn: 3868357.0000 - val_fn: 155285.0000 - val_accuracy: 0.9562 - val_precision: 0.8176 - val_recall: 0.4420 - val_auc: 0.9073\n",
            "Epoch 9/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.1978 - tp: 114146.0494 - fp: 38187.8025 - tn: 3977409.0370 - fn: 172682.2963 - accuracy: 0.9507 - precision: 0.7481 - recall: 0.3921 - auc: 0.8957 - val_loss: 0.1558 - val_tp: 152038.0000 - val_fp: 31038.0000 - val_tn: 3864756.0000 - val_fn: 126233.0000 - val_accuracy: 0.9623 - val_precision: 0.8305 - val_recall: 0.5464 - val_auc: 0.9373\n",
            "Epoch 10/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.1671 - tp: 129856.3210 - fp: 40623.6296 - tn: 3974973.2099 - fn: 156972.0247 - accuracy: 0.9539 - precision: 0.7612 - recall: 0.4493 - auc: 0.9247 - val_loss: 0.1334 - val_tp: 156615.0000 - val_fp: 33771.0000 - val_tn: 3862023.0000 - val_fn: 121656.0000 - val_accuracy: 0.9628 - val_precision: 0.8226 - val_recall: 0.5628 - val_auc: 0.9542\n",
            "Epoch 11/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.1463 - tp: 141172.3951 - fp: 43202.0123 - tn: 3972394.8272 - fn: 145655.9506 - accuracy: 0.9559 - precision: 0.7652 - recall: 0.4891 - auc: 0.9415 - val_loss: 0.1181 - val_tp: 162688.0000 - val_fp: 39131.0000 - val_tn: 3856663.0000 - val_fn: 115583.0000 - val_accuracy: 0.9629 - val_precision: 0.8061 - val_recall: 0.5846 - val_auc: 0.9620\n",
            "Epoch 12/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.1315 - tp: 148136.8765 - fp: 44415.3210 - tn: 3971181.5185 - fn: 138691.4691 - accuracy: 0.9573 - precision: 0.7688 - recall: 0.5149 - auc: 0.9510 - val_loss: 0.1071 - val_tp: 164700.0000 - val_fp: 42045.0000 - val_tn: 3853749.0000 - val_fn: 113571.0000 - val_accuracy: 0.9627 - val_precision: 0.7966 - val_recall: 0.5919 - val_auc: 0.9660\n",
            "Epoch 13/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.1208 - tp: 153561.4568 - fp: 43338.2963 - tn: 3972258.5432 - fn: 133266.8889 - accuracy: 0.9588 - precision: 0.7789 - recall: 0.5331 - auc: 0.9570 - val_loss: 0.0986 - val_tp: 166665.0000 - val_fp: 43681.0000 - val_tn: 3852113.0000 - val_fn: 111606.0000 - val_accuracy: 0.9628 - val_precision: 0.7923 - val_recall: 0.5989 - val_auc: 0.9695\n",
            "Epoch 14/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.1122 - tp: 162895.3457 - fp: 42750.0123 - tn: 3972846.8272 - fn: 123933.0000 - accuracy: 0.9611 - precision: 0.7912 - recall: 0.5654 - auc: 0.9612 - val_loss: 0.0914 - val_tp: 202331.0000 - val_fp: 42138.0000 - val_tn: 3853656.0000 - val_fn: 75940.0000 - val_accuracy: 0.9717 - val_precision: 0.8276 - val_recall: 0.7271 - val_auc: 0.9726\n",
            "Epoch 15/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.1051 - tp: 174398.9877 - fp: 39862.5432 - tn: 3975734.2963 - fn: 112429.3580 - accuracy: 0.9644 - precision: 0.8126 - recall: 0.6051 - auc: 0.9647 - val_loss: 0.0850 - val_tp: 205855.0000 - val_fp: 40552.0000 - val_tn: 3855242.0000 - val_fn: 72416.0000 - val_accuracy: 0.9729 - val_precision: 0.8354 - val_recall: 0.7398 - val_auc: 0.9746\n",
            "Epoch 16/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0985 - tp: 184992.8395 - fp: 38019.8272 - tn: 3977577.0123 - fn: 101835.5062 - accuracy: 0.9673 - precision: 0.8287 - recall: 0.6419 - auc: 0.9677 - val_loss: 0.0796 - val_tp: 214701.0000 - val_fp: 36328.0000 - val_tn: 3859466.0000 - val_fn: 63570.0000 - val_accuracy: 0.9761 - val_precision: 0.8553 - val_recall: 0.7716 - val_auc: 0.9767\n",
            "Epoch 17/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0934 - tp: 195316.8889 - fp: 37397.8889 - tn: 3978198.9506 - fn: 91511.4568 - accuracy: 0.9698 - precision: 0.8386 - recall: 0.6779 - auc: 0.9699 - val_loss: 0.0751 - val_tp: 220369.0000 - val_fp: 31805.0000 - val_tn: 3863989.0000 - val_fn: 57902.0000 - val_accuracy: 0.9785 - val_precision: 0.8739 - val_recall: 0.7919 - val_auc: 0.9780\n",
            "Epoch 18/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0887 - tp: 203116.0864 - fp: 37853.0000 - tn: 3977743.8395 - fn: 83712.2593 - accuracy: 0.9716 - precision: 0.8424 - recall: 0.7063 - auc: 0.9718 - val_loss: 0.0711 - val_tp: 221994.0000 - val_fp: 27880.0000 - val_tn: 3867914.0000 - val_fn: 56277.0000 - val_accuracy: 0.9798 - val_precision: 0.8884 - val_recall: 0.7978 - val_auc: 0.9786\n",
            "Epoch 19/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0843 - tp: 208567.4938 - fp: 37994.9259 - tn: 3977601.9136 - fn: 78260.8519 - accuracy: 0.9729 - precision: 0.8459 - recall: 0.7263 - auc: 0.9734 - val_loss: 0.0676 - val_tp: 223404.0000 - val_fp: 24673.0000 - val_tn: 3871121.0000 - val_fn: 54867.0000 - val_accuracy: 0.9809 - val_precision: 0.9005 - val_recall: 0.8028 - val_auc: 0.9792\n",
            "Epoch 20/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0809 - tp: 212809.0123 - fp: 37379.1111 - tn: 3978217.7284 - fn: 74019.3333 - accuracy: 0.9741 - precision: 0.8506 - recall: 0.7410 - auc: 0.9744 - val_loss: 0.0645 - val_tp: 225031.0000 - val_fp: 21667.0000 - val_tn: 3874127.0000 - val_fn: 53240.0000 - val_accuracy: 0.9821 - val_precision: 0.9122 - val_recall: 0.8087 - val_auc: 0.9796\n",
            "Epoch 21/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0776 - tp: 216407.0000 - fp: 37227.5802 - tn: 3978369.2593 - fn: 70421.3457 - accuracy: 0.9749 - precision: 0.8532 - recall: 0.7538 - auc: 0.9757 - val_loss: 0.0617 - val_tp: 228875.0000 - val_fp: 19745.0000 - val_tn: 3876049.0000 - val_fn: 49396.0000 - val_accuracy: 0.9834 - val_precision: 0.9206 - val_recall: 0.8225 - val_auc: 0.9800\n",
            "Epoch 22/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0748 - tp: 219078.7407 - fp: 36591.2593 - tn: 3979005.5802 - fn: 67749.6049 - accuracy: 0.9757 - precision: 0.8564 - recall: 0.7628 - auc: 0.9766 - val_loss: 0.0593 - val_tp: 231055.0000 - val_fp: 18056.0000 - val_tn: 3877738.0000 - val_fn: 47216.0000 - val_accuracy: 0.9844 - val_precision: 0.9275 - val_recall: 0.8303 - val_auc: 0.9802\n",
            "Epoch 23/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0721 - tp: 221624.0000 - fp: 35894.4074 - tn: 3979702.4321 - fn: 65204.3457 - accuracy: 0.9764 - precision: 0.8600 - recall: 0.7718 - auc: 0.9775 - val_loss: 0.0571 - val_tp: 232228.0000 - val_fp: 17188.0000 - val_tn: 3878606.0000 - val_fn: 46043.0000 - val_accuracy: 0.9849 - val_precision: 0.9311 - val_recall: 0.8345 - val_auc: 0.9797\n",
            "Epoch 24/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0697 - tp: 223849.4568 - fp: 34956.5062 - tn: 3980640.3333 - fn: 62978.8889 - accuracy: 0.9772 - precision: 0.8647 - recall: 0.7796 - auc: 0.9785 - val_loss: 0.0550 - val_tp: 233858.0000 - val_fp: 16801.0000 - val_tn: 3878993.0000 - val_fn: 44413.0000 - val_accuracy: 0.9853 - val_precision: 0.9330 - val_recall: 0.8404 - val_auc: 0.9798\n",
            "Epoch 25/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0670 - tp: 226115.9383 - fp: 34225.0000 - tn: 3981371.8395 - fn: 60712.4074 - accuracy: 0.9779 - precision: 0.8683 - recall: 0.7878 - auc: 0.9797 - val_loss: 0.0530 - val_tp: 234348.0000 - val_fp: 15714.0000 - val_tn: 3880080.0000 - val_fn: 43923.0000 - val_accuracy: 0.9857 - val_precision: 0.9372 - val_recall: 0.8422 - val_auc: 0.9803\n",
            "Epoch 26/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0646 - tp: 228869.3333 - fp: 33269.3704 - tn: 3982327.4691 - fn: 57959.0123 - accuracy: 0.9788 - precision: 0.8730 - recall: 0.7974 - auc: 0.9804 - val_loss: 0.0511 - val_tp: 235778.0000 - val_fp: 15217.0000 - val_tn: 3880577.0000 - val_fn: 42493.0000 - val_accuracy: 0.9862 - val_precision: 0.9394 - val_recall: 0.8473 - val_auc: 0.9814\n",
            "Epoch 27/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0624 - tp: 231508.9753 - fp: 32350.2222 - tn: 3983246.6173 - fn: 55319.3704 - accuracy: 0.9795 - precision: 0.8768 - recall: 0.8060 - auc: 0.9812 - val_loss: 0.0493 - val_tp: 237983.0000 - val_fp: 14738.0000 - val_tn: 3881056.0000 - val_fn: 40288.0000 - val_accuracy: 0.9868 - val_precision: 0.9417 - val_recall: 0.8552 - val_auc: 0.9816\n",
            "Epoch 28/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0600 - tp: 234001.4938 - fp: 31574.9506 - tn: 3984021.8889 - fn: 52826.8519 - accuracy: 0.9803 - precision: 0.8808 - recall: 0.8152 - auc: 0.9821 - val_loss: 0.0478 - val_tp: 239274.0000 - val_fp: 14858.0000 - val_tn: 3880936.0000 - val_fn: 38997.0000 - val_accuracy: 0.9871 - val_precision: 0.9415 - val_recall: 0.8599 - val_auc: 0.9817\n",
            "Epoch 29/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0580 - tp: 236214.5309 - fp: 30644.9012 - tn: 3984951.9383 - fn: 50613.8148 - accuracy: 0.9811 - precision: 0.8850 - recall: 0.8229 - auc: 0.9828 - val_loss: 0.0464 - val_tp: 240688.0000 - val_fp: 14700.0000 - val_tn: 3881094.0000 - val_fn: 37583.0000 - val_accuracy: 0.9875 - val_precision: 0.9424 - val_recall: 0.8649 - val_auc: 0.9818\n",
            "Epoch 30/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0565 - tp: 237878.7160 - fp: 29834.0247 - tn: 3985762.8148 - fn: 48949.6296 - accuracy: 0.9816 - precision: 0.8880 - recall: 0.8284 - auc: 0.9827 - val_loss: 0.0451 - val_tp: 242031.0000 - val_fp: 14515.0000 - val_tn: 3881279.0000 - val_fn: 36240.0000 - val_accuracy: 0.9878 - val_precision: 0.9434 - val_recall: 0.8698 - val_auc: 0.9826\n",
            "Epoch 31/100\n",
            "80/80 [==============================] - 1s 18ms/step - loss: 0.0542 - tp: 239946.5802 - fp: 28694.7407 - tn: 3986902.0988 - fn: 46881.7654 - accuracy: 0.9824 - precision: 0.8927 - recall: 0.8359 - auc: 0.9837 - val_loss: 0.0439 - val_tp: 242969.0000 - val_fp: 14290.0000 - val_tn: 3881504.0000 - val_fn: 35302.0000 - val_accuracy: 0.9881 - val_precision: 0.9445 - val_recall: 0.8731 - val_auc: 0.9841\n",
            "Epoch 32/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0524 - tp: 241696.1481 - fp: 27512.8025 - tn: 3988084.0370 - fn: 45132.1975 - accuracy: 0.9831 - precision: 0.8975 - recall: 0.8420 - auc: 0.9841 - val_loss: 0.0429 - val_tp: 243455.0000 - val_fp: 14019.0000 - val_tn: 3881775.0000 - val_fn: 34816.0000 - val_accuracy: 0.9883 - val_precision: 0.9456 - val_recall: 0.8749 - val_auc: 0.9851\n",
            "Epoch 33/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0507 - tp: 243500.8272 - fp: 26404.3827 - tn: 3989192.4568 - fn: 43327.5185 - accuracy: 0.9838 - precision: 0.9019 - recall: 0.8485 - auc: 0.9846 - val_loss: 0.0420 - val_tp: 243930.0000 - val_fp: 13748.0000 - val_tn: 3882046.0000 - val_fn: 34341.0000 - val_accuracy: 0.9885 - val_precision: 0.9466 - val_recall: 0.8766 - val_auc: 0.9853\n",
            "Epoch 34/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0493 - tp: 245254.2716 - fp: 25311.6049 - tn: 3990285.2346 - fn: 41574.0741 - accuracy: 0.9844 - precision: 0.9058 - recall: 0.8541 - auc: 0.9850 - val_loss: 0.0413 - val_tp: 244093.0000 - val_fp: 13541.0000 - val_tn: 3882253.0000 - val_fn: 34178.0000 - val_accuracy: 0.9886 - val_precision: 0.9474 - val_recall: 0.8772 - val_auc: 0.9855\n",
            "Epoch 35/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0477 - tp: 247102.2963 - fp: 24609.8642 - tn: 3990986.9753 - fn: 39726.0494 - accuracy: 0.9850 - precision: 0.9093 - recall: 0.8613 - auc: 0.9856 - val_loss: 0.0405 - val_tp: 244186.0000 - val_fp: 13353.0000 - val_tn: 3882441.0000 - val_fn: 34085.0000 - val_accuracy: 0.9886 - val_precision: 0.9482 - val_recall: 0.8775 - val_auc: 0.9857\n",
            "Epoch 36/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0466 - tp: 248690.6790 - fp: 23742.9012 - tn: 3991853.9383 - fn: 38137.6667 - accuracy: 0.9855 - precision: 0.9121 - recall: 0.8661 - auc: 0.9859 - val_loss: 0.0398 - val_tp: 245602.0000 - val_fp: 13289.0000 - val_tn: 3882505.0000 - val_fn: 32669.0000 - val_accuracy: 0.9890 - val_precision: 0.9487 - val_recall: 0.8826 - val_auc: 0.9858\n",
            "Epoch 37/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0452 - tp: 250332.7160 - fp: 22932.0988 - tn: 3992664.7407 - fn: 36495.6296 - accuracy: 0.9862 - precision: 0.9162 - recall: 0.8727 - auc: 0.9864 - val_loss: 0.0392 - val_tp: 247208.0000 - val_fp: 13398.0000 - val_tn: 3882396.0000 - val_fn: 31063.0000 - val_accuracy: 0.9893 - val_precision: 0.9486 - val_recall: 0.8884 - val_auc: 0.9859\n",
            "Epoch 38/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0441 - tp: 251609.2469 - fp: 22396.6420 - tn: 3993200.1975 - fn: 35219.0988 - accuracy: 0.9866 - precision: 0.9181 - recall: 0.8770 - auc: 0.9868 - val_loss: 0.0386 - val_tp: 249713.0000 - val_fp: 13351.0000 - val_tn: 3882443.0000 - val_fn: 28558.0000 - val_accuracy: 0.9900 - val_precision: 0.9492 - val_recall: 0.8974 - val_auc: 0.9860\n",
            "Epoch 39/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0428 - tp: 253053.3210 - fp: 21792.1975 - tn: 3993804.6420 - fn: 33775.0247 - accuracy: 0.9871 - precision: 0.9207 - recall: 0.8818 - auc: 0.9871 - val_loss: 0.0381 - val_tp: 251749.0000 - val_fp: 13330.0000 - val_tn: 3882464.0000 - val_fn: 26522.0000 - val_accuracy: 0.9905 - val_precision: 0.9497 - val_recall: 0.9047 - val_auc: 0.9861\n",
            "Epoch 40/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0418 - tp: 254316.3333 - fp: 21523.3704 - tn: 3994073.4691 - fn: 32512.0123 - accuracy: 0.9874 - precision: 0.9219 - recall: 0.8866 - auc: 0.9877 - val_loss: 0.0376 - val_tp: 253133.0000 - val_fp: 13367.0000 - val_tn: 3882427.0000 - val_fn: 25138.0000 - val_accuracy: 0.9908 - val_precision: 0.9498 - val_recall: 0.9097 - val_auc: 0.9863\n",
            "Epoch 41/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0409 - tp: 255132.2222 - fp: 21428.6296 - tn: 3994168.2099 - fn: 31696.1235 - accuracy: 0.9876 - precision: 0.9224 - recall: 0.8891 - auc: 0.9881 - val_loss: 0.0371 - val_tp: 254574.0000 - val_fp: 13389.0000 - val_tn: 3882405.0000 - val_fn: 23697.0000 - val_accuracy: 0.9911 - val_precision: 0.9500 - val_recall: 0.9148 - val_auc: 0.9865\n",
            "Epoch 42/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0400 - tp: 256299.5185 - fp: 21049.3333 - tn: 3994547.5062 - fn: 30528.8272 - accuracy: 0.9880 - precision: 0.9239 - recall: 0.8933 - auc: 0.9882 - val_loss: 0.0367 - val_tp: 255474.0000 - val_fp: 14516.0000 - val_tn: 3881278.0000 - val_fn: 22797.0000 - val_accuracy: 0.9911 - val_precision: 0.9462 - val_recall: 0.9181 - val_auc: 0.9865\n",
            "Epoch 43/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0391 - tp: 256944.3827 - fp: 20842.9012 - tn: 3994753.9383 - fn: 29883.9630 - accuracy: 0.9882 - precision: 0.9252 - recall: 0.8959 - auc: 0.9886 - val_loss: 0.0362 - val_tp: 255794.0000 - val_fp: 16359.0000 - val_tn: 3879435.0000 - val_fn: 22477.0000 - val_accuracy: 0.9907 - val_precision: 0.9399 - val_recall: 0.9192 - val_auc: 0.9867\n",
            "Epoch 44/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0382 - tp: 257935.0494 - fp: 20450.7654 - tn: 3995146.0741 - fn: 28893.2963 - accuracy: 0.9885 - precision: 0.9266 - recall: 0.8993 - auc: 0.9890 - val_loss: 0.0358 - val_tp: 256076.0000 - val_fp: 17691.0000 - val_tn: 3878103.0000 - val_fn: 22195.0000 - val_accuracy: 0.9904 - val_precision: 0.9354 - val_recall: 0.9202 - val_auc: 0.9868\n",
            "Epoch 45/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0377 - tp: 258621.8889 - fp: 20394.0494 - tn: 3995202.7901 - fn: 28206.4568 - accuracy: 0.9887 - precision: 0.9270 - recall: 0.9016 - auc: 0.9892 - val_loss: 0.0354 - val_tp: 256009.0000 - val_fp: 19051.0000 - val_tn: 3876743.0000 - val_fn: 22262.0000 - val_accuracy: 0.9901 - val_precision: 0.9307 - val_recall: 0.9200 - val_auc: 0.9870\n",
            "Epoch 46/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0368 - tp: 259135.1481 - fp: 20091.5185 - tn: 3995505.3210 - fn: 27693.1975 - accuracy: 0.9889 - precision: 0.9281 - recall: 0.9034 - auc: 0.9897 - val_loss: 0.0351 - val_tp: 255674.0000 - val_fp: 19370.0000 - val_tn: 3876424.0000 - val_fn: 22597.0000 - val_accuracy: 0.9899 - val_precision: 0.9296 - val_recall: 0.9188 - val_auc: 0.9872\n",
            "Epoch 47/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0362 - tp: 259920.4815 - fp: 19811.0494 - tn: 3995785.7901 - fn: 26907.8642 - accuracy: 0.9891 - precision: 0.9292 - recall: 0.9062 - auc: 0.9899 - val_loss: 0.0347 - val_tp: 255654.0000 - val_fp: 19355.0000 - val_tn: 3876439.0000 - val_fn: 22617.0000 - val_accuracy: 0.9899 - val_precision: 0.9296 - val_recall: 0.9187 - val_auc: 0.9873\n",
            "Epoch 48/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0354 - tp: 260416.1605 - fp: 19643.6914 - tn: 3995953.1481 - fn: 26412.1852 - accuracy: 0.9893 - precision: 0.9297 - recall: 0.9076 - auc: 0.9904 - val_loss: 0.0344 - val_tp: 255780.0000 - val_fp: 19598.0000 - val_tn: 3876196.0000 - val_fn: 22491.0000 - val_accuracy: 0.9899 - val_precision: 0.9288 - val_recall: 0.9192 - val_auc: 0.9873\n",
            "Epoch 49/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0348 - tp: 260928.4321 - fp: 19503.4321 - tn: 3996093.4074 - fn: 25899.9136 - accuracy: 0.9894 - precision: 0.9301 - recall: 0.9095 - auc: 0.9906 - val_loss: 0.0340 - val_tp: 255709.0000 - val_fp: 20192.0000 - val_tn: 3875602.0000 - val_fn: 22562.0000 - val_accuracy: 0.9898 - val_precision: 0.9268 - val_recall: 0.9189 - val_auc: 0.9874\n",
            "Epoch 50/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0342 - tp: 261418.6296 - fp: 19206.5926 - tn: 3996390.2469 - fn: 25409.7160 - accuracy: 0.9896 - precision: 0.9316 - recall: 0.9113 - auc: 0.9910 - val_loss: 0.0337 - val_tp: 255874.0000 - val_fp: 20301.0000 - val_tn: 3875493.0000 - val_fn: 22397.0000 - val_accuracy: 0.9898 - val_precision: 0.9265 - val_recall: 0.9195 - val_auc: 0.9875\n",
            "Epoch 51/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0337 - tp: 261886.4568 - fp: 18883.6296 - tn: 3996713.2099 - fn: 24941.8889 - accuracy: 0.9898 - precision: 0.9326 - recall: 0.9128 - auc: 0.9911 - val_loss: 0.0335 - val_tp: 255790.0000 - val_fp: 20469.0000 - val_tn: 3875325.0000 - val_fn: 22481.0000 - val_accuracy: 0.9897 - val_precision: 0.9259 - val_recall: 0.9192 - val_auc: 0.9875\n",
            "Epoch 52/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0328 - tp: 262417.3333 - fp: 18605.4074 - tn: 3996991.4321 - fn: 24411.0123 - accuracy: 0.9900 - precision: 0.9337 - recall: 0.9148 - auc: 0.9917 - val_loss: 0.0333 - val_tp: 255974.0000 - val_fp: 20411.0000 - val_tn: 3875383.0000 - val_fn: 22297.0000 - val_accuracy: 0.9898 - val_precision: 0.9262 - val_recall: 0.9199 - val_auc: 0.9877\n",
            "Epoch 53/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0326 - tp: 262756.6420 - fp: 18431.9136 - tn: 3997164.9259 - fn: 24071.7037 - accuracy: 0.9901 - precision: 0.9341 - recall: 0.9157 - auc: 0.9917 - val_loss: 0.0330 - val_tp: 256034.0000 - val_fp: 20336.0000 - val_tn: 3875458.0000 - val_fn: 22237.0000 - val_accuracy: 0.9898 - val_precision: 0.9264 - val_recall: 0.9201 - val_auc: 0.9878\n",
            "Epoch 54/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0317 - tp: 263045.1235 - fp: 18242.7284 - tn: 3997354.1111 - fn: 23783.2222 - accuracy: 0.9903 - precision: 0.9353 - recall: 0.9173 - auc: 0.9920 - val_loss: 0.0328 - val_tp: 256074.0000 - val_fp: 20188.0000 - val_tn: 3875606.0000 - val_fn: 22197.0000 - val_accuracy: 0.9898 - val_precision: 0.9269 - val_recall: 0.9202 - val_auc: 0.9886\n",
            "Epoch 55/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0314 - tp: 263243.5185 - fp: 18180.3951 - tn: 3997416.4444 - fn: 23584.8272 - accuracy: 0.9903 - precision: 0.9356 - recall: 0.9178 - auc: 0.9921 - val_loss: 0.0326 - val_tp: 255946.0000 - val_fp: 20131.0000 - val_tn: 3875663.0000 - val_fn: 22325.0000 - val_accuracy: 0.9898 - val_precision: 0.9271 - val_recall: 0.9198 - val_auc: 0.9882\n",
            "Epoch 56/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0309 - tp: 263758.9630 - fp: 17795.3827 - tn: 3997801.4568 - fn: 23069.3827 - accuracy: 0.9905 - precision: 0.9367 - recall: 0.9194 - auc: 0.9924 - val_loss: 0.0323 - val_tp: 256022.0000 - val_fp: 20452.0000 - val_tn: 3875342.0000 - val_fn: 22249.0000 - val_accuracy: 0.9898 - val_precision: 0.9260 - val_recall: 0.9200 - val_auc: 0.9883\n",
            "Epoch 57/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0304 - tp: 263967.1852 - fp: 17691.7531 - tn: 3997905.0864 - fn: 22861.1605 - accuracy: 0.9906 - precision: 0.9371 - recall: 0.9201 - auc: 0.9925 - val_loss: 0.0321 - val_tp: 256046.0000 - val_fp: 20482.0000 - val_tn: 3875312.0000 - val_fn: 22225.0000 - val_accuracy: 0.9898 - val_precision: 0.9259 - val_recall: 0.9201 - val_auc: 0.9884\n",
            "Epoch 58/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0298 - tp: 264403.0741 - fp: 17383.8889 - tn: 3998212.9506 - fn: 22425.2716 - accuracy: 0.9908 - precision: 0.9384 - recall: 0.9221 - auc: 0.9928 - val_loss: 0.0319 - val_tp: 256136.0000 - val_fp: 20398.0000 - val_tn: 3875396.0000 - val_fn: 22135.0000 - val_accuracy: 0.9898 - val_precision: 0.9262 - val_recall: 0.9205 - val_auc: 0.9885\n",
            "Epoch 59/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0295 - tp: 264561.6914 - fp: 17232.0617 - tn: 3998364.7778 - fn: 22266.6543 - accuracy: 0.9908 - precision: 0.9385 - recall: 0.9219 - auc: 0.9928 - val_loss: 0.0317 - val_tp: 256202.0000 - val_fp: 20343.0000 - val_tn: 3875451.0000 - val_fn: 22069.0000 - val_accuracy: 0.9898 - val_precision: 0.9264 - val_recall: 0.9207 - val_auc: 0.9886\n",
            "Epoch 60/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0293 - tp: 264728.2469 - fp: 17106.0370 - tn: 3998490.8025 - fn: 22100.0988 - accuracy: 0.9908 - precision: 0.9389 - recall: 0.9224 - auc: 0.9931 - val_loss: 0.0316 - val_tp: 256194.0000 - val_fp: 20361.0000 - val_tn: 3875433.0000 - val_fn: 22077.0000 - val_accuracy: 0.9898 - val_precision: 0.9264 - val_recall: 0.9207 - val_auc: 0.9887\n",
            "Epoch 61/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0287 - tp: 265165.1852 - fp: 16828.3086 - tn: 3998768.5309 - fn: 21663.1605 - accuracy: 0.9910 - precision: 0.9401 - recall: 0.9243 - auc: 0.9931 - val_loss: 0.0315 - val_tp: 256231.0000 - val_fp: 20325.0000 - val_tn: 3875469.0000 - val_fn: 22040.0000 - val_accuracy: 0.9899 - val_precision: 0.9265 - val_recall: 0.9208 - val_auc: 0.9889\n",
            "Epoch 62/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0283 - tp: 265436.0617 - fp: 16573.5679 - tn: 3999023.2716 - fn: 21392.2840 - accuracy: 0.9912 - precision: 0.9411 - recall: 0.9253 - auc: 0.9934 - val_loss: 0.0314 - val_tp: 256391.0000 - val_fp: 20248.0000 - val_tn: 3875546.0000 - val_fn: 21880.0000 - val_accuracy: 0.9899 - val_precision: 0.9268 - val_recall: 0.9214 - val_auc: 0.9892\n",
            "Epoch 63/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0279 - tp: 265627.1605 - fp: 16444.5309 - tn: 3999152.3086 - fn: 21201.1852 - accuracy: 0.9912 - precision: 0.9414 - recall: 0.9258 - auc: 0.9935 - val_loss: 0.0312 - val_tp: 256355.0000 - val_fp: 20175.0000 - val_tn: 3875619.0000 - val_fn: 21916.0000 - val_accuracy: 0.9899 - val_precision: 0.9270 - val_recall: 0.9212 - val_auc: 0.9892\n",
            "Epoch 64/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0272 - tp: 265844.1235 - fp: 16179.9753 - tn: 3999416.8642 - fn: 20984.2222 - accuracy: 0.9914 - precision: 0.9428 - recall: 0.9271 - auc: 0.9938 - val_loss: 0.0311 - val_tp: 256344.0000 - val_fp: 20156.0000 - val_tn: 3875638.0000 - val_fn: 21927.0000 - val_accuracy: 0.9899 - val_precision: 0.9271 - val_recall: 0.9212 - val_auc: 0.9892\n",
            "Epoch 65/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0269 - tp: 266088.0247 - fp: 15991.1605 - tn: 3999605.6790 - fn: 20740.3210 - accuracy: 0.9915 - precision: 0.9433 - recall: 0.9277 - auc: 0.9939 - val_loss: 0.0309 - val_tp: 256375.0000 - val_fp: 20085.0000 - val_tn: 3875709.0000 - val_fn: 21896.0000 - val_accuracy: 0.9899 - val_precision: 0.9273 - val_recall: 0.9213 - val_auc: 0.9893\n",
            "Epoch 66/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0268 - tp: 266268.6790 - fp: 15817.5432 - tn: 3999779.2963 - fn: 20559.6667 - accuracy: 0.9915 - precision: 0.9438 - recall: 0.9282 - auc: 0.9939 - val_loss: 0.0308 - val_tp: 256372.0000 - val_fp: 19973.0000 - val_tn: 3875821.0000 - val_fn: 21899.0000 - val_accuracy: 0.9900 - val_precision: 0.9277 - val_recall: 0.9213 - val_auc: 0.9893\n",
            "Epoch 67/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0262 - tp: 266585.8395 - fp: 15599.4321 - tn: 3999997.4074 - fn: 20242.5062 - accuracy: 0.9917 - precision: 0.9446 - recall: 0.9294 - auc: 0.9939 - val_loss: 0.0307 - val_tp: 256365.0000 - val_fp: 19724.0000 - val_tn: 3876070.0000 - val_fn: 21906.0000 - val_accuracy: 0.9900 - val_precision: 0.9286 - val_recall: 0.9213 - val_auc: 0.9894\n",
            "Epoch 68/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0257 - tp: 266806.4074 - fp: 15433.8272 - tn: 4000163.0123 - fn: 20021.9383 - accuracy: 0.9918 - precision: 0.9456 - recall: 0.9306 - auc: 0.9942 - val_loss: 0.0306 - val_tp: 256522.0000 - val_fp: 19805.0000 - val_tn: 3875989.0000 - val_fn: 21749.0000 - val_accuracy: 0.9900 - val_precision: 0.9283 - val_recall: 0.9218 - val_auc: 0.9894\n",
            "Epoch 69/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0257 - tp: 266903.0617 - fp: 15382.5556 - tn: 4000214.2840 - fn: 19925.2840 - accuracy: 0.9918 - precision: 0.9454 - recall: 0.9305 - auc: 0.9942 - val_loss: 0.0306 - val_tp: 256526.0000 - val_fp: 19872.0000 - val_tn: 3875922.0000 - val_fn: 21745.0000 - val_accuracy: 0.9900 - val_precision: 0.9281 - val_recall: 0.9219 - val_auc: 0.9895\n",
            "Epoch 70/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0252 - tp: 267092.2346 - fp: 15172.3580 - tn: 4000424.4815 - fn: 19736.1111 - accuracy: 0.9919 - precision: 0.9462 - recall: 0.9311 - auc: 0.9944 - val_loss: 0.0305 - val_tp: 256621.0000 - val_fp: 19720.0000 - val_tn: 3876074.0000 - val_fn: 21650.0000 - val_accuracy: 0.9901 - val_precision: 0.9286 - val_recall: 0.9222 - val_auc: 0.9895\n",
            "Epoch 71/100\n",
            "80/80 [==============================] - 1s 16ms/step - loss: 0.0250 - tp: 267229.7160 - fp: 14991.1975 - tn: 4000605.6420 - fn: 19598.6296 - accuracy: 0.9919 - precision: 0.9467 - recall: 0.9316 - auc: 0.9945 - val_loss: 0.0303 - val_tp: 256588.0000 - val_fp: 19847.0000 - val_tn: 3875947.0000 - val_fn: 21683.0000 - val_accuracy: 0.9901 - val_precision: 0.9282 - val_recall: 0.9221 - val_auc: 0.9897\n",
            "Epoch 72/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0245 - tp: 267437.4198 - fp: 14855.4198 - tn: 4000741.4198 - fn: 19390.9259 - accuracy: 0.9921 - precision: 0.9474 - recall: 0.9325 - auc: 0.9946 - val_loss: 0.0302 - val_tp: 256728.0000 - val_fp: 19769.0000 - val_tn: 3876025.0000 - val_fn: 21543.0000 - val_accuracy: 0.9901 - val_precision: 0.9285 - val_recall: 0.9226 - val_auc: 0.9897\n",
            "Epoch 73/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0245 - tp: 267446.1605 - fp: 14760.5309 - tn: 4000836.3086 - fn: 19382.1852 - accuracy: 0.9920 - precision: 0.9472 - recall: 0.9319 - auc: 0.9947 - val_loss: 0.0303 - val_tp: 256817.0000 - val_fp: 19583.0000 - val_tn: 3876211.0000 - val_fn: 21454.0000 - val_accuracy: 0.9902 - val_precision: 0.9291 - val_recall: 0.9229 - val_auc: 0.9897\n",
            "Epoch 74/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0239 - tp: 267810.3951 - fp: 14553.8395 - tn: 4001043.0000 - fn: 19017.9506 - accuracy: 0.9922 - precision: 0.9486 - recall: 0.9339 - auc: 0.9947 - val_loss: 0.0301 - val_tp: 256852.0000 - val_fp: 19468.0000 - val_tn: 3876326.0000 - val_fn: 21419.0000 - val_accuracy: 0.9902 - val_precision: 0.9295 - val_recall: 0.9230 - val_auc: 0.9899\n",
            "Epoch 75/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0236 - tp: 268046.3333 - fp: 14303.0123 - tn: 4001293.8272 - fn: 18782.0123 - accuracy: 0.9923 - precision: 0.9494 - recall: 0.9345 - auc: 0.9950 - val_loss: 0.0301 - val_tp: 256801.0000 - val_fp: 19476.0000 - val_tn: 3876318.0000 - val_fn: 21470.0000 - val_accuracy: 0.9902 - val_precision: 0.9295 - val_recall: 0.9228 - val_auc: 0.9901\n",
            "Epoch 76/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0235 - tp: 268097.5679 - fp: 14185.1605 - tn: 4001411.6790 - fn: 18730.7778 - accuracy: 0.9923 - precision: 0.9497 - recall: 0.9346 - auc: 0.9950 - val_loss: 0.0301 - val_tp: 256659.0000 - val_fp: 19599.0000 - val_tn: 3876195.0000 - val_fn: 21612.0000 - val_accuracy: 0.9901 - val_precision: 0.9291 - val_recall: 0.9223 - val_auc: 0.9902\n",
            "Epoch 77/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0232 - tp: 268333.1975 - fp: 14013.2840 - tn: 4001583.5556 - fn: 18495.1481 - accuracy: 0.9924 - precision: 0.9502 - recall: 0.9353 - auc: 0.9951 - val_loss: 0.0299 - val_tp: 256721.0000 - val_fp: 19610.0000 - val_tn: 3876184.0000 - val_fn: 21550.0000 - val_accuracy: 0.9901 - val_precision: 0.9290 - val_recall: 0.9226 - val_auc: 0.9903\n",
            "Epoch 78/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0229 - tp: 268474.5926 - fp: 13837.7654 - tn: 4001759.0741 - fn: 18353.7531 - accuracy: 0.9925 - precision: 0.9511 - recall: 0.9361 - auc: 0.9952 - val_loss: 0.0301 - val_tp: 256642.0000 - val_fp: 19719.0000 - val_tn: 3876075.0000 - val_fn: 21629.0000 - val_accuracy: 0.9901 - val_precision: 0.9286 - val_recall: 0.9223 - val_auc: 0.9903\n",
            "Epoch 79/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0227 - tp: 268616.0123 - fp: 13687.7407 - tn: 4001909.0988 - fn: 18212.3333 - accuracy: 0.9926 - precision: 0.9516 - recall: 0.9366 - auc: 0.9953 - val_loss: 0.0301 - val_tp: 256658.0000 - val_fp: 19677.0000 - val_tn: 3876117.0000 - val_fn: 21613.0000 - val_accuracy: 0.9901 - val_precision: 0.9288 - val_recall: 0.9223 - val_auc: 0.9903\n",
            "Epoch 80/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0223 - tp: 268816.7778 - fp: 13659.1481 - tn: 4001937.6914 - fn: 18011.5679 - accuracy: 0.9926 - precision: 0.9516 - recall: 0.9371 - auc: 0.9954 - val_loss: 0.0301 - val_tp: 256859.0000 - val_fp: 19580.0000 - val_tn: 3876214.0000 - val_fn: 21412.0000 - val_accuracy: 0.9902 - val_precision: 0.9292 - val_recall: 0.9231 - val_auc: 0.9903\n",
            "Epoch 81/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0220 - tp: 269218.1481 - fp: 13320.7284 - tn: 4002276.1111 - fn: 17610.1975 - accuracy: 0.9928 - precision: 0.9529 - recall: 0.9386 - auc: 0.9955 - val_loss: 0.0302 - val_tp: 256949.0000 - val_fp: 19442.0000 - val_tn: 3876352.0000 - val_fn: 21322.0000 - val_accuracy: 0.9902 - val_precision: 0.9297 - val_recall: 0.9234 - val_auc: 0.9903\n",
            "Epoch 82/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0217 - tp: 269396.8395 - fp: 13100.9012 - tn: 4002495.9383 - fn: 17431.5062 - accuracy: 0.9929 - precision: 0.9537 - recall: 0.9392 - auc: 0.9956 - val_loss: 0.0303 - val_tp: 256769.0000 - val_fp: 19629.0000 - val_tn: 3876165.0000 - val_fn: 21502.0000 - val_accuracy: 0.9901 - val_precision: 0.9290 - val_recall: 0.9227 - val_auc: 0.9900\n",
            "Epoch 83/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0217 - tp: 269465.5432 - fp: 13174.1111 - tn: 4002422.7284 - fn: 17362.8025 - accuracy: 0.9929 - precision: 0.9532 - recall: 0.9392 - auc: 0.9956 - val_loss: 0.0302 - val_tp: 256598.0000 - val_fp: 19719.0000 - val_tn: 3876075.0000 - val_fn: 21673.0000 - val_accuracy: 0.9901 - val_precision: 0.9286 - val_recall: 0.9221 - val_auc: 0.9904\n",
            "Epoch 84/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0213 - tp: 269811.4444 - fp: 12798.7901 - tn: 4002798.0494 - fn: 17016.9012 - accuracy: 0.9931 - precision: 0.9547 - recall: 0.9406 - auc: 0.9958 - val_loss: 0.0303 - val_tp: 256824.0000 - val_fp: 19524.0000 - val_tn: 3876270.0000 - val_fn: 21447.0000 - val_accuracy: 0.9902 - val_precision: 0.9293 - val_recall: 0.9229 - val_auc: 0.9904\n",
            "Epoch 85/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0209 - tp: 269944.1358 - fp: 12660.8519 - tn: 4002935.9877 - fn: 16884.2099 - accuracy: 0.9931 - precision: 0.9552 - recall: 0.9413 - auc: 0.9960 - val_loss: 0.0303 - val_tp: 257128.0000 - val_fp: 19267.0000 - val_tn: 3876527.0000 - val_fn: 21143.0000 - val_accuracy: 0.9903 - val_precision: 0.9303 - val_recall: 0.9240 - val_auc: 0.9904\n",
            "Epoch 86/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0209 - tp: 270042.3333 - fp: 12675.3457 - tn: 4002921.4938 - fn: 16786.0123 - accuracy: 0.9931 - precision: 0.9550 - recall: 0.9413 - auc: 0.9960 - val_loss: 0.0304 - val_tp: 257313.0000 - val_fp: 19267.0000 - val_tn: 3876527.0000 - val_fn: 20958.0000 - val_accuracy: 0.9904 - val_precision: 0.9303 - val_recall: 0.9247 - val_auc: 0.9903\n",
            "Epoch 87/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0204 - tp: 270219.2963 - fp: 12400.4568 - tn: 4003196.3827 - fn: 16609.0494 - accuracy: 0.9933 - precision: 0.9564 - recall: 0.9424 - auc: 0.9961 - val_loss: 0.0306 - val_tp: 257258.0000 - val_fp: 19374.0000 - val_tn: 3876420.0000 - val_fn: 21013.0000 - val_accuracy: 0.9903 - val_precision: 0.9300 - val_recall: 0.9245 - val_auc: 0.9902\n",
            "Epoch 88/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0204 - tp: 270290.1975 - fp: 12327.9877 - tn: 4003268.8519 - fn: 16538.1481 - accuracy: 0.9933 - precision: 0.9563 - recall: 0.9423 - auc: 0.9960 - val_loss: 0.0304 - val_tp: 257323.0000 - val_fp: 19345.0000 - val_tn: 3876449.0000 - val_fn: 20948.0000 - val_accuracy: 0.9903 - val_precision: 0.9301 - val_recall: 0.9247 - val_auc: 0.9905\n",
            "Epoch 89/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0202 - tp: 270590.0370 - fp: 12098.3210 - tn: 4003498.5185 - fn: 16238.3086 - accuracy: 0.9934 - precision: 0.9570 - recall: 0.9431 - auc: 0.9961 - val_loss: 0.0306 - val_tp: 257414.0000 - val_fp: 19228.0000 - val_tn: 3876566.0000 - val_fn: 20857.0000 - val_accuracy: 0.9904 - val_precision: 0.9305 - val_recall: 0.9250 - val_auc: 0.9904\n",
            "Epoch 90/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0199 - tp: 270688.6667 - fp: 12055.9012 - tn: 4003540.9383 - fn: 16139.6790 - accuracy: 0.9934 - precision: 0.9572 - recall: 0.9437 - auc: 0.9962 - val_loss: 0.0304 - val_tp: 257512.0000 - val_fp: 19182.0000 - val_tn: 3876612.0000 - val_fn: 20759.0000 - val_accuracy: 0.9904 - val_precision: 0.9307 - val_recall: 0.9254 - val_auc: 0.9905\n",
            "Epoch 91/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0198 - tp: 270908.0370 - fp: 11729.1852 - tn: 4003867.6543 - fn: 15920.3086 - accuracy: 0.9936 - precision: 0.9586 - recall: 0.9445 - auc: 0.9961 - val_loss: 0.0303 - val_tp: 257644.0000 - val_fp: 19013.0000 - val_tn: 3876781.0000 - val_fn: 20627.0000 - val_accuracy: 0.9905 - val_precision: 0.9313 - val_recall: 0.9259 - val_auc: 0.9905\n",
            "Epoch 92/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0194 - tp: 271042.6296 - fp: 11674.7531 - tn: 4003922.0864 - fn: 15785.7160 - accuracy: 0.9936 - precision: 0.9587 - recall: 0.9450 - auc: 0.9963 - val_loss: 0.0304 - val_tp: 257556.0000 - val_fp: 19177.0000 - val_tn: 3876617.0000 - val_fn: 20715.0000 - val_accuracy: 0.9904 - val_precision: 0.9307 - val_recall: 0.9256 - val_auc: 0.9905\n",
            "Epoch 93/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0191 - tp: 271293.8642 - fp: 11437.2963 - tn: 4004159.5432 - fn: 15534.4815 - accuracy: 0.9937 - precision: 0.9596 - recall: 0.9457 - auc: 0.9963 - val_loss: 0.0307 - val_tp: 257561.0000 - val_fp: 19328.0000 - val_tn: 3876466.0000 - val_fn: 20710.0000 - val_accuracy: 0.9904 - val_precision: 0.9302 - val_recall: 0.9256 - val_auc: 0.9904\n",
            "Epoch 94/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0191 - tp: 271398.3951 - fp: 11274.2840 - tn: 4004322.5556 - fn: 15429.9506 - accuracy: 0.9938 - precision: 0.9601 - recall: 0.9460 - auc: 0.9963 - val_loss: 0.0309 - val_tp: 257736.0000 - val_fp: 19451.0000 - val_tn: 3876343.0000 - val_fn: 20535.0000 - val_accuracy: 0.9904 - val_precision: 0.9298 - val_recall: 0.9262 - val_auc: 0.9903\n",
            "Epoch 95/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0188 - tp: 271578.5432 - fp: 11146.8642 - tn: 4004449.9753 - fn: 15249.8025 - accuracy: 0.9938 - precision: 0.9605 - recall: 0.9467 - auc: 0.9963 - val_loss: 0.0311 - val_tp: 257592.0000 - val_fp: 19644.0000 - val_tn: 3876150.0000 - val_fn: 20679.0000 - val_accuracy: 0.9903 - val_precision: 0.9291 - val_recall: 0.9257 - val_auc: 0.9903\n",
            "Epoch 96/100\n",
            "80/80 [==============================] - 1s 19ms/step - loss: 0.0185 - tp: 271823.6790 - fp: 10959.7531 - tn: 4004637.0864 - fn: 15004.6667 - accuracy: 0.9940 - precision: 0.9612 - recall: 0.9477 - auc: 0.9963 - val_loss: 0.0308 - val_tp: 257719.0000 - val_fp: 19521.0000 - val_tn: 3876273.0000 - val_fn: 20552.0000 - val_accuracy: 0.9904 - val_precision: 0.9296 - val_recall: 0.9261 - val_auc: 0.9904\n",
            "Epoch 97/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0183 - tp: 271990.4938 - fp: 10832.5062 - tn: 4004764.3333 - fn: 14837.8519 - accuracy: 0.9940 - precision: 0.9616 - recall: 0.9482 - auc: 0.9964 - val_loss: 0.0316 - val_tp: 257174.0000 - val_fp: 20131.0000 - val_tn: 3875663.0000 - val_fn: 21097.0000 - val_accuracy: 0.9901 - val_precision: 0.9274 - val_recall: 0.9242 - val_auc: 0.9901\n",
            "Epoch 98/100\n",
            "80/80 [==============================] - 1s 17ms/step - loss: 0.0184 - tp: 272095.5309 - fp: 10706.8519 - tn: 4004889.9877 - fn: 14732.8148 - accuracy: 0.9941 - precision: 0.9622 - recall: 0.9486 - auc: 0.9963 - val_loss: 0.0314 - val_tp: 257229.0000 - val_fp: 20086.0000 - val_tn: 3875708.0000 - val_fn: 21042.0000 - val_accuracy: 0.9901 - val_precision: 0.9276 - val_recall: 0.9244 - val_auc: 0.9902\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00098: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPDH0IA_8xee"
      },
      "source": [
        "y_pred=model_dnn.predict(X_test)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PdaWVhX9BvJ",
        "outputId": "59370904-9ec3-4df0-fe64-8526948521bc"
      },
      "source": [
        "accuracy = model_dnn.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)\r\n",
        "print('Accuracy is: ', accuracy)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 0s 10ms/step - loss: 0.0349 - tp: 253694.0000 - fp: 20036.0000 - tn: 3875758.0000 - fn: 24577.0000 - accuracy: 0.9893 - precision: 0.9268 - recall: 0.9117 - auc: 0.9880\n",
            "Accuracy is:  [0.034887660294771194, 253694.0, 20036.0, 3875758.0, 24577.0, 0.9893119931221008, 0.9268037676811218, 0.9116796255111694, 0.987982451915741]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZcyLQPqAlWS",
        "outputId": "e20403de-6aad-4844-cd51-0103ef6e08ff"
      },
      "source": [
        "display_metrics(y_test_ada, np.argmax(y_pred, axis = 1), labels_d)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.92\n",
            "\n",
            "Micro Precision: 0.92\n",
            "Micro Recall: 0.92\n",
            "Micro F1-score: 0.92\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.39\n",
            "Macro Recall: 0.28\n",
            "Macro F1-score: 0.28\n",
            "\n",
            "Weighted Precision: 0.90\n",
            "Weighted Recall: 0.92\n",
            "Weighted F1-score: 0.90\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.92      0.94    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.81      0.99      0.89     32006\n",
            "             DoS GoldenEye       0.36      0.23      0.28      2573\n",
            "                  DoS Hulk       0.91      0.96      0.94     57531\n",
            "          DoS Slowhttptest       1.00      0.01      0.01      1374\n",
            "             DoS slowloris       0.98      0.13      0.23      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.91      1.00      0.95     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.92    278271\n",
            "                 macro avg       0.39      0.28      0.28    278271\n",
            "              weighted avg       0.90      0.92      0.90    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erUa8gpIZ04U"
      },
      "source": [
        "# Model 10: CNN1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTmLBZDpZ04U"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Flatten, Dense, Activation,Dropout\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLXy3meMZ04U",
        "outputId": "f144a82f-a850-49c4-974e-1d2ae9ca3ca2"
      },
      "source": [
        "#hyper-params\n",
        "batch_size = 1024 # increasing batch size with more gpu added\n",
        "input_dim = X_train.shape[1]\n",
        "num_class = 15                   # 15 intrusion classes, including benign traffic class\n",
        "num_epochs = 30\n",
        "learning_rates = 1e-3\n",
        "regularizations = 1e-3\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "print(input_dim)\n",
        "print(num_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNRbsq6CZ04V",
        "outputId": "148ee859-e320-4a22-c118-cabe52ab7d56"
      },
      "source": [
        "#X_train_r = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_train_r = np.zeros((len(X_train), input_dim, 1))\n",
        "X_train_r[:, :, 0] = X_train[:, :input_dim]\n",
        "print(X_train_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjmIpn1HZ04W",
        "outputId": "2c1c2f93-014b-4c08-8435-e739318768fc"
      },
      "source": [
        "X_test_r = np.zeros((len(X_test), input_dim, 1))\n",
        "X_test_r[:, :, 0] = X_test[:, :input_dim]\n",
        "print(X_test_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_KlI1VeZ04W",
        "outputId": "68ffab00-7db7-42ca-d671-16330e9fa8f9"
      },
      "source": [
        "X_val_r = np.zeros((len(X_val), input_dim, 1))\n",
        "X_val_r[:, :, 0] = X_val[:, :input_dim]\n",
        "print(X_val_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkQlzQU0ajuD",
        "outputId": "71d58c31-8c34-435a-934b-9006668ffdc5"
      },
      "source": [
        "X_train_r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[7.66048676e-01],\n",
              "        [5.93603125e-03],\n",
              "        [1.00000000e+00],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[7.29167620e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [7.12500000e-01],\n",
              "        [7.12500000e-01]],\n",
              "\n",
              "       [[7.86144808e-01],\n",
              "        [6.76005616e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [8.96864890e-03],\n",
              "        [4.90833333e-01],\n",
              "        [4.83333333e-01]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[2.53009842e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[7.85610742e-01],\n",
              "        [8.08765183e-04],\n",
              "        [1.00000000e+00],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[8.26276036e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpTZU5OPZ04W",
        "outputId": "31aa9725-0fe7-4c0d-9ab7-1b4176ad7fcd"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', input_shape=(71,1)))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=3))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_class))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_8 (Conv1D)            (None, 71, 32)            128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 71, 32)            284       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 71, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 69, 128)           12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 69, 128)           276       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 69, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8832)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               883300    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 15)                1515      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 897,919\n",
            "Trainable params: 897,639\n",
            "Non-trainable params: 280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEpzDBNyC7P2"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_accuracy', \r\n",
        "    verbose=1,\r\n",
        "    patience=10,\r\n",
        "    mode='max',\r\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb52JhHSZ04W"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFJZ11t1Z04X"
      },
      "source": [
        "## Step 5. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq31n_0YZ04X",
        "outputId": "ad9b4e62-35a9-429f-9d10-4086725a878f"
      },
      "source": [
        "# fit network\n",
        "epochs = 50\n",
        "model.fit(X_train_r, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_r, y_test), verbose=1, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.9541 - val_accuracy: 0.8272\n",
            "Epoch 2/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.5939 - val_accuracy: 0.8978\n",
            "Epoch 3/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0106 - accuracy: 0.9963 - val_loss: 0.4867 - val_accuracy: 0.9373\n",
            "Epoch 4/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 1.2544 - val_accuracy: 0.7860\n",
            "Epoch 5/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.5194 - val_accuracy: 0.9147\n",
            "Epoch 6/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 0.7574 - val_accuracy: 0.8717\n",
            "Epoch 7/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.4678 - val_accuracy: 0.9551\n",
            "Epoch 8/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.3560 - val_accuracy: 0.9581\n",
            "Epoch 9/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 0.7849 - val_accuracy: 0.8832\n",
            "Epoch 10/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0091 - accuracy: 0.9967 - val_loss: 0.3110 - val_accuracy: 0.9654\n",
            "Epoch 11/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 0.7204 - val_accuracy: 0.8893\n",
            "Epoch 12/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 0.5198 - val_accuracy: 0.9458\n",
            "Epoch 13/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0090 - accuracy: 0.9967 - val_loss: 0.9469 - val_accuracy: 0.8524\n",
            "Epoch 14/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0085 - accuracy: 0.9968 - val_loss: 0.3824 - val_accuracy: 0.9637\n",
            "Epoch 15/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 0.3664 - val_accuracy: 0.9600\n",
            "Epoch 16/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0088 - accuracy: 0.9967 - val_loss: 0.5777 - val_accuracy: 0.9319\n",
            "Epoch 17/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0087 - accuracy: 0.9967 - val_loss: 0.3704 - val_accuracy: 0.9621\n",
            "Epoch 18/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0086 - accuracy: 0.9968 - val_loss: 0.4081 - val_accuracy: 0.9647\n",
            "Epoch 19/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.4933 - val_accuracy: 0.9559\n",
            "Epoch 20/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 1.0666 - val_accuracy: 0.8716\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00020: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7fd100bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSo0TpT5Z04X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07be9f6-400d-481c-8ee2-ebceae01716b"
      },
      "source": [
        "# evaluate model\n",
        "accuracy = model.evaluate(X_val_r, y_val, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "272/272 [==============================] - 2s 6ms/step - loss: 79.3116 - accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqGNmOrhH1CL"
      },
      "source": [
        "y_pred=model.predict(X_val_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltyd2KVLIBCK",
        "outputId": "aad22263-814e-4f9e-a3c2-621c490ef3df"
      },
      "source": [
        "display_metrics(y_val_ada, np.argmax(y_pred, axis = 1), labels_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.50\n",
            "\n",
            "Micro Precision: 0.50\n",
            "Micro Recall: 0.50\n",
            "Micro F1-score: 0.50\n",
            "\n",
            "Macro Precision: 0.03\n",
            "Macro Recall: 0.07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.04\n",
            "\n",
            "Weighted Precision: 0.25\n",
            "Weighted Recall: 0.50\n",
            "Weighted F1-score: 0.33\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.50      1.00      0.67    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.00      0.00      0.00     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         2\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.50    278270\n",
            "                 macro avg       0.03      0.07      0.04    278270\n",
            "              weighted avg       0.25      0.50      0.33    278270\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbhp6oW_Z04X"
      },
      "source": [
        ""
      ]
    }
  ]
}