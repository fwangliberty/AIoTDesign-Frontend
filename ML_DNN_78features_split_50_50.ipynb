{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ML_DNN_78features_split_50_50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwangliberty/AIoTDesign-Frontend/blob/master/ML_DNN_78features_split_50_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq5p9swvZ04G"
      },
      "source": [
        "# Baseline Models for CICIDS 2017 Data Set with 78 Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXg4HMXOZ04H"
      },
      "source": [
        "We use the pre-processing dataset from mlp4nids (Multi-layer perceptron for network intrusion detection) https://github.com/ArnaudRosay/mlp4nids. The dataset is augmented by 7 new connection based features, and splitted into 50:25:25.  We use the following classification methods: PCA+RF,Naive Bayes model, Decision Tree Classifier, Random Foresty with DecisionTree, Logistic Regression Classifier, Adaboost, Voting, kNN, and DNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOKT1sRZ04I"
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH9hOnpmzv6-"
      },
      "source": [
        "def display_metrics(y_test, y_pred, label_names):\r\n",
        "  print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\r\n",
        "\r\n",
        "  print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\r\n",
        "  print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\r\n",
        "  print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\r\n",
        "\r\n",
        "  print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\r\n",
        "  print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\r\n",
        "  print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\r\n",
        "\r\n",
        "  print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\r\n",
        "  print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\r\n",
        "  print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\r\n",
        "\r\n",
        "  print('\\nClassification Report\\n')\r\n",
        "  print(classification_report(y_test, y_pred, target_names=label_names))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCwZZFMTZ04J"
      },
      "source": [
        "def display_all(df):\n",
        "    with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 100): \n",
        "        print(df)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDogrv3Z04J"
      },
      "source": [
        "def make_value2index(attacks):\n",
        "    #make dictionary\n",
        "    attacks = sorted(attacks)\n",
        "    d = {}\n",
        "    counter=0\n",
        "    for attack in attacks:\n",
        "        d[attack] = counter\n",
        "        counter+=1\n",
        "    return d"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3b8hIQZ04K"
      },
      "source": [
        "# chganges label from string to integer/index\n",
        "def encode_label(Y_str):\n",
        "    labels_d = make_value2index(np.unique(Y_str))\n",
        "    Y = [labels_d[y_str] for y_str  in Y_str]\n",
        "    Y = np.array(Y)\n",
        "    return np.array(Y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMisXWZWZ04K"
      },
      "source": [
        "## Step 1. Loading csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwxZ7DrXZ04L"
      },
      "source": [
        "# All columns\n",
        "col_names = np.array(['dst sport count', 'src dport count', 'dst src count', 'dport count', 'sport count', 'dst host count','src host count','Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoCkMRAcaIA6"
      },
      "source": [
        "### Option 1. Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uaw_A5kaHSj",
        "outputId": "d0d80e08-86e0-4d8f-c41b-2a1acd5eea2b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qUtBnilZ04M"
      },
      "source": [
        "# load three csv files generated by mlp4nids (Multi-layer perceptron for network intrusion detection )\n",
        "# first load the train set\n",
        "df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set_ext78.csv',names=col_names, skiprows=1)  "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBn8DsjhX_U4",
        "outputId": "1ce9fcd4-2f1d-4344-8aad-cfcf86315d20"
      },
      "source": [
        "print('Train set size: ', df_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size:  (556548, 79)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYvaCAjZ04P",
        "outputId": "86d7efcf-ee78-470b-e71c-a06cbc599155"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set_ext78.csv',names=col_names, skiprows=1)  \n",
        "print('Test set size: ', df_test.shape)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set_ext78.csv',names=col_names, skiprows=1)  \n",
        "print('Validation set size: ', df_val.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set size:  (278271, 79)\n",
            "Validation set size:  (278271, 79)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cbbuInpXhvz"
      },
      "source": [
        "### Option 2. Load from local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLcmd-A8Xhv0"
      },
      "source": [
        "dataroot = '../data/cicids2017clean/train_set.csv'\n",
        "df_train = pd.read_csv(dataroot, names=col_names, skiprows=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9oqAbFyXhv0"
      },
      "source": [
        "dataroot = '../data/cicids2017clean/crossval_set.csv'\n",
        "df_val = pd.read_csv(dataroot, names=col_names, skiprows=1) \n",
        "dataroot = '../data/cicids2017clean/test_set.csv'\n",
        "df_test = pd.read_csv(dataroot, names=col_names, skiprows=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls46tMA9Xhv0"
      },
      "source": [
        "## Step 2. Exploring the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "q-oBvrtQXhv0",
        "outputId": "d944dc14-56c1-4a51-ee82-3c71c7811927"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>480307</th>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>48</td>\n",
              "      <td>81</td>\n",
              "      <td>56014</td>\n",
              "      <td>53</td>\n",
              "      <td>17</td>\n",
              "      <td>60843</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>94</td>\n",
              "      <td>226</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>113</td>\n",
              "      <td>113</td>\n",
              "      <td>113.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5259.438226</td>\n",
              "      <td>65.742978</td>\n",
              "      <td>20281.0</td>\n",
              "      <td>3.512166e+04</td>\n",
              "      <td>60836</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>40</td>\n",
              "      <td>32.871489</td>\n",
              "      <td>32.871489</td>\n",
              "      <td>47</td>\n",
              "      <td>113</td>\n",
              "      <td>73.400000</td>\n",
              "      <td>36.149689</td>\n",
              "      <td>1306.800000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>91.75</td>\n",
              "      <td>47.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>2</td>\n",
              "      <td>94</td>\n",
              "      <td>2</td>\n",
              "      <td>226</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2273904</th>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>96</td>\n",
              "      <td>96</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>96</td>\n",
              "      <td>33606</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1000000.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>1000000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>251</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DoS Hulk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2173332</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>54288</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>181818.181800</td>\n",
              "      <td>45454.545450</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>22727.272730</td>\n",
              "      <td>22727.272730</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>4.618802</td>\n",
              "      <td>21.333333</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.00</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>229</td>\n",
              "      <td>235</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DoS slowloris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2267207</th>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>50768</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1000000.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>1000000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>251</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DoS Hulk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90817</th>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>62083</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>3795333</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.323556</td>\n",
              "      <td>1.053926</td>\n",
              "      <td>1265111.0</td>\n",
              "      <td>2.190632e+06</td>\n",
              "      <td>3794635</td>\n",
              "      <td>348</td>\n",
              "      <td>3795333</td>\n",
              "      <td>1265111.0</td>\n",
              "      <td>2190632.043</td>\n",
              "      <td>3794635</td>\n",
              "      <td>348</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>1.053926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.50</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>256</td>\n",
              "      <td>-1</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DDoS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         dst sport count  src dport count  ...  Idle Min          Label\n",
              "480307                 1               36  ...         0         BENIGN\n",
              "2273904                1               96  ...         0       DoS Hulk\n",
              "2173332                1                1  ...         0  DoS slowloris\n",
              "2267207                1              100  ...         0       DoS Hulk\n",
              "90817                  1              100  ...         0           DDoS\n",
              "\n",
              "[5 rows x 79 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miDxhmNyZ04N"
      },
      "source": [
        "Count the number of attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpKy7b9fZ04O",
        "scrolled": true,
        "outputId": "4592c45e-96a9-4299-eba2-1c7d19995af8"
      },
      "source": [
        "df_train['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        278274\n",
              "DoS Hulk                      115062\n",
              "PortScan                       79402\n",
              "DDoS                           64012\n",
              "DoS GoldenEye                   5146\n",
              "FTP-Patator                     3967\n",
              "SSH-Patator                     2948\n",
              "DoS slowloris                   2898\n",
              "DoS Slowhttptest                2749\n",
              "Bot                              978\n",
              "Web Attack � Brute Force         753\n",
              "Web Attack � XSS                 326\n",
              "Infiltration                      18\n",
              "Web Attack � Sql Injection        10\n",
              "Heartbleed                         5\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2X3KG4zZ04P"
      },
      "source": [
        "Read test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5hWm9Q-Z04Q",
        "outputId": "f421bd4e-f2a4-4fb6-cadb-42c2795db70f"
      },
      "source": [
        "print('Test set: ')\n",
        "df_test['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack � Brute Force         376\n",
              "Web Attack � XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack � Sql Injection         5\n",
              "Heartbleed                         3\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0-lHNicZ04Q",
        "outputId": "43647dfc-700e-4411-8b86-f5b033e9385f"
      },
      "source": [
        "print('Validation set: ')\n",
        "df_val['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack � Brute Force         376\n",
              "Web Attack � XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack � Sql Injection         5\n",
              "Heartbleed                         3\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0O9ZuKoXhv3"
      },
      "source": [
        "## Step 3. Encode Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwXKhZOjXhv3"
      },
      "source": [
        "Encoding the labels, and generate numpy array. Note that the label has not been encoded as one-hot coding. We will use one-hot code later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyAnny9yZ04R"
      },
      "source": [
        "### Step 3.1 Encoding train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsR6_hJDZ04R"
      },
      "source": [
        "df_label = df_train['Label']\n",
        "data = df_train.drop(columns=['Label'])\n",
        "Xtrain = data.values\n",
        "y_train = encode_label(df_label.values)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XlV2AK3Z04S"
      },
      "source": [
        "### Step 3.2. Encoding test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVSrGExFZ04S",
        "scrolled": true
      },
      "source": [
        "df_label = df_test['Label']\n",
        "data = df_test.drop(columns=['Label'])\n",
        "Xtest = data.values\n",
        "y_test = encode_label(df_label.values)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO3PgredZ04T"
      },
      "source": [
        "### Step 3.3 Encoding validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ2wDKpZZ04T"
      },
      "source": [
        "df_label = df_val['Label']\n",
        "data = df_val.drop(columns=['Label'])\n",
        "Xval = data.values\n",
        "y_val = encode_label(df_label.values)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viMki-R0Z04Q"
      },
      "source": [
        "## Step 4. Normalization or Standardization\n",
        "\n",
        "The continuous feature values are normalized into the same feature space. This is important when using features that have different measurements, and is a general requirement of many machine learning algorithms. We implement the two methods to see the impact on the final classifications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8ERJidxXhv5"
      },
      "source": [
        "## Option 1. Normalization\n",
        "\n",
        "The values of the datasets are normalized using the Min-Max scaling technique, bringing them all within a range of [0,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c523vLd-Z04R"
      },
      "source": [
        "### Step 4.1 Normalizing train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5izaj07Z04R"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbYQFmfgZ04S",
        "scrolled": true,
        "outputId": "f2ac0b34-6637-42d3-ac44-a91a4224e01e"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(Xtrain)\n",
        "X_train"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.35353535, 0.35353535, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.95959596, 0.95959596, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.05050505, 0.        , 0.05050505, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.09090909, 0.09090909, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.32323232, 0.32323232, ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ5x1QxAXhv5"
      },
      "source": [
        "### Step 4.2. Normalizing validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-_BpWSsXhv6",
        "outputId": "0c00a352-62da-4ae6-bde6-94a76bf754b9"
      },
      "source": [
        "X_val = scaler.fit_transform(Xval)\n",
        "X_val"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01724138, 1.        , 1.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.94949495, 0.94949495, ..., 0.        , 0.82083333,\n",
              "        0.82083333],\n",
              "       [0.06896552, 0.06060606, 0.04040404, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.63636364, 0.63636364, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.19191919, 0.29292929, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.03448276, 0.02020202, 0.02020202, ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TItkmTF1Z04S"
      },
      "source": [
        "### Step 4.3. Normalizing test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXmp2w2bZ04T",
        "outputId": "b54fa18e-226c-4eba-fe5f-c50f0740321d"
      },
      "source": [
        "X_test = scaler.fit_transform(Xtest)\n",
        "X_test"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01010101, 0.26262626, 0.01010101, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.7979798 , 0.02020202, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.01010101, 0.50505051, 0.49494949, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.96969697, 0.96969697, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.83838384, 0.83838384, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.02020202, 0.02020202, 0.02020202, ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge2XVkhTXhv6"
      },
      "source": [
        "## Option 2. Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu459dh3Xhv7"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOJud1h5Xhv7",
        "outputId": "b4614821-64c3-4de1-e8f9-3e6cf4430a36"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(Xtrain)\n",
        "X_val = scaler.fit_transform(Xval)\n",
        "X_test = scaler.fit_transform(Xtest)\n",
        "\n",
        "X_train"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.46360917, -0.21367668, -0.60970034, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [-0.46360917,  1.19349981,  0.79867464, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [-0.46360917, -1.03452964, -1.43125242, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       ...,\n",
              "       [-0.32230411, -1.03452964, -1.31388783, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [-0.46360917, -0.82345316, -1.21999617, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [-0.46360917, -0.28403551, -0.68011909, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reoNDQZhZ04T"
      },
      "source": [
        "## Step 5 One-hot encoding for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So8gvIF8Z04T"
      },
      "source": [
        "y_train, y_test and y_val have to be one-hot-encoded. That means they must have dimension (number_of_samples, 15), where 15 denotes number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc97u4oZZ04U"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfeM_ZzsXhv8"
      },
      "source": [
        "Save the labels for AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N0GfC_zXhv8"
      },
      "source": [
        "y_train_ada = y_train\n",
        "y_test_ada = y_test\n",
        "y_val_ada = y_val"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQVqV19KZ04U"
      },
      "source": [
        "y_train = to_categorical(y_train, 15)\n",
        "y_test = to_categorical(y_test, 15)\n",
        "y_val = to_categorical(y_val, 15)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd9_XX_5Xhv8"
      },
      "source": [
        "## Step 6. Define the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOSi1KcXhv8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#importing confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#importing accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUup6sodXhv9"
      },
      "source": [
        "METRICS = [\n",
        "      tf.keras.metrics.TruePositives(name='tp'),\n",
        "      tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.AUC(name='auc'),\n",
        "]"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJywwX9iXhv9"
      },
      "source": [
        "#  Model 1: PCA  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTYHuZtxXhv9"
      },
      "source": [
        "X_pca = df_train.drop('Label',axis=1)\n",
        "y_pca = df_train['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XWs2_kXhv9"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_pca = scaler.fit_transform(X_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGVxxHw2Xhv9"
      },
      "source": [
        "dfx = pd.DataFrame(data=X_pca,columns=df_train.columns[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "2-gTg4_uXhv-",
        "outputId": "419f2375-0733-4fee-c003-dc4e4901d0fa"
      },
      "source": [
        "dfx.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.474747</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.474747</td>\n",
              "      <td>0.808081</td>\n",
              "      <td>0.854719</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.071335e-04</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>3.604466e-07</td>\n",
              "      <td>0.001894</td>\n",
              "      <td>0.031929</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006503</td>\n",
              "      <td>0.077397</td>\n",
              "      <td>0.025854</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085287</td>\n",
              "      <td>0.400013</td>\n",
              "      <td>1.691166e-04</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>5.070749e-04</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>1.095716e-05</td>\n",
              "      <td>1.643574e-05</td>\n",
              "      <td>0.034584</td>\n",
              "      <td>0.004553</td>\n",
              "      <td>0.037897</td>\n",
              "      <td>0.007640</td>\n",
              "      <td>5.833929e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.036294</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.025854</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>3.604239e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.512795</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.252525</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.828382</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.750002e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.005435</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085365</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>4.749999e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.749999e-07</td>\n",
              "      <td>4.749999e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>7.575758e-03</td>\n",
              "      <td>1.136364e-02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.002754</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>9.523810e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.774670</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.947326</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.162789e-02</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.054270e-02</td>\n",
              "      <td>0.025833</td>\n",
              "      <td>3.162206e-02</td>\n",
              "      <td>3.008333e-06</td>\n",
              "      <td>3.162778e-02</td>\n",
              "      <td>1.054259e-02</td>\n",
              "      <td>0.026267</td>\n",
              "      <td>3.162196e-02</td>\n",
              "      <td>3.000000e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.513087e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.004415</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.003098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002967</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.131313</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.023667</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.833335e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085403</td>\n",
              "      <td>0.408889</td>\n",
              "      <td>4.833333e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.833333e-07</td>\n",
              "      <td>4.833333e-07</td>\n",
              "      <td>3.750000e-07</td>\n",
              "      <td>3.750000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.750000e-07</td>\n",
              "      <td>4.750000e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.481481e-02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.004415</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.003098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003560</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.245697</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.949495</td>\n",
              "      <td>0.949495</td>\n",
              "      <td>0.949495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.949495</td>\n",
              "      <td>0.949495</td>\n",
              "      <td>0.909255</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>8.355307e-01</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>1.849282e-05</td>\n",
              "      <td>0.015471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009378</td>\n",
              "      <td>0.020537</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.442150</td>\n",
              "      <td>0.262135</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>6.962763e-02</td>\n",
              "      <td>0.340802</td>\n",
              "      <td>8.333334e-01</td>\n",
              "      <td>1.083333e-07</td>\n",
              "      <td>8.333333e-01</td>\n",
              "      <td>1.391667e-01</td>\n",
              "      <td>0.490408</td>\n",
              "      <td>8.333333e-01</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>1.253725e-03</td>\n",
              "      <td>2.507450e-04</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>1.150642e-03</td>\n",
              "      <td>3.916667e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>2.327198e-08</td>\n",
              "      <td>2.992112e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.175020</td>\n",
              "      <td>0.441995</td>\n",
              "      <td>0.308464</td>\n",
              "      <td>9.509609e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.364685</td>\n",
              "      <td>0.009378</td>\n",
              "      <td>0.442150</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>1.849166e-05</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>4.545455e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.545455e-08</td>\n",
              "      <td>4.545455e-08</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.050505</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.080808</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.087728</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.916668e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>9.569378e-09</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.004110</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085400</td>\n",
              "      <td>0.408696</td>\n",
              "      <td>4.916666e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.916666e-07</td>\n",
              "      <td>4.916666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>7.246377e-03</td>\n",
              "      <td>1.086957e-02</td>\n",
              "      <td>0.004415</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.003098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.003560</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>9.568776e-09</td>\n",
              "      <td>0.005127</td>\n",
              "      <td>0.252701</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.434343</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.979690</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.992170e-04</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>4.720893e-07</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>0.036685</td>\n",
              "      <td>0.009090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008517</td>\n",
              "      <td>0.101370</td>\n",
              "      <td>0.033862</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085287</td>\n",
              "      <td>0.400007</td>\n",
              "      <td>2.998111e-04</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>8.987832e-04</td>\n",
              "      <td>1.416667e-07</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>5.000000e-07</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>6.178961e-06</td>\n",
              "      <td>9.268442e-06</td>\n",
              "      <td>0.039735</td>\n",
              "      <td>0.005963</td>\n",
              "      <td>0.047294</td>\n",
              "      <td>0.010881</td>\n",
              "      <td>1.183393e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.045293</td>\n",
              "      <td>0.009090</td>\n",
              "      <td>0.033862</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>4.720596e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.228977</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.766667e-06</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>3.253589e-07</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.021739</td>\n",
              "      <td>0.005386</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005870</td>\n",
              "      <td>0.069863</td>\n",
              "      <td>0.023337</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085880</td>\n",
              "      <td>0.404020</td>\n",
              "      <td>6.611110e-07</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>1.716666e-06</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>3.350084e-03</td>\n",
              "      <td>5.025126e-03</td>\n",
              "      <td>0.023547</td>\n",
              "      <td>0.004110</td>\n",
              "      <td>0.030978</td>\n",
              "      <td>0.008103</td>\n",
              "      <td>6.562500e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.029668</td>\n",
              "      <td>0.005386</td>\n",
              "      <td>0.023337</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>3.253384e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   src dport count  dst src count  dport count  ...  Idle Max  Idle Min     Label\n",
              "0         0.000000       0.353535     0.353535  ...       0.0  0.000000  0.000000\n",
              "1         0.000000       0.959596     0.959596  ...       0.0  0.000000  0.000000\n",
              "2         0.000000       0.000000     0.000000  ...       0.0  0.000000  0.000000\n",
              "3         0.000000       1.000000     1.000000  ...       0.0  0.000000  0.000000\n",
              "4         0.000000       1.000000     1.000000  ...       0.0  0.000000  0.000000\n",
              "5         0.000000       0.060606     0.020202  ...       0.0  0.000000  0.000000\n",
              "6         0.000000       0.949495     0.949495  ...       0.0  0.833333  0.833333\n",
              "7         0.050505       0.000000     0.000000  ...       0.0  0.000000  0.000000\n",
              "8         0.000000       0.232323     0.232323  ...       0.0  0.000000  0.000000\n",
              "9         0.000000       1.000000     1.000000  ...       0.0  0.000000  0.000000\n",
              "\n",
              "[10 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzLYPxy-Xhv-"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIE01O4yXhv-"
      },
      "source": [
        "pca = PCA(n_components=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd2crc5zXhv-"
      },
      "source": [
        "dfx_pca = pca.fit(dfx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wPwtI_6RXhv-",
        "outputId": "0db9d0f6-d9b0-4c07-de50-11ab88e9c241"
      },
      "source": [
        "plt.figure(figsize=(24,5))\n",
        "plt.scatter(x=[i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],\n",
        "            y=dfx_pca.explained_variance_ratio_,\n",
        "           s=200, alpha=0.75,c='orange',edgecolor='k')\n",
        "plt.grid(True)\n",
        "plt.title(\"Explained variance ratio of the \\nfitted principal component vector\\n\",fontsize=25)\n",
        "plt.xlabel(\"Principal components\",fontsize=15)\n",
        "plt.xticks([i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.ylabel(\"Explained variance ratio\",fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeVjWmuZXhv_"
      },
      "source": [
        "The above two plots means that the $1^{st}$ principal component explains about 38% of the total variance in the data and the $2^{nd}$ component explians further 20%. Therefore, if we just consider first two components, they together explain 58% of the total variance. Using the first 10 features should give very hight detection rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8xM86XhXhv_"
      },
      "source": [
        "Transform the scaled data set using the fitted PCA object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srySEJvRXhv_"
      },
      "source": [
        "dfx_trans = pca.transform(dfx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COMFpsuyXhv_"
      },
      "source": [
        "Put it in a data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "rtptL8AMXhwA",
        "scrolled": true,
        "outputId": "1de1ce31-e271-4700-cdc6-72eeeb09f8b1"
      },
      "source": [
        "dfx_trans = pd.DataFrame(data=dfx_trans)\n",
        "dfx_trans.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.488597</td>\n",
              "      <td>0.242597</td>\n",
              "      <td>-0.540661</td>\n",
              "      <td>-0.178630</td>\n",
              "      <td>-0.514452</td>\n",
              "      <td>0.193199</td>\n",
              "      <td>-0.164727</td>\n",
              "      <td>-0.023271</td>\n",
              "      <td>0.158543</td>\n",
              "      <td>-0.020178</td>\n",
              "      <td>0.053654</td>\n",
              "      <td>-0.044077</td>\n",
              "      <td>-0.235297</td>\n",
              "      <td>-0.082241</td>\n",
              "      <td>0.028475</td>\n",
              "      <td>0.098638</td>\n",
              "      <td>-0.007841</td>\n",
              "      <td>-0.041150</td>\n",
              "      <td>-0.000887</td>\n",
              "      <td>0.038087</td>\n",
              "      <td>-0.023412</td>\n",
              "      <td>0.058491</td>\n",
              "      <td>0.058092</td>\n",
              "      <td>-0.024476</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.013029</td>\n",
              "      <td>0.083534</td>\n",
              "      <td>0.007055</td>\n",
              "      <td>-0.004332</td>\n",
              "      <td>0.005906</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>0.004857</td>\n",
              "      <td>0.004889</td>\n",
              "      <td>-0.003708</td>\n",
              "      <td>-0.001726</td>\n",
              "      <td>0.001525</td>\n",
              "      <td>-0.000286</td>\n",
              "      <td>-0.000494</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>0.001185</td>\n",
              "      <td>-0.001260</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.000146</td>\n",
              "      <td>-0.000221</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>-0.000048</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>-0.000767</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>-0.000226</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>-0.000075</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>-0.000041</td>\n",
              "      <td>-0.000129</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>9.326158e-06</td>\n",
              "      <td>-1.076908e-06</td>\n",
              "      <td>1.217870e-06</td>\n",
              "      <td>-1.442722e-06</td>\n",
              "      <td>-5.668453e-10</td>\n",
              "      <td>-9.399371e-15</td>\n",
              "      <td>-2.251133e-17</td>\n",
              "      <td>8.732720e-17</td>\n",
              "      <td>4.796419e-17</td>\n",
              "      <td>-3.583144e-17</td>\n",
              "      <td>1.640306e-18</td>\n",
              "      <td>-1.578207e-17</td>\n",
              "      <td>-1.511041e-17</td>\n",
              "      <td>-1.353358e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.263218</td>\n",
              "      <td>-0.044837</td>\n",
              "      <td>-0.166820</td>\n",
              "      <td>-1.174673</td>\n",
              "      <td>0.019609</td>\n",
              "      <td>-0.275334</td>\n",
              "      <td>0.418882</td>\n",
              "      <td>-0.031768</td>\n",
              "      <td>-0.056712</td>\n",
              "      <td>0.080211</td>\n",
              "      <td>-0.167943</td>\n",
              "      <td>0.098513</td>\n",
              "      <td>0.037086</td>\n",
              "      <td>0.066169</td>\n",
              "      <td>0.074802</td>\n",
              "      <td>0.018870</td>\n",
              "      <td>-0.020877</td>\n",
              "      <td>0.074176</td>\n",
              "      <td>-0.052254</td>\n",
              "      <td>-0.178544</td>\n",
              "      <td>0.167828</td>\n",
              "      <td>0.042994</td>\n",
              "      <td>0.044787</td>\n",
              "      <td>0.019081</td>\n",
              "      <td>-0.012203</td>\n",
              "      <td>0.008204</td>\n",
              "      <td>-0.009393</td>\n",
              "      <td>-0.003438</td>\n",
              "      <td>0.002675</td>\n",
              "      <td>-0.004593</td>\n",
              "      <td>-0.009801</td>\n",
              "      <td>0.004191</td>\n",
              "      <td>0.003660</td>\n",
              "      <td>-0.002434</td>\n",
              "      <td>-0.003423</td>\n",
              "      <td>-0.010165</td>\n",
              "      <td>-0.002741</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>-0.000326</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>-0.003654</td>\n",
              "      <td>-0.001374</td>\n",
              "      <td>-0.000601</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.001867</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>-0.000487</td>\n",
              "      <td>-0.000521</td>\n",
              "      <td>-0.000798</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.000127</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>-0.000034</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>3.246140e-06</td>\n",
              "      <td>-3.989182e-07</td>\n",
              "      <td>7.471152e-07</td>\n",
              "      <td>1.052650e-08</td>\n",
              "      <td>-2.566533e-09</td>\n",
              "      <td>1.702081e-15</td>\n",
              "      <td>5.289142e-17</td>\n",
              "      <td>-5.292686e-17</td>\n",
              "      <td>5.910479e-17</td>\n",
              "      <td>1.149871e-16</td>\n",
              "      <td>1.931173e-17</td>\n",
              "      <td>6.335587e-17</td>\n",
              "      <td>1.550500e-17</td>\n",
              "      <td>4.173343e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.938037</td>\n",
              "      <td>1.169584</td>\n",
              "      <td>0.154573</td>\n",
              "      <td>-0.236835</td>\n",
              "      <td>0.323263</td>\n",
              "      <td>-0.405435</td>\n",
              "      <td>-0.152761</td>\n",
              "      <td>1.296169</td>\n",
              "      <td>-0.054324</td>\n",
              "      <td>-0.112227</td>\n",
              "      <td>0.139533</td>\n",
              "      <td>-0.075226</td>\n",
              "      <td>0.036911</td>\n",
              "      <td>0.061415</td>\n",
              "      <td>0.026876</td>\n",
              "      <td>-0.065117</td>\n",
              "      <td>0.052207</td>\n",
              "      <td>0.098921</td>\n",
              "      <td>-0.057813</td>\n",
              "      <td>0.023476</td>\n",
              "      <td>-0.025837</td>\n",
              "      <td>-0.108801</td>\n",
              "      <td>-0.106587</td>\n",
              "      <td>-0.102261</td>\n",
              "      <td>-0.077703</td>\n",
              "      <td>0.045351</td>\n",
              "      <td>-0.057078</td>\n",
              "      <td>-0.008897</td>\n",
              "      <td>0.001767</td>\n",
              "      <td>0.006659</td>\n",
              "      <td>-0.006239</td>\n",
              "      <td>-0.003782</td>\n",
              "      <td>-0.013287</td>\n",
              "      <td>-0.007082</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>0.015822</td>\n",
              "      <td>0.009373</td>\n",
              "      <td>0.002984</td>\n",
              "      <td>-0.000611</td>\n",
              "      <td>-0.003884</td>\n",
              "      <td>0.002453</td>\n",
              "      <td>-0.003782</td>\n",
              "      <td>-0.005225</td>\n",
              "      <td>-0.001269</td>\n",
              "      <td>0.004527</td>\n",
              "      <td>0.000740</td>\n",
              "      <td>0.004608</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>-0.000434</td>\n",
              "      <td>0.006968</td>\n",
              "      <td>0.000695</td>\n",
              "      <td>-0.001429</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>-0.001422</td>\n",
              "      <td>-0.001487</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>-0.000621</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>-0.000129</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>-0.000094</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>-3.405002e-06</td>\n",
              "      <td>-4.076882e-06</td>\n",
              "      <td>5.675976e-07</td>\n",
              "      <td>-1.465755e-06</td>\n",
              "      <td>-1.378632e-08</td>\n",
              "      <td>6.869159e-15</td>\n",
              "      <td>3.919971e-16</td>\n",
              "      <td>-1.513926e-16</td>\n",
              "      <td>-2.197564e-16</td>\n",
              "      <td>-1.400548e-16</td>\n",
              "      <td>6.489696e-17</td>\n",
              "      <td>4.434442e-17</td>\n",
              "      <td>-2.705708e-17</td>\n",
              "      <td>-2.683301e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.333071</td>\n",
              "      <td>-0.104661</td>\n",
              "      <td>-0.250695</td>\n",
              "      <td>-1.204513</td>\n",
              "      <td>-0.060205</td>\n",
              "      <td>-0.368469</td>\n",
              "      <td>0.366575</td>\n",
              "      <td>0.007332</td>\n",
              "      <td>-0.054611</td>\n",
              "      <td>-0.086900</td>\n",
              "      <td>-0.101379</td>\n",
              "      <td>0.160482</td>\n",
              "      <td>0.047840</td>\n",
              "      <td>0.014102</td>\n",
              "      <td>0.041110</td>\n",
              "      <td>0.042839</td>\n",
              "      <td>-0.028882</td>\n",
              "      <td>0.076088</td>\n",
              "      <td>-0.045639</td>\n",
              "      <td>-0.172334</td>\n",
              "      <td>0.169792</td>\n",
              "      <td>0.045254</td>\n",
              "      <td>0.046676</td>\n",
              "      <td>0.017519</td>\n",
              "      <td>-0.006068</td>\n",
              "      <td>0.007395</td>\n",
              "      <td>-0.000457</td>\n",
              "      <td>-0.001880</td>\n",
              "      <td>0.005492</td>\n",
              "      <td>-0.003065</td>\n",
              "      <td>-0.007237</td>\n",
              "      <td>0.003011</td>\n",
              "      <td>0.002421</td>\n",
              "      <td>-0.002205</td>\n",
              "      <td>-0.001838</td>\n",
              "      <td>-0.010218</td>\n",
              "      <td>-0.001617</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.003826</td>\n",
              "      <td>-0.001700</td>\n",
              "      <td>-0.000710</td>\n",
              "      <td>0.001625</td>\n",
              "      <td>-0.000148</td>\n",
              "      <td>0.001813</td>\n",
              "      <td>0.001261</td>\n",
              "      <td>-0.000333</td>\n",
              "      <td>-0.000644</td>\n",
              "      <td>-0.000427</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>-0.000134</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>-0.000062</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>2.053972e-06</td>\n",
              "      <td>1.285055e-06</td>\n",
              "      <td>4.125033e-07</td>\n",
              "      <td>3.229756e-07</td>\n",
              "      <td>-2.084796e-09</td>\n",
              "      <td>7.445140e-16</td>\n",
              "      <td>4.942197e-17</td>\n",
              "      <td>-5.639631e-17</td>\n",
              "      <td>6.601658e-17</td>\n",
              "      <td>9.568830e-17</td>\n",
              "      <td>1.440572e-17</td>\n",
              "      <td>4.133301e-17</td>\n",
              "      <td>1.172384e-17</td>\n",
              "      <td>4.867232e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.385390</td>\n",
              "      <td>-0.100582</td>\n",
              "      <td>-0.272434</td>\n",
              "      <td>-1.149030</td>\n",
              "      <td>-0.118184</td>\n",
              "      <td>-0.407311</td>\n",
              "      <td>0.289029</td>\n",
              "      <td>0.018260</td>\n",
              "      <td>-0.018262</td>\n",
              "      <td>-0.229393</td>\n",
              "      <td>-0.078740</td>\n",
              "      <td>0.136164</td>\n",
              "      <td>0.045215</td>\n",
              "      <td>-0.040308</td>\n",
              "      <td>0.019169</td>\n",
              "      <td>0.044751</td>\n",
              "      <td>0.033189</td>\n",
              "      <td>0.036765</td>\n",
              "      <td>0.047657</td>\n",
              "      <td>0.131724</td>\n",
              "      <td>-0.094340</td>\n",
              "      <td>-0.021822</td>\n",
              "      <td>0.024958</td>\n",
              "      <td>0.012653</td>\n",
              "      <td>0.006594</td>\n",
              "      <td>-0.002633</td>\n",
              "      <td>0.009401</td>\n",
              "      <td>0.016648</td>\n",
              "      <td>0.001393</td>\n",
              "      <td>-0.010418</td>\n",
              "      <td>-0.002976</td>\n",
              "      <td>-0.006587</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.016598</td>\n",
              "      <td>-0.015491</td>\n",
              "      <td>-0.000356</td>\n",
              "      <td>0.007600</td>\n",
              "      <td>0.001463</td>\n",
              "      <td>-0.003105</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>-0.004492</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>-0.000455</td>\n",
              "      <td>-0.001187</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>-0.001320</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000765</td>\n",
              "      <td>-0.000677</td>\n",
              "      <td>-0.000873</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>-0.000149</td>\n",
              "      <td>-0.000819</td>\n",
              "      <td>-0.000031</td>\n",
              "      <td>-0.000398</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>-0.000132</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>8.073475e-07</td>\n",
              "      <td>7.248449e-06</td>\n",
              "      <td>-3.483708e-06</td>\n",
              "      <td>1.628854e-06</td>\n",
              "      <td>1.345458e-08</td>\n",
              "      <td>-9.077991e-15</td>\n",
              "      <td>1.100492e-17</td>\n",
              "      <td>-6.789763e-17</td>\n",
              "      <td>9.785755e-17</td>\n",
              "      <td>5.839002e-17</td>\n",
              "      <td>1.131353e-17</td>\n",
              "      <td>2.844696e-18</td>\n",
              "      <td>4.042168e-18</td>\n",
              "      <td>7.643776e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.950240</td>\n",
              "      <td>1.062221</td>\n",
              "      <td>0.254078</td>\n",
              "      <td>-0.201129</td>\n",
              "      <td>0.450436</td>\n",
              "      <td>-0.106995</td>\n",
              "      <td>0.064394</td>\n",
              "      <td>-0.115120</td>\n",
              "      <td>-0.258565</td>\n",
              "      <td>0.247042</td>\n",
              "      <td>-0.290402</td>\n",
              "      <td>0.019571</td>\n",
              "      <td>-0.056861</td>\n",
              "      <td>0.204908</td>\n",
              "      <td>0.312844</td>\n",
              "      <td>-0.144035</td>\n",
              "      <td>-0.074324</td>\n",
              "      <td>-0.016460</td>\n",
              "      <td>0.017403</td>\n",
              "      <td>0.096149</td>\n",
              "      <td>-0.109954</td>\n",
              "      <td>0.041044</td>\n",
              "      <td>0.042375</td>\n",
              "      <td>0.025975</td>\n",
              "      <td>0.002221</td>\n",
              "      <td>-0.017711</td>\n",
              "      <td>-0.055797</td>\n",
              "      <td>0.006284</td>\n",
              "      <td>-0.000731</td>\n",
              "      <td>0.007835</td>\n",
              "      <td>-0.024756</td>\n",
              "      <td>0.005381</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>-0.007679</td>\n",
              "      <td>-0.007984</td>\n",
              "      <td>0.004290</td>\n",
              "      <td>-0.004503</td>\n",
              "      <td>-0.000366</td>\n",
              "      <td>-0.000433</td>\n",
              "      <td>-0.007423</td>\n",
              "      <td>0.002221</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>-0.000589</td>\n",
              "      <td>-0.000238</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.001758</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.000905</td>\n",
              "      <td>-0.001644</td>\n",
              "      <td>-0.000399</td>\n",
              "      <td>-0.000893</td>\n",
              "      <td>-0.000642</td>\n",
              "      <td>-0.001351</td>\n",
              "      <td>-0.000239</td>\n",
              "      <td>-0.000308</td>\n",
              "      <td>0.000946</td>\n",
              "      <td>-0.000195</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-3.287675e-07</td>\n",
              "      <td>-1.056250e-05</td>\n",
              "      <td>3.433302e-06</td>\n",
              "      <td>-4.369818e-06</td>\n",
              "      <td>2.419419e-08</td>\n",
              "      <td>-4.448401e-15</td>\n",
              "      <td>-1.288013e-16</td>\n",
              "      <td>7.474092e-17</td>\n",
              "      <td>6.613886e-17</td>\n",
              "      <td>-2.196594e-17</td>\n",
              "      <td>3.014280e-17</td>\n",
              "      <td>2.273171e-17</td>\n",
              "      <td>-7.376614e-18</td>\n",
              "      <td>-5.454286e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.115870</td>\n",
              "      <td>0.470686</td>\n",
              "      <td>0.403000</td>\n",
              "      <td>0.024049</td>\n",
              "      <td>-0.002507</td>\n",
              "      <td>-0.463517</td>\n",
              "      <td>-0.466972</td>\n",
              "      <td>-0.107948</td>\n",
              "      <td>0.035002</td>\n",
              "      <td>0.079194</td>\n",
              "      <td>0.058373</td>\n",
              "      <td>-0.058572</td>\n",
              "      <td>0.034821</td>\n",
              "      <td>-0.154894</td>\n",
              "      <td>-0.096132</td>\n",
              "      <td>-0.016243</td>\n",
              "      <td>0.004409</td>\n",
              "      <td>0.096707</td>\n",
              "      <td>0.039044</td>\n",
              "      <td>0.049727</td>\n",
              "      <td>-0.062521</td>\n",
              "      <td>0.139430</td>\n",
              "      <td>0.064474</td>\n",
              "      <td>-0.033708</td>\n",
              "      <td>-0.006584</td>\n",
              "      <td>-0.012505</td>\n",
              "      <td>0.003361</td>\n",
              "      <td>0.030429</td>\n",
              "      <td>-0.002658</td>\n",
              "      <td>0.007782</td>\n",
              "      <td>0.014665</td>\n",
              "      <td>0.022047</td>\n",
              "      <td>-0.020243</td>\n",
              "      <td>-0.014131</td>\n",
              "      <td>-0.007212</td>\n",
              "      <td>-0.000952</td>\n",
              "      <td>-0.003672</td>\n",
              "      <td>0.006071</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>-0.000381</td>\n",
              "      <td>-0.002524</td>\n",
              "      <td>-0.004478</td>\n",
              "      <td>0.002261</td>\n",
              "      <td>-0.000732</td>\n",
              "      <td>-0.002195</td>\n",
              "      <td>-0.001162</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.001573</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>-0.001526</td>\n",
              "      <td>-0.000283</td>\n",
              "      <td>0.002495</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>-0.001661</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>-0.001561</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>-0.000069</td>\n",
              "      <td>-0.000053</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-6.899396e-06</td>\n",
              "      <td>-6.774172e-06</td>\n",
              "      <td>-1.094109e-06</td>\n",
              "      <td>-4.380810e-06</td>\n",
              "      <td>5.723609e-08</td>\n",
              "      <td>6.504444e-15</td>\n",
              "      <td>5.056218e-17</td>\n",
              "      <td>-2.240442e-17</td>\n",
              "      <td>7.228581e-17</td>\n",
              "      <td>1.509376e-17</td>\n",
              "      <td>-1.036537e-17</td>\n",
              "      <td>-4.453303e-17</td>\n",
              "      <td>3.869331e-18</td>\n",
              "      <td>7.514750e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1.086118</td>\n",
              "      <td>1.097851</td>\n",
              "      <td>0.677604</td>\n",
              "      <td>-0.263568</td>\n",
              "      <td>0.544678</td>\n",
              "      <td>0.213681</td>\n",
              "      <td>0.123784</td>\n",
              "      <td>-0.364919</td>\n",
              "      <td>-0.132527</td>\n",
              "      <td>-0.193925</td>\n",
              "      <td>0.099783</td>\n",
              "      <td>-0.430426</td>\n",
              "      <td>0.105124</td>\n",
              "      <td>0.237928</td>\n",
              "      <td>0.107744</td>\n",
              "      <td>-0.074215</td>\n",
              "      <td>0.149988</td>\n",
              "      <td>0.155198</td>\n",
              "      <td>0.046534</td>\n",
              "      <td>0.112035</td>\n",
              "      <td>0.127288</td>\n",
              "      <td>0.033701</td>\n",
              "      <td>0.032766</td>\n",
              "      <td>-0.043731</td>\n",
              "      <td>0.029622</td>\n",
              "      <td>-0.032496</td>\n",
              "      <td>-0.008918</td>\n",
              "      <td>-0.003661</td>\n",
              "      <td>-0.000747</td>\n",
              "      <td>0.010236</td>\n",
              "      <td>0.004703</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.005455</td>\n",
              "      <td>-0.000325</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>0.000901</td>\n",
              "      <td>-0.004072</td>\n",
              "      <td>-0.001543</td>\n",
              "      <td>-0.001827</td>\n",
              "      <td>-0.012483</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>-0.001481</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>-0.000433</td>\n",
              "      <td>-0.000496</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>-0.000049</td>\n",
              "      <td>-0.000248</td>\n",
              "      <td>-0.004741</td>\n",
              "      <td>-0.000797</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>-0.000294</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>-0.000244</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>-0.000036</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>9.108748e-06</td>\n",
              "      <td>-4.427061e-06</td>\n",
              "      <td>1.227877e-06</td>\n",
              "      <td>-2.764531e-06</td>\n",
              "      <td>-5.154350e-09</td>\n",
              "      <td>3.651776e-15</td>\n",
              "      <td>-1.176928e-16</td>\n",
              "      <td>9.631520e-17</td>\n",
              "      <td>-5.374426e-17</td>\n",
              "      <td>-9.270506e-17</td>\n",
              "      <td>4.432874e-17</td>\n",
              "      <td>-8.941912e-17</td>\n",
              "      <td>-4.208174e-17</td>\n",
              "      <td>1.611652e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.706228</td>\n",
              "      <td>0.551787</td>\n",
              "      <td>-0.573611</td>\n",
              "      <td>-0.000701</td>\n",
              "      <td>-0.455097</td>\n",
              "      <td>0.183618</td>\n",
              "      <td>-0.344518</td>\n",
              "      <td>0.015612</td>\n",
              "      <td>0.053065</td>\n",
              "      <td>-0.111768</td>\n",
              "      <td>0.094878</td>\n",
              "      <td>0.100832</td>\n",
              "      <td>0.013824</td>\n",
              "      <td>0.019627</td>\n",
              "      <td>-0.020470</td>\n",
              "      <td>-0.029276</td>\n",
              "      <td>-0.032115</td>\n",
              "      <td>0.040547</td>\n",
              "      <td>-0.023878</td>\n",
              "      <td>-0.024677</td>\n",
              "      <td>0.021891</td>\n",
              "      <td>-0.008143</td>\n",
              "      <td>-0.069512</td>\n",
              "      <td>0.018420</td>\n",
              "      <td>0.026582</td>\n",
              "      <td>-0.014380</td>\n",
              "      <td>-0.010583</td>\n",
              "      <td>0.004745</td>\n",
              "      <td>-0.006689</td>\n",
              "      <td>-0.000873</td>\n",
              "      <td>0.000508</td>\n",
              "      <td>0.011591</td>\n",
              "      <td>0.017653</td>\n",
              "      <td>-0.003032</td>\n",
              "      <td>-0.000094</td>\n",
              "      <td>-0.001799</td>\n",
              "      <td>0.004088</td>\n",
              "      <td>-0.000831</td>\n",
              "      <td>-0.000201</td>\n",
              "      <td>0.003115</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.001354</td>\n",
              "      <td>-0.001456</td>\n",
              "      <td>0.002869</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>-0.001108</td>\n",
              "      <td>-0.000843</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.001630</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>-0.000527</td>\n",
              "      <td>-0.001143</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-0.001544</td>\n",
              "      <td>-0.000103</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>-1.480808e-05</td>\n",
              "      <td>3.954640e-06</td>\n",
              "      <td>-1.030311e-06</td>\n",
              "      <td>1.508507e-06</td>\n",
              "      <td>-6.627337e-09</td>\n",
              "      <td>-2.803008e-15</td>\n",
              "      <td>-7.782320e-17</td>\n",
              "      <td>4.699257e-17</td>\n",
              "      <td>5.204161e-17</td>\n",
              "      <td>-8.337368e-17</td>\n",
              "      <td>-8.262404e-19</td>\n",
              "      <td>-4.260788e-17</td>\n",
              "      <td>-2.045641e-17</td>\n",
              "      <td>-1.128103e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.164733</td>\n",
              "      <td>-0.285201</td>\n",
              "      <td>-0.600493</td>\n",
              "      <td>-0.789680</td>\n",
              "      <td>-0.477579</td>\n",
              "      <td>0.445212</td>\n",
              "      <td>0.289089</td>\n",
              "      <td>-0.115758</td>\n",
              "      <td>0.262846</td>\n",
              "      <td>0.498957</td>\n",
              "      <td>-0.034816</td>\n",
              "      <td>-0.154667</td>\n",
              "      <td>0.044556</td>\n",
              "      <td>0.075744</td>\n",
              "      <td>-0.040956</td>\n",
              "      <td>-0.086913</td>\n",
              "      <td>0.004060</td>\n",
              "      <td>-0.022052</td>\n",
              "      <td>-0.031734</td>\n",
              "      <td>-0.018252</td>\n",
              "      <td>0.030773</td>\n",
              "      <td>-0.068347</td>\n",
              "      <td>-0.034874</td>\n",
              "      <td>-0.025849</td>\n",
              "      <td>0.072895</td>\n",
              "      <td>-0.032605</td>\n",
              "      <td>-0.019593</td>\n",
              "      <td>-0.023409</td>\n",
              "      <td>-0.015352</td>\n",
              "      <td>-0.004923</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>-0.002437</td>\n",
              "      <td>-0.000875</td>\n",
              "      <td>-0.006627</td>\n",
              "      <td>0.003367</td>\n",
              "      <td>-0.003680</td>\n",
              "      <td>-0.001989</td>\n",
              "      <td>0.002366</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>-0.003243</td>\n",
              "      <td>-0.001424</td>\n",
              "      <td>-0.002515</td>\n",
              "      <td>0.002913</td>\n",
              "      <td>-0.005951</td>\n",
              "      <td>-0.000303</td>\n",
              "      <td>0.001562</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>-0.000969</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>-0.000942</td>\n",
              "      <td>0.001277</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000246</td>\n",
              "      <td>-0.000708</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>-0.000653</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>-0.000187</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>6.161955e-07</td>\n",
              "      <td>-3.332764e-06</td>\n",
              "      <td>6.178883e-07</td>\n",
              "      <td>1.544477e-06</td>\n",
              "      <td>-2.780651e-08</td>\n",
              "      <td>6.655686e-15</td>\n",
              "      <td>7.903426e-17</td>\n",
              "      <td>-5.665868e-17</td>\n",
              "      <td>5.905767e-17</td>\n",
              "      <td>1.440478e-16</td>\n",
              "      <td>7.964650e-18</td>\n",
              "      <td>9.236746e-17</td>\n",
              "      <td>2.086314e-17</td>\n",
              "      <td>5.533644e-17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...            75            76            77\n",
              "0 -0.488597  0.242597 -0.540661  ... -1.578207e-17 -1.511041e-17 -1.353358e-16\n",
              "1  0.263218 -0.044837 -0.166820  ...  6.335587e-17  1.550500e-17  4.173343e-17\n",
              "2 -0.938037  1.169584  0.154573  ...  4.434442e-17 -2.705708e-17 -2.683301e-16\n",
              "3  0.333071 -0.104661 -0.250695  ...  4.133301e-17  1.172384e-17  4.867232e-17\n",
              "4  0.385390 -0.100582 -0.272434  ...  2.844696e-18  4.042168e-18  7.643776e-17\n",
              "5 -0.950240  1.062221  0.254078  ...  2.273171e-17 -7.376614e-18 -5.454286e-17\n",
              "6  2.115870  0.470686  0.403000  ... -4.453303e-17  3.869331e-18  7.514750e-17\n",
              "7 -1.086118  1.097851  0.677604  ... -8.941912e-17 -4.208174e-17  1.611652e-16\n",
              "8 -0.706228  0.551787 -0.573611  ... -4.260788e-17 -2.045641e-17 -1.128103e-16\n",
              "9  0.164733 -0.285201 -0.600493  ...  9.236746e-17  2.086314e-17  5.533644e-17\n",
              "\n",
              "[10 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIDYI_dXhwA"
      },
      "source": [
        "## Training and Making Predictions\n",
        "\n",
        "In this case we'll use random forest classification for making the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swXJ6_ydXhwA"
      },
      "source": [
        "pca = PCA(n_components=12)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Yt7PnkXhwA"
      },
      "source": [
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7X48ybRXhwA",
        "outputId": "35049d68-effe-4247-9dc0-1007578f972e"
      },
      "source": [
        "print('Accuracy:%f' %accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:%f\" %metrics.average_precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:%f\" %metrics.recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-score:%f\" %metrics.f1_score(y_test, y_pred,average='weighted'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.631446\n",
            "Precision:0.627443\n",
            "Recall:0.631446\n",
            "F1-score:0.623922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfibn9gAXhwB",
        "scrolled": true,
        "outputId": "be99767f-51c7-4306-cdb3-c104d7f00ce7"
      },
      "source": [
        "#The confusion matrix takes a vector of labels (not the one-hot encoding). \n",
        "\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[138946      0      0      0      0      0      0      0      0      0\n",
            "     189      0      0      0      0]\n",
            " [   489      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [ 31990      0      0      0      0      0      0      0      0      0\n",
            "      16      0      0      0      0]\n",
            " [  2573      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [ 57531      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [  1373      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]\n",
            " [  1444      0      0      0      0      0      0      0      0      0\n",
            "       5      0      0      0      0]\n",
            " [  1983      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     2      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     9      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [  1667      0      0      0      0      0      0      0      0      0\n",
            "   38034      0      0      0      0]\n",
            " [  1474      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   376      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     5      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   162      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxRNywIXXhwC"
      },
      "source": [
        "Get the attacks' names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9AlIiTKXhwC"
      },
      "source": [
        "labels_d = make_value2index(df_test['Label'])"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1vqAhE1XhwC",
        "outputId": "112094b3-09a9-4230-cdc3-29c3a4a48674"
      },
      "source": [
        "print(labels_d)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'BENIGN': 139134, 'Bot': 139623, 'DDoS': 171629, 'DoS GoldenEye': 174202, 'DoS Hulk': 231733, 'DoS Slowhttptest': 233107, 'DoS slowloris': 234556, 'FTP-Patator': 236539, 'Heartbleed': 236542, 'Infiltration': 236551, 'PortScan': 276252, 'SSH-Patator': 277726, 'Web Attack � Brute Force': 278102, 'Web Attack � Sql Injection': 278107, 'Web Attack � XSS': 278270}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO4EmilxXhwD",
        "outputId": "db4a864f-ce2e-4828-e99c-1eed12db92e0"
      },
      "source": [
        "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1), target_names=labels_d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.59      1.00      0.74    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       1.00      0.08      0.15     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.92      0.96     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.65    278271\n",
            "                 macro avg       0.17      0.13      0.12    278271\n",
            "              weighted avg       0.64      0.65      0.54    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtGAy_3gY2n"
      },
      "source": [
        "# Model : Naive Bayes model (GaussianNB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxNz3yQch35o"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izw8QWLIgcTx",
        "outputId": "7bb23d80-4b57-4b21-b8d4-0131af8a7b90"
      },
      "source": [
        "model_gaussian = GaussianNB()\r\n",
        "model_gaussian.fit(X_train, y_train_ada)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c15lYNIdgs9V"
      },
      "source": [
        "# make predictions\r\n",
        "y_pred = model_gaussian.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ6JwTSDg6iE",
        "outputId": "f9a72154-00bc-4cbb-a285-503d352f34d8"
      },
      "source": [
        "display_metrics(y_test_ada, y_pred, labels_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.50\n",
            "\n",
            "Micro Precision: 0.50\n",
            "Micro Recall: 0.50\n",
            "Micro F1-score: 0.50\n",
            "\n",
            "Macro Precision: 0.03\n",
            "Macro Recall: 0.07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.04\n",
            "\n",
            "Weighted Precision: 0.25\n",
            "Weighted Recall: 0.50\n",
            "Weighted F1-score: 0.33\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.50      1.00      0.67    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.00      0.00      0.00     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.50    278271\n",
            "                 macro avg       0.03      0.07      0.04    278271\n",
            "              weighted avg       0.25      0.50      0.33    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXea3TMVXhwD"
      },
      "source": [
        "# Model 3: Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOUaeJwiXhwD"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRMY85VyXhwE"
      },
      "source": [
        "model_dec = DecisionTreeClassifier()\n",
        "model_dec.fit(X_train, y_train_ada)\n",
        "y_pred = model_dec.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFQ6iCJjXhwE",
        "outputId": "42069f40-8f26-455c-ffe2-7f3ef297a390"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\n",
        " \n",
        "print('\\nClassification Report\\n')\n",
        "\n",
        "print(classification_report(y_test_ada , y_pred, target_names=labels_d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.96\n",
            "\n",
            "Micro Precision: 0.96\n",
            "Micro Recall: 0.96\n",
            "Micro F1-score: 0.96\n",
            "\n",
            "Macro Precision: 0.50\n",
            "Macro Recall: 0.61\n",
            "Macro F1-score: 0.53\n",
            "\n",
            "Weighted Precision: 0.96\n",
            "Weighted Recall: 0.96\n",
            "Weighted F1-score: 0.96\n",
            "\n",
            "Classification Report\n",
            "\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.97      0.99      0.98    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.99      0.99      0.99     32006\n",
            "             DoS GoldenEye       0.50      0.87      0.63      2573\n",
            "                  DoS Hulk       0.99      0.93      0.96     57531\n",
            "          DoS Slowhttptest       0.11      0.05      0.06      1374\n",
            "             DoS slowloris       0.38      0.38      0.38      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       1.00      1.00      1.00         3\n",
            "              Infiltration       0.07      0.33      0.12         9\n",
            "                  PortScan       1.00      0.99      0.99     39701\n",
            "               SSH-Patator       0.44      1.00      0.61      1474\n",
            "  Web Attack � Brute Force       0.57      0.97      0.72       376\n",
            "Web Attack � Sql Injection       0.43      0.60      0.50         5\n",
            "          Web Attack � XSS       0.03      0.02      0.02       163\n",
            "\n",
            "                  accuracy                           0.96    278271\n",
            "                 macro avg       0.50      0.61      0.53    278271\n",
            "              weighted avg       0.96      0.96      0.96    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABYQjuBCXhwE"
      },
      "source": [
        "# Model 4: Random Foresty with DecisionTree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI6QGw9LXhwF"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "clf.fit(X_train,y_train)\n",
        "    \n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFh6H0d3lNmz",
        "outputId": "0a27f10c-3d38-4e99-8c2e-570f577daaf2"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, np.argmax(y_pred, axis = 1))))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')\r\n",
        "\r\n",
        "print(classification_report(y_test_ada ,np.argmax(y_pred, axis = 1), target_names=labels_d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.90\n",
            "\n",
            "Micro Precision: 0.90\n",
            "Micro Recall: 0.90\n",
            "Micro F1-score: 0.90\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.73\n",
            "Macro Recall: 0.59\n",
            "Macro F1-score: 0.61\n",
            "\n",
            "Weighted Precision: 0.92\n",
            "Weighted Recall: 0.90\n",
            "Weighted F1-score: 0.89\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.84      1.00      0.91    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       1.00      0.99      0.99     32006\n",
            "             DoS GoldenEye       0.96      0.85      0.90      2573\n",
            "                  DoS Hulk       1.00      0.91      0.96     57531\n",
            "          DoS Slowhttptest       0.68      0.79      0.73      1374\n",
            "             DoS slowloris       0.99      0.29      0.45      1449\n",
            "               FTP-Patator       1.00      0.50      0.67      1983\n",
            "                Heartbleed       1.00      1.00      1.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.53      0.69     39701\n",
            "               SSH-Patator       1.00      0.98      0.99      1474\n",
            "  Web Attack � Brute Force       0.71      0.93      0.80       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.83      0.03      0.06       163\n",
            "\n",
            "                  accuracy                           0.90    278271\n",
            "                 macro avg       0.73      0.59      0.61    278271\n",
            "              weighted avg       0.92      0.90      0.89    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xi2INVUk9p9"
      },
      "source": [
        "# Model 5: Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_AoQZpOnxuD"
      },
      "source": [
        "from sklearn import linear_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqJDnChnmxJ"
      },
      "source": [
        "attack_classifier = linear_model.LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=10000)\r\n",
        "attack_classifier.fit(X_train, y_train_ada)\r\n",
        "\r\n",
        "y_pred = attack_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3x76HdDorw7",
        "outputId": "4063a2b0-ff60-4976-c6fd-08894d952256"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')\r\n",
        "\r\n",
        "print(classification_report(y_test_ada , y_pred, target_names=labels_d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.84\n",
            "\n",
            "Micro Precision: 0.84\n",
            "Micro Recall: 0.84\n",
            "Micro F1-score: 0.84\n",
            "\n",
            "Macro Precision: 0.51\n",
            "Macro Recall: 0.60\n",
            "Macro F1-score: 0.50\n",
            "\n",
            "Weighted Precision: 0.92\n",
            "Weighted Recall: 0.84\n",
            "Weighted F1-score: 0.85\n",
            "\n",
            "Classification Report\n",
            "\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.97      0.91      0.94    139135\n",
            "                       Bot       0.05      1.00      0.10       489\n",
            "                      DDoS       0.53      1.00      0.69     32006\n",
            "             DoS GoldenEye       0.99      0.81      0.90      2573\n",
            "                  DoS Hulk       0.99      0.48      0.65     57531\n",
            "          DoS Slowhttptest       0.72      0.75      0.73      1374\n",
            "             DoS slowloris       0.91      0.33      0.48      1449\n",
            "               FTP-Patator       0.62      0.94      0.74      1983\n",
            "                Heartbleed       0.50      1.00      0.67         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.99      0.99     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.40      0.81      0.54       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.84    278271\n",
            "                 macro avg       0.51      0.60      0.50    278271\n",
            "              weighted avg       0.92      0.84      0.85    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXJKq1zXhwF"
      },
      "source": [
        "# Model 6: AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqqrLskIXhwF"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT8egNOEXhwG"
      },
      "source": [
        "model_ada = AdaBoostClassifier(n_estimators=100)\n",
        "model_ada.fit(X_train, y_train_ada)\n",
        "\n",
        "# make predictions\n",
        "expected = y_test_ada\n",
        "predicted = model_ada.predict(X_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJCcrCS4XhwG",
        "outputId": "382e7fda-51b8-44ce-ea55-ab4550a9ba15"
      },
      "source": [
        "y_pred = predicted\n",
        "\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\n",
        " \n",
        "print('\\nClassification Report\\n')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.69\n",
            "\n",
            "Micro Precision: 0.69\n",
            "Micro Recall: 0.69\n",
            "Micro F1-score: 0.69\n",
            "\n",
            "Macro Precision: 0.14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Recall: 0.19\n",
            "Macro F1-score: 0.16\n",
            "\n",
            "Weighted Precision: 0.54\n",
            "Weighted Recall: 0.69\n",
            "Weighted F1-score: 0.59\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_lvcZVXXhwG",
        "scrolled": true,
        "outputId": "efd21686-c03b-460a-d135-6ac039daeabf"
      },
      "source": [
        "print(classification_report(y_test_ada, predicted, target_names=labels_d))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.90      1.00      0.95    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.42      0.90      0.57     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.75      1.00      0.86         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.69    278271\n",
            "                 macro avg       0.14      0.19      0.16    278271\n",
            "              weighted avg       0.54      0.69      0.59    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn7bQX_Gvusc"
      },
      "source": [
        "# Model 6: XGBClassifier (very slow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ttK187BvuLn"
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "53QHIzuAv5z9",
        "outputId": "0e392ce3-2300-4f4f-a1e0-44bdc7723ac0"
      },
      "source": [
        "xgboostc = XGBClassifier(learning_rate = 0.1, max_depth = 5,n_estimators = 1165, subsample=0.8,colsample_bytree=0.8,seed=27)\r\n",
        "xgboostc.fit(X_train,y_train)\r\n",
        "    \r\n",
        "y_pred = xgboostc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-07eedb02b743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgboostc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1165\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgboostc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboostc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz8IN3Jfwav0"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KznBGxa2yiLk"
      },
      "source": [
        "# Model 7: Voting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfa2lmvytnh"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIYUdP7iyuRG",
        "outputId": "9fcfad8b-43b6-432f-f52c-524e26272080"
      },
      "source": [
        "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=35, criterion=\"entropy\")\r\n",
        "ada = AdaBoostClassifier(n_estimators=75, learning_rate=1.5)\r\n",
        "etc = ExtraTreesClassifier(n_jobs=-1, criterion=\"entropy\", n_estimators=5)\r\n",
        "eclf = VotingClassifier(estimators=[('ada', ada), ('rfc', rfc), ('etc', etc)], voting='soft', weights=[2, 1, 3],n_jobs=1)\r\n",
        "eclf.fit(X_train,y_train_ada)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('ada',\n",
              "                              AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                                                 base_estimator=None,\n",
              "                                                 learning_rate=1.5,\n",
              "                                                 n_estimators=75,\n",
              "                                                 random_state=None)),\n",
              "                             ('rfc',\n",
              "                              RandomForestClassifier(bootstrap=True,\n",
              "                                                     ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     criterion='entropy',\n",
              "                                                     max_depth=None,\n",
              "                                                     max_features='auto',\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     max_samples=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_im...\n",
              "                                                   criterion='entropy',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   max_samples=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=5, n_jobs=-1,\n",
              "                                                   oob_score=False,\n",
              "                                                   random_state=None, verbose=0,\n",
              "                                                   warm_start=False))],\n",
              "                 flatten_transform=True, n_jobs=1, voting='soft',\n",
              "                 weights=[2, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rig3c_bqzpim"
      },
      "source": [
        "y_pred = eclf.predict(X_test)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuI-C6lZzqeV",
        "outputId": "73f607af-7c0b-4d6e-e7dc-25bf074be896"
      },
      "source": [
        "display_metrics(y_test_ada, y_pred,labels_d)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.98\n",
            "\n",
            "Micro Precision: 0.98\n",
            "Micro Recall: 0.98\n",
            "Micro F1-score: 0.98\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.71\n",
            "Macro Recall: 0.57\n",
            "Macro F1-score: 0.59\n",
            "\n",
            "Weighted Precision: 0.98\n",
            "Weighted Recall: 0.98\n",
            "Weighted F1-score: 0.97\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.97      1.00      0.98    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.97      0.99      0.98     32006\n",
            "             DoS GoldenEye       1.00      0.77      0.87      2573\n",
            "                  DoS Hulk       1.00      0.97      0.99     57531\n",
            "          DoS Slowhttptest       0.73      0.77      0.75      1374\n",
            "             DoS slowloris       0.99      0.65      0.79      1449\n",
            "               FTP-Patator       1.00      0.66      0.79      1983\n",
            "                Heartbleed       1.00      1.00      1.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      1.00      1.00     39701\n",
            "               SSH-Patator       1.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.69      0.77      0.73       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.23      0.02      0.03       163\n",
            "\n",
            "                  accuracy                           0.98    278271\n",
            "                 macro avg       0.71      0.57      0.59    278271\n",
            "              weighted avg       0.98      0.98      0.97    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAuDnGKYXhwH"
      },
      "source": [
        "# Model 8: KNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7fba3ZuXhwH"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgx8g7vuXhwH"
      },
      "source": [
        "features_order = ['dst sport count', 'src dport count', 'dst src count', 'dport count', 'sport count', 'dst host count','src host count', 'Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min']"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyhoOfFgXhwH"
      },
      "source": [
        "features=['dst sport count', 'src dport count', 'dst src count', 'dport count', 'sport count', 'dst host count','src host count', \"Fwd Packet Length Max\",\"Flow IAT Std\",\"Fwd Packet Length Std\" ,\"Fwd IAT Total\",'Flow Packets/s', \"Fwd Packet Length Mean\",  \"Flow Bytes/s\",  \"Flow IAT Mean\", \"Bwd Packet Length Mean\",  \"Flow IAT Max\", \"Bwd Packet Length Std\", ]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "GlhN1BDdXhwI",
        "outputId": "30a1e7ee-1b03-4c31-81e5-1002a01bc226"
      },
      "source": [
        "df_knn_train = pd.DataFrame(X_train, columns = features_order)\n",
        "df_knn_train.head()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.474747</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.474747</td>\n",
              "      <td>0.808081</td>\n",
              "      <td>0.854719</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.071335e-04</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>3.604466e-07</td>\n",
              "      <td>0.001894</td>\n",
              "      <td>0.031929</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006503</td>\n",
              "      <td>0.077397</td>\n",
              "      <td>0.025854</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.085287</td>\n",
              "      <td>0.400013</td>\n",
              "      <td>1.691166e-04</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>5.070749e-04</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>1.095716e-05</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.034584</td>\n",
              "      <td>0.004553</td>\n",
              "      <td>0.037897</td>\n",
              "      <td>0.007640</td>\n",
              "      <td>5.833929e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.036294</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.025854</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>3.604239e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.512795</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.252525</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.828382</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.750002e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.005435</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.085365</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>4.749999e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.749999e-07</td>\n",
              "      <td>4.749999e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>7.575758e-03</td>\n",
              "      <td>0.011364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.002754</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>9.523810e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.774670</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.947326</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.162789e-02</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.054270e-02</td>\n",
              "      <td>0.025833</td>\n",
              "      <td>3.162206e-02</td>\n",
              "      <td>3.008333e-06</td>\n",
              "      <td>3.162778e-02</td>\n",
              "      <td>1.054259e-02</td>\n",
              "      <td>0.026267</td>\n",
              "      <td>3.162196e-02</td>\n",
              "      <td>3.000000e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.513087e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004415</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.003098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002967</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  ...  Idle Max  Idle Min\n",
              "0              0.0         0.353535  ...       0.0       0.0\n",
              "1              0.0         0.959596  ...       0.0       0.0\n",
              "2              0.0         0.000000  ...       0.0       0.0\n",
              "3              0.0         1.000000  ...       0.0       0.0\n",
              "4              0.0         1.000000  ...       0.0       0.0\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eda-FM60XhwI"
      },
      "source": [
        "df_knn_test = pd.DataFrame(X_test, columns = features_order)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "iv6nljzO5_Wd",
        "outputId": "aac397e8-a383-4cb7-f3fc-66ed68b04283"
      },
      "source": [
        "df_knn_test.head()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.262626</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.343434</td>\n",
              "      <td>0.575671</td>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>3.333336e-08</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.002269</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005713</td>\n",
              "      <td>0.005259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014242</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>3.496731e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333483e-08</td>\n",
              "      <td>1.398692e-07</td>\n",
              "      <td>2.500004e-08</td>\n",
              "      <td>2.500812e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500108e-08</td>\n",
              "      <td>3.334416e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002269</td>\n",
              "      <td>0.018822</td>\n",
              "      <td>0.006932</td>\n",
              "      <td>4.804610e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025629</td>\n",
              "      <td>0.005713</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.004852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.344086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.797980</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.838384</td>\n",
              "      <td>0.898816</td>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.141421e-02</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.001844</td>\n",
              "      <td>9.651340e-06</td>\n",
              "      <td>0.029580</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043832</td>\n",
              "      <td>0.042196</td>\n",
              "      <td>0.084024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.161079</td>\n",
              "      <td>0.089580</td>\n",
              "      <td>0.005764</td>\n",
              "      <td>0.400003</td>\n",
              "      <td>5.701852e-04</td>\n",
              "      <td>0.002727</td>\n",
              "      <td>8.925566e-03</td>\n",
              "      <td>1.748365e-07</td>\n",
              "      <td>1.141421e-02</td>\n",
              "      <td>1.037991e-03</td>\n",
              "      <td>0.003722</td>\n",
              "      <td>8.925544e-03</td>\n",
              "      <td>1.092021e-06</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.00030</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>4.084213e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.203023</td>\n",
              "      <td>0.112317</td>\n",
              "      <td>1.261520e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.192676</td>\n",
              "      <td>0.043832</td>\n",
              "      <td>0.161079</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.001844</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>9.651340e-06</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.001938</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.215054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.505051</td>\n",
              "      <td>0.494949</td>\n",
              "      <td>0.656566</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.494949</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.848689</td>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>6.808439e-03</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>9.121935e-06</td>\n",
              "      <td>0.022132</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029502</td>\n",
              "      <td>0.030900</td>\n",
              "      <td>0.084427</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.169159</td>\n",
              "      <td>0.111552</td>\n",
              "      <td>0.005765</td>\n",
              "      <td>0.400004</td>\n",
              "      <td>4.761533e-04</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>2.265493e-03</td>\n",
              "      <td>1.311274e-07</td>\n",
              "      <td>6.807018e-03</td>\n",
              "      <td>1.134870e-03</td>\n",
              "      <td>0.001760</td>\n",
              "      <td>2.272240e-03</td>\n",
              "      <td>1.292086e-06</td>\n",
              "      <td>0.004546</td>\n",
              "      <td>0.000568</td>\n",
              "      <td>0.00149</td>\n",
              "      <td>0.002270</td>\n",
              "      <td>1.667026e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062800</td>\n",
              "      <td>0.217368</td>\n",
              "      <td>0.139331</td>\n",
              "      <td>1.941306e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.209653</td>\n",
              "      <td>0.029502</td>\n",
              "      <td>0.169159</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>9.121935e-06</td>\n",
              "      <td>0.445572</td>\n",
              "      <td>0.003723</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.344086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.894162</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.666668e-08</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.748365e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.666741e-08</td>\n",
              "      <td>1.223856e-07</td>\n",
              "      <td>8.333345e-09</td>\n",
              "      <td>8.336039e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333693e-09</td>\n",
              "      <td>1.667208e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.004196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.344086</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.722358</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.500001e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>9.153974e-09</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000969</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.002796</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>0.423529</td>\n",
              "      <td>1.573529e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.500067e-07</td>\n",
              "      <td>2.622548e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.336039e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.005141</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.001776</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>2.736695e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>9.153974e-09</td>\n",
              "      <td>0.015640</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.258065</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  ...  Idle Max  Idle Min\n",
              "0         0.010101         0.262626  ...       0.0       0.0\n",
              "1         0.000000         0.797980  ...       0.0       0.0\n",
              "2         0.010101         0.505051  ...       0.0       0.0\n",
              "3         0.000000         0.939394  ...       0.0       0.0\n",
              "4         1.000000         0.000000  ...       0.0       0.0\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T8Abk2XhwI"
      },
      "source": [
        "Select a subset of features from the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "LFABov84XhwK",
        "outputId": "d63d6057-5abf-43db-b1e8-fbff6aec4779"
      },
      "source": [
        "df_knn_sub=df_knn_train.loc[:, features]\n",
        "df_knn_sub.head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.474747</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.474747</td>\n",
              "      <td>0.808081</td>\n",
              "      <td>0.001894</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.400013</td>\n",
              "      <td>0.007911</td>\n",
              "      <td>0.085287</td>\n",
              "      <td>1.691166e-04</td>\n",
              "      <td>0.025854</td>\n",
              "      <td>5.070749e-04</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.252525</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.085365</td>\n",
              "      <td>4.749999e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.749999e-07</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.666667e-08</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.025833</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.162778e-02</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.085285</td>\n",
              "      <td>1.054270e-02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.162206e-02</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0              0.0         0.353535  ...  5.070749e-04                    0.0\n",
              "1              0.0         0.959596  ...  1.250000e-07                    0.0\n",
              "2              0.0         0.000000  ...  4.749999e-07                    0.0\n",
              "3              0.0         1.000000  ...  1.250000e-07                    0.0\n",
              "4              0.0         1.000000  ...  3.162206e-02                    0.0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "zhVGlgN5XhwK",
        "outputId": "dd4f039f-1ee8-407c-e616-237212c3717a"
      },
      "source": [
        "df_knn_test_sub=df_knn_test.loc[:, features]\n",
        "df_knn_test_sub.head()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.262626</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.343434</td>\n",
              "      <td>0.002269</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005259</td>\n",
              "      <td>2.500004e-08</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.005713</td>\n",
              "      <td>0.014242</td>\n",
              "      <td>3.496731e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333483e-08</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.797980</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.838384</td>\n",
              "      <td>0.029580</td>\n",
              "      <td>0.002727</td>\n",
              "      <td>0.042196</td>\n",
              "      <td>1.141421e-02</td>\n",
              "      <td>0.400003</td>\n",
              "      <td>0.043832</td>\n",
              "      <td>0.005764</td>\n",
              "      <td>5.701852e-04</td>\n",
              "      <td>0.161079</td>\n",
              "      <td>8.925566e-03</td>\n",
              "      <td>0.089580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.505051</td>\n",
              "      <td>0.494949</td>\n",
              "      <td>0.656566</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.494949</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.022132</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.030900</td>\n",
              "      <td>6.807018e-03</td>\n",
              "      <td>0.400004</td>\n",
              "      <td>0.029502</td>\n",
              "      <td>0.005765</td>\n",
              "      <td>4.761533e-04</td>\n",
              "      <td>0.169159</td>\n",
              "      <td>2.265493e-03</td>\n",
              "      <td>0.111552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333345e-09</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>1.748365e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.666741e-08</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.423529</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>1.573529e-07</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>1.500067e-07</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0         0.010101         0.262626  ...  3.333483e-08               0.000000\n",
              "1         0.000000         0.797980  ...  8.925566e-03               0.089580\n",
              "2         0.010101         0.505051  ...  2.265493e-03               0.111552\n",
              "3         0.000000         0.939394  ...  1.666741e-08               0.000000\n",
              "4         1.000000         0.000000  ...  1.500067e-07               0.000000\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59G_b79XhwK"
      },
      "source": [
        "Convert dataframes to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJHMos8rXhwK"
      },
      "source": [
        "X_train_knn = df_knn_sub.to_numpy()\n",
        "X_test_knn = df_knn_test_sub.to_numpy()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hAzqyPsXhwK",
        "outputId": "0b265558-60bd-40fb-bf50-9e96c78ea0fa"
      },
      "source": [
        "for i in range(5,X_train_knn.shape[1]+1):\n",
        "    knn=KNeighborsClassifier(n_neighbors=i)\n",
        "    model_knn=knn.fit(X_train_knn,y_train)\n",
        "    y_pred=model_knn.predict(X_test_knn)\n",
        "    print(\"for \" , i,  \" as K, accuracy is : \", accuracy_score(y_test, y_pred))\n",
        "    display_metrics(y_test, y_pred, labels_d)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for  5  as K, accuracy is :  0.9452799609014234\n",
            "\n",
            "Accuracy: 0.95\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.95\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.59\n",
            "Macro Recall: 0.64\n",
            "Macro F1-score: 0.60\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.95\n",
            "Weighted F1-score: 0.94\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.15      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.73      0.95      0.83      2573\n",
            "                  DoS Hulk       0.91      0.93      0.92     57531\n",
            "          DoS Slowhttptest       0.81      0.98      0.89      1374\n",
            "             DoS slowloris       0.23      0.22      0.23      1449\n",
            "               FTP-Patator       0.73      0.86      0.79      1983\n",
            "                Heartbleed       0.75      1.00      0.86         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.96      0.98     39701\n",
            "               SSH-Patator       0.89      0.88      0.88      1474\n",
            "  Web Attack � Brute Force       0.40      0.56      0.47       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.26      0.33      0.29       163\n",
            "\n",
            "                 micro avg       0.95      0.95      0.95    278271\n",
            "                 macro avg       0.59      0.64      0.60    278271\n",
            "              weighted avg       0.95      0.95      0.94    278271\n",
            "               samples avg       0.95      0.95      0.95    278271\n",
            "\n",
            "for  6  as K, accuracy is :  0.9433933108372773\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.60\n",
            "Macro Recall: 0.63\n",
            "Macro F1-score: 0.60\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.94\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.25      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.77      0.95      0.85      2573\n",
            "                  DoS Hulk       0.91      0.93      0.92     57531\n",
            "          DoS Slowhttptest       0.81      0.98      0.89      1374\n",
            "             DoS slowloris       0.25      0.22      0.23      1449\n",
            "               FTP-Patator       0.73      0.85      0.78      1983\n",
            "                Heartbleed       0.75      1.00      0.86         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.96      0.98     39701\n",
            "               SSH-Patator       0.89      0.87      0.88      1474\n",
            "  Web Attack � Brute Force       0.42      0.53      0.47       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.27      0.32      0.29       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.60      0.63      0.60    278271\n",
            "              weighted avg       0.95      0.94      0.94    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n",
            "for  7  as K, accuracy is :  0.9456105738650452\n",
            "\n",
            "Accuracy: 0.95\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.95\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.59\n",
            "Macro Recall: 0.64\n",
            "Macro F1-score: 0.60\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.95\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.22      0.02      0.03       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.75      0.95      0.84      2573\n",
            "                  DoS Hulk       0.91      0.93      0.92     57531\n",
            "          DoS Slowhttptest       0.81      0.98      0.89      1374\n",
            "             DoS slowloris       0.22      0.23      0.22      1449\n",
            "               FTP-Patator       0.72      0.85      0.78      1983\n",
            "                Heartbleed       0.75      1.00      0.86         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.96      0.98     39701\n",
            "               SSH-Patator       0.88      0.88      0.88      1474\n",
            "  Web Attack � Brute Force       0.40      0.56      0.46       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.25      0.33      0.29       163\n",
            "\n",
            "                 micro avg       0.95      0.95      0.95    278271\n",
            "                 macro avg       0.59      0.64      0.60    278271\n",
            "              weighted avg       0.95      0.95      0.95    278271\n",
            "               samples avg       0.95      0.95      0.95    278271\n",
            "\n",
            "for  8  as K, accuracy is :  0.9437239238008991\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.60\n",
            "Macro Recall: 0.63\n",
            "Macro F1-score: 0.61\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.27      0.02      0.03       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.79      0.94      0.86      2573\n",
            "                  DoS Hulk       0.91      0.93      0.92     57531\n",
            "          DoS Slowhttptest       0.81      0.98      0.88      1374\n",
            "             DoS slowloris       0.23      0.22      0.23      1449\n",
            "               FTP-Patator       0.72      0.85      0.78      1983\n",
            "                Heartbleed       0.75      1.00      0.86         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.96      0.98     39701\n",
            "               SSH-Patator       0.89      0.86      0.88      1474\n",
            "  Web Attack � Brute Force       0.44      0.52      0.48       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.28      0.31      0.29       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.60      0.63      0.61    278271\n",
            "              weighted avg       0.95      0.94      0.95    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n",
            "for  9  as K, accuracy is :  0.9453338651889704\n",
            "\n",
            "Accuracy: 0.95\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.95\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.60\n",
            "Macro Recall: 0.64\n",
            "Macro F1-score: 0.60\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.95\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.24      0.02      0.03       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.75      0.94      0.84      2573\n",
            "                  DoS Hulk       0.91      0.93      0.92     57531\n",
            "          DoS Slowhttptest       0.80      0.98      0.88      1374\n",
            "             DoS slowloris       0.23      0.26      0.24      1449\n",
            "               FTP-Patator       0.72      0.86      0.78      1983\n",
            "                Heartbleed       0.75      1.00      0.86         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.96      0.98     39701\n",
            "               SSH-Patator       0.88      0.87      0.88      1474\n",
            "  Web Attack � Brute Force       0.42      0.54      0.48       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.27      0.33      0.30       163\n",
            "\n",
            "                 micro avg       0.95      0.95      0.95    278271\n",
            "                 macro avg       0.60      0.64      0.60    278271\n",
            "              weighted avg       0.95      0.95      0.95    278271\n",
            "               samples avg       0.95      0.95      0.95    278271\n",
            "\n",
            "for  10  as K, accuracy is :  0.9430303553011273\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.56\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.30      0.02      0.03       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.79      0.94      0.86      2573\n",
            "                  DoS Hulk       0.91      0.93      0.92     57531\n",
            "          DoS Slowhttptest       0.80      0.98      0.88      1374\n",
            "             DoS slowloris       0.24      0.26      0.25      1449\n",
            "               FTP-Patator       0.72      0.85      0.78      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.96      0.98     39701\n",
            "               SSH-Patator       0.89      0.87      0.88      1474\n",
            "  Web Attack � Brute Force       0.45      0.52      0.48       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.32      0.31      0.32       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.56      0.56      0.55    278271\n",
            "              weighted avg       0.95      0.94      0.95    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n",
            "for  11  as K, accuracy is :  0.9454308929065551\n",
            "\n",
            "Accuracy: 0.95\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.95\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.55\n",
            "Macro Recall: 0.57\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.95\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.27      0.02      0.03       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.79      0.94      0.86      2573\n",
            "                  DoS Hulk       0.91      0.93      0.92     57531\n",
            "          DoS Slowhttptest       0.80      0.98      0.88      1374\n",
            "             DoS slowloris       0.23      0.27      0.24      1449\n",
            "               FTP-Patator       0.71      0.85      0.78      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.97      0.98     39701\n",
            "               SSH-Patator       0.88      0.89      0.88      1474\n",
            "  Web Attack � Brute Force       0.45      0.58      0.51       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.30      0.32      0.31       163\n",
            "\n",
            "                 micro avg       0.95      0.95      0.95    278271\n",
            "                 macro avg       0.55      0.57      0.55    278271\n",
            "              weighted avg       0.95      0.95      0.95    278271\n",
            "               samples avg       0.95      0.95      0.95    278271\n",
            "\n",
            "for  12  as K, accuracy is :  0.9436340833216541\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.56\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.28      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.84      0.93      0.88      2573\n",
            "                  DoS Hulk       0.91      0.92      0.92     57531\n",
            "          DoS Slowhttptest       0.81      0.98      0.89      1374\n",
            "             DoS slowloris       0.23      0.26      0.25      1449\n",
            "               FTP-Patator       0.71      0.84      0.77      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.97      0.98     39701\n",
            "               SSH-Patator       0.88      0.88      0.88      1474\n",
            "  Web Attack � Brute Force       0.43      0.49      0.45       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.31      0.29      0.30       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.56      0.56      0.55    278271\n",
            "              weighted avg       0.95      0.94      0.95    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n",
            "for  13  as K, accuracy is :  0.9451793395646689\n",
            "\n",
            "Accuracy: 0.95\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.95\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.55\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.95\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.24      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.83      0.93      0.88      2573\n",
            "                  DoS Hulk       0.91      0.92      0.92     57531\n",
            "          DoS Slowhttptest       0.81      0.98      0.89      1374\n",
            "             DoS slowloris       0.22      0.28      0.25      1449\n",
            "               FTP-Patator       0.70      0.84      0.77      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.97      0.98     39701\n",
            "               SSH-Patator       0.87      0.88      0.88      1474\n",
            "  Web Attack � Brute Force       0.42      0.53      0.47       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.27      0.29      0.28       163\n",
            "\n",
            "                 micro avg       0.95      0.95      0.95    278271\n",
            "                 macro avg       0.55      0.56      0.55    278271\n",
            "              weighted avg       0.95      0.95      0.95    278271\n",
            "               samples avg       0.95      0.95      0.95    278271\n",
            "\n",
            "for  14  as K, accuracy is :  0.9438065770418046\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.56\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.36      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.83      0.93      0.88      2573\n",
            "                  DoS Hulk       0.91      0.92      0.92     57531\n",
            "          DoS Slowhttptest       0.81      0.98      0.89      1374\n",
            "             DoS slowloris       0.24      0.28      0.26      1449\n",
            "               FTP-Patator       0.70      0.83      0.76      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.97      0.98     39701\n",
            "               SSH-Patator       0.88      0.88      0.88      1474\n",
            "  Web Attack � Brute Force       0.43      0.49      0.46       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.29      0.29      0.29       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.56      0.56      0.55    278271\n",
            "              weighted avg       0.95      0.94      0.95    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n",
            "for  15  as K, accuracy is :  0.9447696669793115\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.55\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.94\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.33      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.82      0.93      0.87      2573\n",
            "                  DoS Hulk       0.91      0.92      0.92     57531\n",
            "          DoS Slowhttptest       0.80      0.98      0.88      1374\n",
            "             DoS slowloris       0.22      0.29      0.25      1449\n",
            "               FTP-Patator       0.68      0.84      0.75      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.97      0.98     39701\n",
            "               SSH-Patator       0.87      0.88      0.88      1474\n",
            "  Web Attack � Brute Force       0.43      0.52      0.47       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.27      0.30      0.29       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.55      0.56      0.55    278271\n",
            "              weighted avg       0.95      0.94      0.94    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n",
            "for  16  as K, accuracy is :  0.9436484577983333\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.56\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.38      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.82      0.93      0.87      2573\n",
            "                  DoS Hulk       0.91      0.92      0.92     57531\n",
            "          DoS Slowhttptest       0.80      0.98      0.88      1374\n",
            "             DoS slowloris       0.24      0.28      0.26      1449\n",
            "               FTP-Patator       0.69      0.83      0.75      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.97      0.98     39701\n",
            "               SSH-Patator       0.91      0.88      0.89      1474\n",
            "  Web Attack � Brute Force       0.44      0.50      0.47       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.29      0.25      0.27       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.56      0.56      0.55    278271\n",
            "              weighted avg       0.95      0.94      0.95    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n",
            "for  17  as K, accuracy is :  0.9449205989844432\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.56\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.36      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.81      0.93      0.87      2573\n",
            "                  DoS Hulk       0.91      0.92      0.92     57531\n",
            "          DoS Slowhttptest       0.80      0.98      0.88      1374\n",
            "             DoS slowloris       0.22      0.28      0.25      1449\n",
            "               FTP-Patator       0.68      0.84      0.75      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.97      0.98     39701\n",
            "               SSH-Patator       0.90      0.88      0.89      1474\n",
            "  Web Attack � Brute Force       0.43      0.53      0.47       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.28      0.25      0.26       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.56      0.56      0.55    278271\n",
            "              weighted avg       0.95      0.94      0.95    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n",
            "for  18  as K, accuracy is :  0.9435693981765977\n",
            "\n",
            "Accuracy: 0.94\n",
            "\n",
            "Micro Precision: 0.95\n",
            "Micro Recall: 0.94\n",
            "Micro F1-score: 0.95\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.56\n",
            "Macro Recall: 0.56\n",
            "Macro F1-score: 0.55\n",
            "\n",
            "Weighted Precision: 0.95\n",
            "Weighted Recall: 0.94\n",
            "Weighted F1-score: 0.95\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      0.98      0.97    139135\n",
            "                       Bot       0.38      0.01      0.02       489\n",
            "                      DDoS       0.99      0.86      0.92     32006\n",
            "             DoS GoldenEye       0.82      0.92      0.87      2573\n",
            "                  DoS Hulk       0.91      0.92      0.92     57531\n",
            "          DoS Slowhttptest       0.80      0.98      0.88      1374\n",
            "             DoS slowloris       0.23      0.28      0.25      1449\n",
            "               FTP-Patator       0.68      0.82      0.74      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.97      0.98     39701\n",
            "               SSH-Patator       0.91      0.88      0.89      1474\n",
            "  Web Attack � Brute Force       0.44      0.51      0.47       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.30      0.23      0.26       163\n",
            "\n",
            "                 micro avg       0.95      0.94      0.95    278271\n",
            "                 macro avg       0.56      0.56      0.55    278271\n",
            "              weighted avg       0.95      0.94      0.95    278271\n",
            "               samples avg       0.94      0.94      0.94    278271\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQvcVlbmXhwL"
      },
      "source": [
        "# Model 9: DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tL-xBt_XhwL"
      },
      "source": [
        "def make_model(metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          256, activation='relu',\n",
        "          input_shape=(X_train.shape[-1],)),\n",
        "      #tf.keras.layers.Dropout(0.9),\n",
        "      tf.keras.layers.Dense(256, activation ='relu'),\n",
        "      #tf.keras.layers.Dropout(0.4),\n",
        "      #tf.keras.layers.Dense(256, activation ='relu'),\n",
        "      #tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(128, activation ='relu'),\n",
        "      #tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(64, activation ='relu'),\n",
        "      tf.keras.layers.Dense(y_train.shape[-1], activation='softmax',\n",
        "                         bias_initializer=output_bias),\n",
        "  ])\n",
        " \n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAzpGMEoXhwL"
      },
      "source": [
        "EPOCHS = 300\n",
        "BATCH_SIZE = 9000\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWmjTb9kXhwL",
        "outputId": "ad016762-be9b-4ae3-e2bb-e0b0d2b012c3"
      },
      "source": [
        "model_dnn = make_model()\n",
        "model_dnn.summary()"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_155 (Dense)            (None, 256)               20224     \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_157 (Dense)            (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_159 (Dense)            (None, 15)                975       \n",
            "=================================================================\n",
            "Total params: 128,143\n",
            "Trainable params: 128,143\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajWocDr4_30a"
      },
      "source": [
        "If loading the validation dataset has an issue, please load the csv files again, and encode it again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DeRlOQj6XhwL",
        "outputId": "e0bbcd48-4e9b-4e90-a519-16fb0726fea1"
      },
      "source": [
        "baseline_history = model_dnn.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    #callbacks=[early_stopping],\n",
        "    validation_data=(X_val, y_val))"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "62/62 [==============================] - 4s 43ms/step - loss: 0.7372 - tp: 130776.0000 - fp: 2938.0000 - tn: 5930416.6667 - fn: 293035.0476 - accuracy: 0.9576 - precision: 0.9780 - recall: 0.3729 - auc: 0.7489 - val_loss: 0.4959 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 3895794.0000 - val_fn: 278271.0000 - val_accuracy: 0.9333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7435\n",
            "Epoch 2/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.4167 - tp: 21331.6190 - fp: 4385.0476 - tn: 4024969.6190 - fn: 266479.4286 - accuracy: 0.9362 - precision: 0.7339 - recall: 0.0539 - auc: 0.7592 - val_loss: 0.1819 - val_tp: 49057.0000 - val_fp: 3886.0000 - val_tn: 3891908.0000 - val_fn: 229214.0000 - val_accuracy: 0.9442 - val_precision: 0.9266 - val_recall: 0.1763 - val_auc: 0.8753\n",
            "Epoch 3/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.1699 - tp: 86619.5238 - fp: 6177.8889 - tn: 4023176.7778 - fn: 201191.5238 - accuracy: 0.9498 - precision: 0.9383 - recall: 0.2648 - auc: 0.9128 - val_loss: 0.1198 - val_tp: 179121.0000 - val_fp: 44693.0000 - val_tn: 3851101.0000 - val_fn: 99150.0000 - val_accuracy: 0.9655 - val_precision: 0.8003 - val_recall: 0.6437 - val_auc: 0.9579\n",
            "Epoch 4/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.1063 - tp: 189424.4127 - fp: 16309.5079 - tn: 4013045.1587 - fn: 98386.6349 - accuracy: 0.9723 - precision: 0.9187 - recall: 0.6405 - auc: 0.9683 - val_loss: 0.0853 - val_tp: 180118.0000 - val_fp: 43898.0000 - val_tn: 3851896.0000 - val_fn: 98153.0000 - val_accuracy: 0.9660 - val_precision: 0.8040 - val_recall: 0.6473 - val_auc: 0.9710\n",
            "Epoch 5/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0692 - tp: 236245.3175 - fp: 22531.3651 - tn: 4006823.3016 - fn: 51565.7302 - accuracy: 0.9828 - precision: 0.9191 - recall: 0.8131 - auc: 0.9785 - val_loss: 0.0680 - val_tp: 228715.0000 - val_fp: 32210.0000 - val_tn: 3863584.0000 - val_fn: 49556.0000 - val_accuracy: 0.9804 - val_precision: 0.8766 - val_recall: 0.8219 - val_auc: 0.9760\n",
            "Epoch 6/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0519 - tp: 251672.8254 - fp: 32523.5873 - tn: 3996831.0794 - fn: 36138.2222 - accuracy: 0.9840 - precision: 0.8852 - recall: 0.8741 - auc: 0.9813 - val_loss: 0.0598 - val_tp: 230558.0000 - val_fp: 20448.0000 - val_tn: 3875346.0000 - val_fn: 47713.0000 - val_accuracy: 0.9837 - val_precision: 0.9185 - val_recall: 0.8285 - val_auc: 0.9774\n",
            "Epoch 7/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0433 - tp: 254490.5556 - fp: 28569.1905 - tn: 4000785.4762 - fn: 33320.4921 - accuracy: 0.9854 - precision: 0.8968 - recall: 0.8827 - auc: 0.9852 - val_loss: 0.0555 - val_tp: 235070.0000 - val_fp: 24073.0000 - val_tn: 3871721.0000 - val_fn: 43201.0000 - val_accuracy: 0.9839 - val_precision: 0.9071 - val_recall: 0.8448 - val_auc: 0.9785\n",
            "Epoch 8/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0374 - tp: 261104.7302 - fp: 21610.7937 - tn: 4007743.8730 - fn: 26706.3175 - accuracy: 0.9885 - precision: 0.9214 - recall: 0.9044 - auc: 0.9912 - val_loss: 0.0539 - val_tp: 238022.0000 - val_fp: 33110.0000 - val_tn: 3862684.0000 - val_fn: 40249.0000 - val_accuracy: 0.9824 - val_precision: 0.8779 - val_recall: 0.8554 - val_auc: 0.9789\n",
            "Epoch 9/300\n",
            "62/62 [==============================] - 2s 34ms/step - loss: 0.0329 - tp: 269670.9683 - fp: 15105.2381 - tn: 4014249.4286 - fn: 18140.0794 - accuracy: 0.9921 - precision: 0.9460 - recall: 0.9353 - auc: 0.9937 - val_loss: 0.0535 - val_tp: 240321.0000 - val_fp: 35891.0000 - val_tn: 3859903.0000 - val_fn: 37950.0000 - val_accuracy: 0.9823 - val_precision: 0.8701 - val_recall: 0.8636 - val_auc: 0.9790\n",
            "Epoch 10/300\n",
            "62/62 [==============================] - 2s 31ms/step - loss: 0.0288 - tp: 271812.6349 - fp: 13671.6667 - tn: 4015683.0000 - fn: 15998.4127 - accuracy: 0.9931 - precision: 0.9517 - recall: 0.9441 - auc: 0.9947 - val_loss: 0.0559 - val_tp: 238367.0000 - val_fp: 38885.0000 - val_tn: 3856909.0000 - val_fn: 39904.0000 - val_accuracy: 0.9811 - val_precision: 0.8597 - val_recall: 0.8566 - val_auc: 0.9800\n",
            "Epoch 11/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0247 - tp: 273185.1905 - fp: 11981.6508 - tn: 4017373.0159 - fn: 14625.8571 - accuracy: 0.9938 - precision: 0.9577 - recall: 0.9489 - auc: 0.9951 - val_loss: 0.0591 - val_tp: 236460.0000 - val_fp: 41380.0000 - val_tn: 3854414.0000 - val_fn: 41811.0000 - val_accuracy: 0.9801 - val_precision: 0.8511 - val_recall: 0.8497 - val_auc: 0.9818\n",
            "Epoch 12/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0212 - tp: 274111.8413 - fp: 10220.9206 - tn: 4019133.7460 - fn: 13699.2063 - accuracy: 0.9944 - precision: 0.9632 - recall: 0.9520 - auc: 0.9962 - val_loss: 0.0644 - val_tp: 235995.0000 - val_fp: 41775.0000 - val_tn: 3854019.0000 - val_fn: 42276.0000 - val_accuracy: 0.9799 - val_precision: 0.8496 - val_recall: 0.8481 - val_auc: 0.9686\n",
            "Epoch 13/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0178 - tp: 275220.1587 - fp: 7764.1270 - tn: 4021590.5397 - fn: 12590.8889 - accuracy: 0.9953 - precision: 0.9723 - recall: 0.9564 - auc: 0.9974 - val_loss: 0.0733 - val_tp: 236042.0000 - val_fp: 40350.0000 - val_tn: 3855444.0000 - val_fn: 42229.0000 - val_accuracy: 0.9802 - val_precision: 0.8540 - val_recall: 0.8482 - val_auc: 0.9356\n",
            "Epoch 14/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0156 - tp: 276114.5714 - fp: 6201.3175 - tn: 4023153.3492 - fn: 11696.4762 - accuracy: 0.9958 - precision: 0.9775 - recall: 0.9588 - auc: 0.9976 - val_loss: 0.0810 - val_tp: 236104.0000 - val_fp: 39875.0000 - val_tn: 3855919.0000 - val_fn: 42167.0000 - val_accuracy: 0.9803 - val_precision: 0.8555 - val_recall: 0.8485 - val_auc: 0.9325\n",
            "Epoch 15/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0137 - tp: 277305.2222 - fp: 5301.5397 - tn: 4024053.1270 - fn: 10505.8254 - accuracy: 0.9963 - precision: 0.9809 - recall: 0.9631 - auc: 0.9978 - val_loss: 0.0873 - val_tp: 236125.0000 - val_fp: 39712.0000 - val_tn: 3856082.0000 - val_fn: 42146.0000 - val_accuracy: 0.9804 - val_precision: 0.8560 - val_recall: 0.8485 - val_auc: 0.9337\n",
            "Epoch 16/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0124 - tp: 278400.5873 - fp: 4731.2222 - tn: 4024623.4444 - fn: 9410.4603 - accuracy: 0.9967 - precision: 0.9829 - recall: 0.9671 - auc: 0.9978 - val_loss: 0.0981 - val_tp: 236514.0000 - val_fp: 39294.0000 - val_tn: 3856500.0000 - val_fn: 41757.0000 - val_accuracy: 0.9806 - val_precision: 0.8575 - val_recall: 0.8499 - val_auc: 0.9337\n",
            "Epoch 17/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0113 - tp: 279457.3810 - fp: 4125.7778 - tn: 4025228.8889 - fn: 8353.6667 - accuracy: 0.9971 - precision: 0.9852 - recall: 0.9707 - auc: 0.9980 - val_loss: 0.1074 - val_tp: 237294.0000 - val_fp: 39261.0000 - val_tn: 3856533.0000 - val_fn: 40977.0000 - val_accuracy: 0.9808 - val_precision: 0.8580 - val_recall: 0.8527 - val_auc: 0.9338\n",
            "Epoch 18/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0103 - tp: 280085.2698 - fp: 3717.1429 - tn: 4025637.5238 - fn: 7725.7778 - accuracy: 0.9973 - precision: 0.9867 - recall: 0.9729 - auc: 0.9980 - val_loss: 0.1167 - val_tp: 238183.0000 - val_fp: 38553.0000 - val_tn: 3857241.0000 - val_fn: 40088.0000 - val_accuracy: 0.9812 - val_precision: 0.8607 - val_recall: 0.8559 - val_auc: 0.9340\n",
            "Epoch 19/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0095 - tp: 280596.0159 - fp: 3529.2540 - tn: 4025825.4127 - fn: 7215.0317 - accuracy: 0.9975 - precision: 0.9875 - recall: 0.9748 - auc: 0.9982 - val_loss: 0.1256 - val_tp: 238902.0000 - val_fp: 38347.0000 - val_tn: 3857447.0000 - val_fn: 39369.0000 - val_accuracy: 0.9814 - val_precision: 0.8617 - val_recall: 0.8585 - val_auc: 0.9336\n",
            "Epoch 20/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0089 - tp: 280843.4444 - fp: 3434.3175 - tn: 4025920.3492 - fn: 6967.6032 - accuracy: 0.9976 - precision: 0.9877 - recall: 0.9755 - auc: 0.9982 - val_loss: 0.1342 - val_tp: 239030.0000 - val_fp: 38623.0000 - val_tn: 3857171.0000 - val_fn: 39241.0000 - val_accuracy: 0.9813 - val_precision: 0.8609 - val_recall: 0.8590 - val_auc: 0.9337\n",
            "Epoch 21/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0083 - tp: 281156.5714 - fp: 3278.0952 - tn: 4026076.5714 - fn: 6654.4762 - accuracy: 0.9977 - precision: 0.9884 - recall: 0.9766 - auc: 0.9982 - val_loss: 0.1450 - val_tp: 239527.0000 - val_fp: 38029.0000 - val_tn: 3857765.0000 - val_fn: 38744.0000 - val_accuracy: 0.9816 - val_precision: 0.8630 - val_recall: 0.8608 - val_auc: 0.9347\n",
            "Epoch 22/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0078 - tp: 281475.5238 - fp: 3179.6190 - tn: 4026175.0476 - fn: 6335.5238 - accuracy: 0.9978 - precision: 0.9888 - recall: 0.9779 - auc: 0.9982 - val_loss: 0.1530 - val_tp: 239637.0000 - val_fp: 37899.0000 - val_tn: 3857895.0000 - val_fn: 38634.0000 - val_accuracy: 0.9817 - val_precision: 0.8634 - val_recall: 0.8612 - val_auc: 0.9349\n",
            "Epoch 23/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0073 - tp: 281898.7143 - fp: 2875.3492 - tn: 4026479.3175 - fn: 5912.3333 - accuracy: 0.9979 - precision: 0.9898 - recall: 0.9793 - auc: 0.9985 - val_loss: 0.1615 - val_tp: 239806.0000 - val_fp: 37785.0000 - val_tn: 3858009.0000 - val_fn: 38465.0000 - val_accuracy: 0.9817 - val_precision: 0.8639 - val_recall: 0.8618 - val_auc: 0.9350\n",
            "Epoch 24/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0069 - tp: 282227.5079 - fp: 2650.7778 - tn: 4026703.8889 - fn: 5583.5397 - accuracy: 0.9981 - precision: 0.9907 - recall: 0.9806 - auc: 0.9985 - val_loss: 0.1731 - val_tp: 239849.0000 - val_fp: 37764.0000 - val_tn: 3858030.0000 - val_fn: 38422.0000 - val_accuracy: 0.9817 - val_precision: 0.8640 - val_recall: 0.8619 - val_auc: 0.9349\n",
            "Epoch 25/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0066 - tp: 282622.5714 - fp: 2399.5556 - tn: 4026955.1111 - fn: 5188.4762 - accuracy: 0.9982 - precision: 0.9916 - recall: 0.9819 - auc: 0.9984 - val_loss: 0.1804 - val_tp: 239900.0000 - val_fp: 37773.0000 - val_tn: 3858021.0000 - val_fn: 38371.0000 - val_accuracy: 0.9818 - val_precision: 0.8640 - val_recall: 0.8621 - val_auc: 0.9349\n",
            "Epoch 26/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0064 - tp: 282969.2540 - fp: 2304.8413 - tn: 4027049.8254 - fn: 4841.7937 - accuracy: 0.9983 - precision: 0.9918 - recall: 0.9829 - auc: 0.9985 - val_loss: 0.1920 - val_tp: 240121.0000 - val_fp: 37507.0000 - val_tn: 3858287.0000 - val_fn: 38150.0000 - val_accuracy: 0.9819 - val_precision: 0.8649 - val_recall: 0.8629 - val_auc: 0.9350\n",
            "Epoch 27/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0061 - tp: 283473.9048 - fp: 2209.3175 - tn: 4027145.3492 - fn: 4337.1429 - accuracy: 0.9985 - precision: 0.9922 - recall: 0.9849 - auc: 0.9985 - val_loss: 0.1994 - val_tp: 240156.0000 - val_fp: 37479.0000 - val_tn: 3858315.0000 - val_fn: 38115.0000 - val_accuracy: 0.9819 - val_precision: 0.8650 - val_recall: 0.8630 - val_auc: 0.9350\n",
            "Epoch 28/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0058 - tp: 283731.7937 - fp: 2116.8889 - tn: 4027237.7778 - fn: 4079.2540 - accuracy: 0.9986 - precision: 0.9927 - recall: 0.9858 - auc: 0.9986 - val_loss: 0.2081 - val_tp: 240147.0000 - val_fp: 37546.0000 - val_tn: 3858248.0000 - val_fn: 38124.0000 - val_accuracy: 0.9819 - val_precision: 0.8648 - val_recall: 0.8630 - val_auc: 0.9349\n",
            "Epoch 29/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0056 - tp: 283893.3016 - fp: 2052.5873 - tn: 4027302.0794 - fn: 3917.7460 - accuracy: 0.9986 - precision: 0.9927 - recall: 0.9863 - auc: 0.9985 - val_loss: 0.2196 - val_tp: 240131.0000 - val_fp: 37489.0000 - val_tn: 3858305.0000 - val_fn: 38140.0000 - val_accuracy: 0.9819 - val_precision: 0.8650 - val_recall: 0.8629 - val_auc: 0.9345\n",
            "Epoch 30/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0054 - tp: 284043.1746 - fp: 1940.3333 - tn: 4027414.3333 - fn: 3767.8730 - accuracy: 0.9987 - precision: 0.9932 - recall: 0.9867 - auc: 0.9986 - val_loss: 0.2262 - val_tp: 240119.0000 - val_fp: 37677.0000 - val_tn: 3858117.0000 - val_fn: 38152.0000 - val_accuracy: 0.9818 - val_precision: 0.8644 - val_recall: 0.8629 - val_auc: 0.9342\n",
            "Epoch 31/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0052 - tp: 284204.7302 - fp: 1891.7302 - tn: 4027462.9365 - fn: 3606.3175 - accuracy: 0.9987 - precision: 0.9934 - recall: 0.9875 - auc: 0.9986 - val_loss: 0.2360 - val_tp: 240116.0000 - val_fp: 37829.0000 - val_tn: 3857965.0000 - val_fn: 38155.0000 - val_accuracy: 0.9818 - val_precision: 0.8639 - val_recall: 0.8629 - val_auc: 0.9338\n",
            "Epoch 32/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0051 - tp: 284272.1429 - fp: 1813.1270 - tn: 4027541.5397 - fn: 3538.9048 - accuracy: 0.9987 - precision: 0.9935 - recall: 0.9875 - auc: 0.9986 - val_loss: 0.2445 - val_tp: 239996.0000 - val_fp: 38070.0000 - val_tn: 3857724.0000 - val_fn: 38275.0000 - val_accuracy: 0.9817 - val_precision: 0.8631 - val_recall: 0.8625 - val_auc: 0.9330\n",
            "Epoch 33/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0050 - tp: 284428.7143 - fp: 1789.2063 - tn: 4027565.4603 - fn: 3382.3333 - accuracy: 0.9988 - precision: 0.9937 - recall: 0.9882 - auc: 0.9987 - val_loss: 0.2559 - val_tp: 240025.0000 - val_fp: 37848.0000 - val_tn: 3857946.0000 - val_fn: 38246.0000 - val_accuracy: 0.9818 - val_precision: 0.8638 - val_recall: 0.8626 - val_auc: 0.9334\n",
            "Epoch 34/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0047 - tp: 284529.8413 - fp: 1711.5079 - tn: 4027643.1587 - fn: 3281.2063 - accuracy: 0.9988 - precision: 0.9940 - recall: 0.9886 - auc: 0.9987 - val_loss: 0.2643 - val_tp: 239969.0000 - val_fp: 37946.0000 - val_tn: 3857848.0000 - val_fn: 38302.0000 - val_accuracy: 0.9817 - val_precision: 0.8635 - val_recall: 0.8624 - val_auc: 0.9329\n",
            "Epoch 35/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0045 - tp: 284582.9206 - fp: 1665.9841 - tn: 4027688.6825 - fn: 3228.1270 - accuracy: 0.9989 - precision: 0.9943 - recall: 0.9889 - auc: 0.9988 - val_loss: 0.2725 - val_tp: 239897.0000 - val_fp: 38018.0000 - val_tn: 3857776.0000 - val_fn: 38374.0000 - val_accuracy: 0.9817 - val_precision: 0.8632 - val_recall: 0.8621 - val_auc: 0.9316\n",
            "Epoch 36/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0044 - tp: 284646.2698 - fp: 1644.3492 - tn: 4027710.3175 - fn: 3164.7778 - accuracy: 0.9989 - precision: 0.9943 - recall: 0.9890 - auc: 0.9989 - val_loss: 0.2850 - val_tp: 239092.0000 - val_fp: 37976.0000 - val_tn: 3857818.0000 - val_fn: 39179.0000 - val_accuracy: 0.9815 - val_precision: 0.8629 - val_recall: 0.8592 - val_auc: 0.9318\n",
            "Epoch 37/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0043 - tp: 284709.2698 - fp: 1624.0476 - tn: 4027730.6190 - fn: 3101.7778 - accuracy: 0.9989 - precision: 0.9944 - recall: 0.9893 - auc: 0.9989 - val_loss: 0.2892 - val_tp: 239073.0000 - val_fp: 38168.0000 - val_tn: 3857626.0000 - val_fn: 39198.0000 - val_accuracy: 0.9815 - val_precision: 0.8623 - val_recall: 0.8591 - val_auc: 0.9298\n",
            "Epoch 38/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0042 - tp: 284897.1111 - fp: 1590.7937 - tn: 4027763.8730 - fn: 2913.9365 - accuracy: 0.9989 - precision: 0.9945 - recall: 0.9897 - auc: 0.9989 - val_loss: 0.3045 - val_tp: 238976.0000 - val_fp: 38795.0000 - val_tn: 3856999.0000 - val_fn: 39295.0000 - val_accuracy: 0.9813 - val_precision: 0.8603 - val_recall: 0.8588 - val_auc: 0.9296\n",
            "Epoch 39/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0041 - tp: 285048.8254 - fp: 1589.1746 - tn: 4027765.4921 - fn: 2762.2222 - accuracy: 0.9990 - precision: 0.9945 - recall: 0.9903 - auc: 0.9989 - val_loss: 0.3036 - val_tp: 238854.0000 - val_fp: 38401.0000 - val_tn: 3857393.0000 - val_fn: 39417.0000 - val_accuracy: 0.9814 - val_precision: 0.8615 - val_recall: 0.8584 - val_auc: 0.9293\n",
            "Epoch 40/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0041 - tp: 285231.2222 - fp: 1604.5397 - tn: 4027750.1270 - fn: 2579.8254 - accuracy: 0.9990 - precision: 0.9943 - recall: 0.9910 - auc: 0.9989 - val_loss: 0.3197 - val_tp: 238713.0000 - val_fp: 38950.0000 - val_tn: 3856844.0000 - val_fn: 39558.0000 - val_accuracy: 0.9812 - val_precision: 0.8597 - val_recall: 0.8578 - val_auc: 0.9293\n",
            "Epoch 41/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0039 - tp: 285470.0794 - fp: 1583.5873 - tn: 4027771.0794 - fn: 2340.9683 - accuracy: 0.9991 - precision: 0.9945 - recall: 0.9919 - auc: 0.9990 - val_loss: 0.3185 - val_tp: 238701.0000 - val_fp: 39138.0000 - val_tn: 3856656.0000 - val_fn: 39570.0000 - val_accuracy: 0.9811 - val_precision: 0.8591 - val_recall: 0.8578 - val_auc: 0.9290\n",
            "Epoch 42/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0038 - tp: 285613.6825 - fp: 1622.1429 - tn: 4027732.5238 - fn: 2197.3651 - accuracy: 0.9991 - precision: 0.9944 - recall: 0.9924 - auc: 0.9991 - val_loss: 0.3255 - val_tp: 238551.0000 - val_fp: 39370.0000 - val_tn: 3856424.0000 - val_fn: 39720.0000 - val_accuracy: 0.9811 - val_precision: 0.8583 - val_recall: 0.8573 - val_auc: 0.9288\n",
            "Epoch 43/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0037 - tp: 285722.5238 - fp: 1578.5079 - tn: 4027776.1587 - fn: 2088.5238 - accuracy: 0.9992 - precision: 0.9946 - recall: 0.9927 - auc: 0.9991 - val_loss: 0.3331 - val_tp: 238442.0000 - val_fp: 39396.0000 - val_tn: 3856398.0000 - val_fn: 39829.0000 - val_accuracy: 0.9810 - val_precision: 0.8582 - val_recall: 0.8569 - val_auc: 0.9286\n",
            "Epoch 44/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0036 - tp: 285791.8889 - fp: 1618.5238 - tn: 4027736.1429 - fn: 2019.1587 - accuracy: 0.9992 - precision: 0.9943 - recall: 0.9930 - auc: 0.9991 - val_loss: 0.3370 - val_tp: 238305.0000 - val_fp: 39546.0000 - val_tn: 3856248.0000 - val_fn: 39966.0000 - val_accuracy: 0.9810 - val_precision: 0.8577 - val_recall: 0.8564 - val_auc: 0.9284\n",
            "Epoch 45/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0035 - tp: 285833.3016 - fp: 1633.9048 - tn: 4027720.7619 - fn: 1977.7460 - accuracy: 0.9992 - precision: 0.9943 - recall: 0.9931 - auc: 0.9992 - val_loss: 0.3484 - val_tp: 238201.0000 - val_fp: 39678.0000 - val_tn: 3856116.0000 - val_fn: 40070.0000 - val_accuracy: 0.9809 - val_precision: 0.8572 - val_recall: 0.8560 - val_auc: 0.9282\n",
            "Epoch 46/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0035 - tp: 285903.3175 - fp: 1628.0794 - tn: 4027726.5873 - fn: 1907.7302 - accuracy: 0.9992 - precision: 0.9944 - recall: 0.9934 - auc: 0.9993 - val_loss: 0.3507 - val_tp: 237794.0000 - val_fp: 40093.0000 - val_tn: 3855701.0000 - val_fn: 40477.0000 - val_accuracy: 0.9807 - val_precision: 0.8557 - val_recall: 0.8545 - val_auc: 0.9277\n",
            "Epoch 47/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0033 - tp: 285991.9206 - fp: 1559.8889 - tn: 4027794.7778 - fn: 1819.1270 - accuracy: 0.9992 - precision: 0.9947 - recall: 0.9938 - auc: 0.9994 - val_loss: 0.3593 - val_tp: 236724.0000 - val_fp: 41140.0000 - val_tn: 3854654.0000 - val_fn: 41547.0000 - val_accuracy: 0.9802 - val_precision: 0.8519 - val_recall: 0.8507 - val_auc: 0.9276\n",
            "Epoch 48/300\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 0.0033 - tp: 286024.5873 - fp: 1536.4921 - tn: 4027818.1746 - fn: 1786.4603 - accuracy: 0.9992 - precision: 0.9947 - recall: 0.9938 - auc: 0.9993 - val_loss: 0.3669 - val_tp: 234919.0000 - val_fp: 42991.0000 - val_tn: 3852803.0000 - val_fn: 43352.0000 - val_accuracy: 0.9793 - val_precision: 0.8453 - val_recall: 0.8442 - val_auc: 0.9273\n",
            "Epoch 49/300\n",
            "43/62 [===================>..........] - ETA: 0s - loss: 0.0032 - tp: 196810.7674 - fp: 1027.7907 - tn: 2770972.2093 - fn: 1189.2326 - accuracy: 0.9992 - precision: 0.9947 - recall: 0.9939 - auc: 0.9994"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-238-87e8e49ce6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#callbacks=[early_stopping],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     validation_data=(X_val, y_val))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7q_MD9Q5_Jd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPDH0IA_8xee"
      },
      "source": [
        "y_pred=model_dnn.predict(X_test)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PdaWVhX9BvJ",
        "outputId": "f55da8d9-0aaf-45fe-b75e-98c7b4fca841"
      },
      "source": [
        "accuracy = model_dnn.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)\r\n",
        "print('Accuracy is: ', accuracy)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 1s 20ms/step - loss: 0.0593 - tp: 258242.0000 - fp: 19900.0000 - tn: 3875894.0000 - fn: 20029.0000 - accuracy: 0.9904 - precision: 0.9285 - recall: 0.9280 - auc: 0.9694\n",
            "Accuracy is:  [0.05925154313445091, 258242.0, 19900.0, 3875894.0, 20029.0, 0.9904339909553528, 0.928453803062439, 0.9280233979225159, 0.9694490432739258]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZcyLQPqAlWS",
        "outputId": "cc8ea8a2-a1bc-40b6-c5e8-564b77207347"
      },
      "source": [
        "display_metrics(y_test_ada, np.argmax(y_pred, axis = 1), labels_d)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.93\n",
            "\n",
            "Micro Precision: 0.93\n",
            "Micro Recall: 0.93\n",
            "Micro F1-score: 0.93\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.54\n",
            "Macro Recall: 0.42\n",
            "Macro F1-score: 0.43\n",
            "\n",
            "Weighted Precision: 0.94\n",
            "Weighted Recall: 0.93\n",
            "Weighted F1-score: 0.92\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.96      1.00      0.98    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.71      1.00      0.83     32006\n",
            "             DoS GoldenEye       0.94      0.82      0.88      2573\n",
            "                  DoS Hulk       1.00      0.75      0.86     57531\n",
            "          DoS Slowhttptest       0.58      0.84      0.69      1374\n",
            "             DoS slowloris       0.92      0.47      0.63      1449\n",
            "               FTP-Patator       1.00      0.30      0.46      1983\n",
            "                Heartbleed       0.00      0.00      0.00         3\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       1.00      0.99      1.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       1.00      0.10      0.17       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.93    278271\n",
            "                 macro avg       0.54      0.42      0.43    278271\n",
            "              weighted avg       0.94      0.93      0.92    278271\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erUa8gpIZ04U"
      },
      "source": [
        "# Model 10: CNN1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTmLBZDpZ04U"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Flatten, Dense, Activation,Dropout\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLXy3meMZ04U",
        "outputId": "f144a82f-a850-49c4-974e-1d2ae9ca3ca2"
      },
      "source": [
        "#hyper-params\n",
        "batch_size = 1024 # increasing batch size with more gpu added\n",
        "input_dim = X_train.shape[1]\n",
        "num_class = 15                   # 15 intrusion classes, including benign traffic class\n",
        "num_epochs = 30\n",
        "learning_rates = 1e-3\n",
        "regularizations = 1e-3\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "print(input_dim)\n",
        "print(num_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNRbsq6CZ04V",
        "outputId": "148ee859-e320-4a22-c118-cabe52ab7d56"
      },
      "source": [
        "#X_train_r = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_train_r = np.zeros((len(X_train), input_dim, 1))\n",
        "X_train_r[:, :, 0] = X_train[:, :input_dim]\n",
        "print(X_train_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjmIpn1HZ04W",
        "outputId": "2c1c2f93-014b-4c08-8435-e739318768fc"
      },
      "source": [
        "X_test_r = np.zeros((len(X_test), input_dim, 1))\n",
        "X_test_r[:, :, 0] = X_test[:, :input_dim]\n",
        "print(X_test_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_KlI1VeZ04W",
        "outputId": "68ffab00-7db7-42ca-d671-16330e9fa8f9"
      },
      "source": [
        "X_val_r = np.zeros((len(X_val), input_dim, 1))\n",
        "X_val_r[:, :, 0] = X_val[:, :input_dim]\n",
        "print(X_val_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkQlzQU0ajuD",
        "outputId": "71d58c31-8c34-435a-934b-9006668ffdc5"
      },
      "source": [
        "X_train_r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[7.66048676e-01],\n",
              "        [5.93603125e-03],\n",
              "        [1.00000000e+00],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[7.29167620e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [7.12500000e-01],\n",
              "        [7.12500000e-01]],\n",
              "\n",
              "       [[7.86144808e-01],\n",
              "        [6.76005616e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [8.96864890e-03],\n",
              "        [4.90833333e-01],\n",
              "        [4.83333333e-01]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[2.53009842e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[7.85610742e-01],\n",
              "        [8.08765183e-04],\n",
              "        [1.00000000e+00],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[8.26276036e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpTZU5OPZ04W",
        "outputId": "31aa9725-0fe7-4c0d-9ab7-1b4176ad7fcd"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', input_shape=(71,1)))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=3))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_class))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_8 (Conv1D)            (None, 71, 32)            128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 71, 32)            284       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 71, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 69, 128)           12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 69, 128)           276       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 69, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8832)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               883300    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 15)                1515      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 897,919\n",
            "Trainable params: 897,639\n",
            "Non-trainable params: 280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEpzDBNyC7P2"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_accuracy', \r\n",
        "    verbose=1,\r\n",
        "    patience=10,\r\n",
        "    mode='max',\r\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb52JhHSZ04W"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFJZ11t1Z04X"
      },
      "source": [
        "## Step 5. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq31n_0YZ04X",
        "outputId": "ad9b4e62-35a9-429f-9d10-4086725a878f"
      },
      "source": [
        "# fit network\n",
        "epochs = 50\n",
        "model.fit(X_train_r, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_r, y_test), verbose=1, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.9541 - val_accuracy: 0.8272\n",
            "Epoch 2/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.5939 - val_accuracy: 0.8978\n",
            "Epoch 3/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0106 - accuracy: 0.9963 - val_loss: 0.4867 - val_accuracy: 0.9373\n",
            "Epoch 4/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 1.2544 - val_accuracy: 0.7860\n",
            "Epoch 5/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.5194 - val_accuracy: 0.9147\n",
            "Epoch 6/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 0.7574 - val_accuracy: 0.8717\n",
            "Epoch 7/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.4678 - val_accuracy: 0.9551\n",
            "Epoch 8/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.3560 - val_accuracy: 0.9581\n",
            "Epoch 9/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 0.7849 - val_accuracy: 0.8832\n",
            "Epoch 10/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0091 - accuracy: 0.9967 - val_loss: 0.3110 - val_accuracy: 0.9654\n",
            "Epoch 11/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 0.7204 - val_accuracy: 0.8893\n",
            "Epoch 12/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 0.5198 - val_accuracy: 0.9458\n",
            "Epoch 13/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0090 - accuracy: 0.9967 - val_loss: 0.9469 - val_accuracy: 0.8524\n",
            "Epoch 14/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0085 - accuracy: 0.9968 - val_loss: 0.3824 - val_accuracy: 0.9637\n",
            "Epoch 15/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 0.3664 - val_accuracy: 0.9600\n",
            "Epoch 16/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0088 - accuracy: 0.9967 - val_loss: 0.5777 - val_accuracy: 0.9319\n",
            "Epoch 17/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0087 - accuracy: 0.9967 - val_loss: 0.3704 - val_accuracy: 0.9621\n",
            "Epoch 18/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0086 - accuracy: 0.9968 - val_loss: 0.4081 - val_accuracy: 0.9647\n",
            "Epoch 19/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.4933 - val_accuracy: 0.9559\n",
            "Epoch 20/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 1.0666 - val_accuracy: 0.8716\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00020: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7fd100bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSo0TpT5Z04X",
        "outputId": "d07be9f6-400d-481c-8ee2-ebceae01716b"
      },
      "source": [
        "# evaluate model\n",
        "accuracy = model.evaluate(X_val_r, y_val, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "272/272 [==============================] - 2s 6ms/step - loss: 79.3116 - accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqGNmOrhH1CL"
      },
      "source": [
        "y_pred=model.predict(X_val_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltyd2KVLIBCK",
        "outputId": "aad22263-814e-4f9e-a3c2-621c490ef3df"
      },
      "source": [
        "display_metrics(y_val_ada, np.argmax(y_pred, axis = 1), labels_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.50\n",
            "\n",
            "Micro Precision: 0.50\n",
            "Micro Recall: 0.50\n",
            "Micro F1-score: 0.50\n",
            "\n",
            "Macro Precision: 0.03\n",
            "Macro Recall: 0.07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.04\n",
            "\n",
            "Weighted Precision: 0.25\n",
            "Weighted Recall: 0.50\n",
            "Weighted F1-score: 0.33\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.50      1.00      0.67    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.00      0.00      0.00     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         2\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.50    278270\n",
            "                 macro avg       0.03      0.07      0.04    278270\n",
            "              weighted avg       0.25      0.50      0.33    278270\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbhp6oW_Z04X"
      },
      "source": [
        ""
      ]
    }
  ]
}