{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ML_DNN_78features_split_70_30.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwangliberty/AIoTDesign-Frontend/blob/master/ML_DNN_78features_split_70_30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq5p9swvZ04G"
      },
      "source": [
        "# Baseline Models for CICIDS 2017 Data Set with 78 Features "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXg4HMXOZ04H"
      },
      "source": [
        "We use the pre-processing dataset that is augmented by 7 new connection based features, and splitted into 70:15:15.  We use the following classification methods: PCA+RF,Naive Bayes model, Decision Tree Classifier, Random Foresty with DecisionTree, Logistic Regression Classifier, Adaboost, Voting, kNN, and DNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOKT1sRZ04I"
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH9hOnpmzv6-"
      },
      "source": [
        "def display_metrics(y_test, y_pred, label_names):\r\n",
        "  print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\r\n",
        "\r\n",
        "  print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\r\n",
        "  print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\r\n",
        "  print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\r\n",
        "\r\n",
        "  print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\r\n",
        "  print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\r\n",
        "  print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\r\n",
        "\r\n",
        "  print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\r\n",
        "  print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\r\n",
        "  print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\r\n",
        "\r\n",
        "  print('\\nClassification Report\\n')\r\n",
        "  print(classification_report(y_test, y_pred, target_names=label_names))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCwZZFMTZ04J"
      },
      "source": [
        "def display_all(df):\n",
        "    with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 100): \n",
        "        print(df)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDogrv3Z04J"
      },
      "source": [
        "def make_value2index(attacks):\n",
        "    #make dictionary\n",
        "    attacks = sorted(attacks)\n",
        "    d = {}\n",
        "    counter=0\n",
        "    for attack in attacks:\n",
        "        d[attack] = counter\n",
        "        counter+=1\n",
        "    return d"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3b8hIQZ04K"
      },
      "source": [
        "# chganges label from string to integer/index\n",
        "def encode_label(Y_str):\n",
        "    labels_d = make_value2index(np.unique(Y_str))\n",
        "    Y = [labels_d[y_str] for y_str  in Y_str]\n",
        "    Y = np.array(Y)\n",
        "    return np.array(Y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMisXWZWZ04K"
      },
      "source": [
        "## Step 1. Loading csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwxZ7DrXZ04L"
      },
      "source": [
        "# All columns\n",
        "col_names = np.array(['dst sport count', 'src dport count', 'dst src count', 'dport count', 'sport count', 'dst host count','src host count','Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoCkMRAcaIA6"
      },
      "source": [
        "### Option 1. Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uaw_A5kaHSj",
        "outputId": "f2a0b593-d10c-46f9-ae9c-e9d78edba577"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qUtBnilZ04M"
      },
      "source": [
        "# load three csv files generated by mlp4nids (Multi-layer perceptron for network intrusion detection )\n",
        "# first load the train set\n",
        "df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set_ext78_2.csv',names=col_names, skiprows=1)  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBn8DsjhX_U4",
        "outputId": "872c2a6a-6a85-4ed5-8a72-1da0805d0678"
      },
      "source": [
        "print('Train set size: ', df_train.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size:  (879589, 79)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYvaCAjZ04P",
        "outputId": "4663a30e-68f2-4412-ba1b-384bfa4d0778"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set_ext78_2.csv',names=col_names, skiprows=1)  \n",
        "print('Test set size: ', df_test.shape)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set_ext78_2.csv',names=col_names, skiprows=1)  \n",
        "print('Validation set size: ', df_val.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set size:  (188483, 79)\n",
            "Validation set size:  (188484, 79)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cbbuInpXhvz"
      },
      "source": [
        "### Option 2. Load from local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLcmd-A8Xhv0"
      },
      "source": [
        "dataroot = '../data/cicids2017clean/train_set_ext78_2.csv'\n",
        "df_train = pd.read_csv(dataroot, names=col_names, skiprows=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9oqAbFyXhv0"
      },
      "source": [
        "dataroot = '../data/cicids2017clean/crossval_set_ext78_2.csv'\n",
        "df_val = pd.read_csv(dataroot, names=col_names, skiprows=1) \n",
        "dataroot = '../data/cicids2017clean/test_set_ext78_2.csv'\n",
        "df_test = pd.read_csv(dataroot, names=col_names, skiprows=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls46tMA9Xhv0"
      },
      "source": [
        "## Step 2. Exploring the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "q-oBvrtQXhv0",
        "outputId": "840aba8e-0a1a-4d4f-9c0a-19314dcc542c"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>61477</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>98136542</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>56</td>\n",
              "      <td>11601</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.656854</td>\n",
              "      <td>5840</td>\n",
              "      <td>0</td>\n",
              "      <td>2320.20</td>\n",
              "      <td>2436.833027</td>\n",
              "      <td>118.783480</td>\n",
              "      <td>0.132468</td>\n",
              "      <td>8.178045e+06</td>\n",
              "      <td>2.460000e+07</td>\n",
              "      <td>85700000</td>\n",
              "      <td>1</td>\n",
              "      <td>97900000</td>\n",
              "      <td>14000000.0</td>\n",
              "      <td>3.190000e+07</td>\n",
              "      <td>85700000</td>\n",
              "      <td>1</td>\n",
              "      <td>286190</td>\n",
              "      <td>71547.5</td>\n",
              "      <td>137766.35380</td>\n",
              "      <td>278139</td>\n",
              "      <td>181</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>112</td>\n",
              "      <td>0.081519</td>\n",
              "      <td>0.050949</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>833.071429</td>\n",
              "      <td>1774.906302</td>\n",
              "      <td>3.150292e+06</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>897.153846</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2320.20</td>\n",
              "      <td>8</td>\n",
              "      <td>56</td>\n",
              "      <td>5</td>\n",
              "      <td>11601</td>\n",
              "      <td>256</td>\n",
              "      <td>229</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>996.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>996</td>\n",
              "      <td>996</td>\n",
              "      <td>48900000.0</td>\n",
              "      <td>51900000.0</td>\n",
              "      <td>85700000</td>\n",
              "      <td>12200000</td>\n",
              "      <td>DDoS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>13</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>33</td>\n",
              "      <td>49665</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>121917</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>687</td>\n",
              "      <td>361</td>\n",
              "      <td>681</td>\n",
              "      <td>0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>391.454978</td>\n",
              "      <td>349</td>\n",
              "      <td>0</td>\n",
              "      <td>90.25</td>\n",
              "      <td>172.523187</td>\n",
              "      <td>8596.012041</td>\n",
              "      <td>57.416111</td>\n",
              "      <td>2.031950e+04</td>\n",
              "      <td>2.482528e+04</td>\n",
              "      <td>61854</td>\n",
              "      <td>1</td>\n",
              "      <td>91167</td>\n",
              "      <td>45583.5</td>\n",
              "      <td>2.300996e+04</td>\n",
              "      <td>61854</td>\n",
              "      <td>29313</td>\n",
              "      <td>92688</td>\n",
              "      <td>30896.0</td>\n",
              "      <td>52536.13923</td>\n",
              "      <td>91556</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>88</td>\n",
              "      <td>24.606905</td>\n",
              "      <td>32.809206</td>\n",
              "      <td>0</td>\n",
              "      <td>681</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>253.090046</td>\n",
              "      <td>6.405457e+04</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>149.714286</td>\n",
              "      <td>229.0</td>\n",
              "      <td>90.25</td>\n",
              "      <td>3</td>\n",
              "      <td>687</td>\n",
              "      <td>4</td>\n",
              "      <td>361</td>\n",
              "      <td>8192</td>\n",
              "      <td>5061</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>7</td>\n",
              "      <td>59588</td>\n",
              "      <td>53</td>\n",
              "      <td>17</td>\n",
              "      <td>30773</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>74</td>\n",
              "      <td>262</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>131</td>\n",
              "      <td>131</td>\n",
              "      <td>131.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10918.662464</td>\n",
              "      <td>129.984077</td>\n",
              "      <td>1.025767e+04</td>\n",
              "      <td>1.776074e+04</td>\n",
              "      <td>30766</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>64.992038</td>\n",
              "      <td>64.992038</td>\n",
              "      <td>37</td>\n",
              "      <td>131</td>\n",
              "      <td>74.600000</td>\n",
              "      <td>51.485920</td>\n",
              "      <td>2.650800e+03</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>93.250000</td>\n",
              "      <td>37.0</td>\n",
              "      <td>131.00</td>\n",
              "      <td>2</td>\n",
              "      <td>74</td>\n",
              "      <td>2</td>\n",
              "      <td>262</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>80</td>\n",
              "      <td>50918</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16949.152540</td>\n",
              "      <td>1.180000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>118</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>8474.576271</td>\n",
              "      <td>8474.576271</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>905</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>55989</td>\n",
              "      <td>80</td>\n",
              "      <td>6</td>\n",
              "      <td>47</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>42553.191490</td>\n",
              "      <td>4.700000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>47.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>47</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>42553.191490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>259</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  dst src count  ...  Idle Max  Idle Min   Label\n",
              "0                2              100            100  ...  85700000  12200000    DDoS\n",
              "1                2               23             13  ...         0         0  BENIGN\n",
              "2                1                2              3  ...         0         0  BENIGN\n",
              "3                2                1              2  ...         0         0  BENIGN\n",
              "4                2                4              2  ...         0         0  BENIGN\n",
              "\n",
              "[5 rows x 79 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miDxhmNyZ04N"
      },
      "source": [
        "Count the number of attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpKy7b9fZ04O",
        "scrolled": true,
        "outputId": "605613af-e833-43d0-d24f-9a5858878256"
      },
      "source": [
        "df_train['Label'].value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        490029\n",
              "DoS Hulk                      161134\n",
              "PortScan                      110992\n",
              "DDoS                           89590\n",
              "DoS GoldenEye                   7220\n",
              "FTP-Patator                     5621\n",
              "SSH-Patator                     4153\n",
              "DoS slowloris                   4049\n",
              "DoS Slowhttptest                3833\n",
              "Bot                             1401\n",
              "Web Attack � Brute Force        1059\n",
              "Web Attack � XSS                 463\n",
              "Infiltration                      25\n",
              "Web Attack � Sql Injection        12\n",
              "Heartbleed                         8\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2X3KG4zZ04P"
      },
      "source": [
        "Read test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5hWm9Q-Z04Q",
        "outputId": "d62ed139-09af-4d89-b717-9dfc11d0a4e6"
      },
      "source": [
        "print('Test set: ')\n",
        "df_test['Label'].value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        105019\n",
              "DoS Hulk                       34547\n",
              "PortScan                       23846\n",
              "DDoS                           19271\n",
              "DoS GoldenEye                   1542\n",
              "FTP-Patator                     1178\n",
              "DoS slowloris                    834\n",
              "DoS Slowhttptest                 828\n",
              "SSH-Patator                      826\n",
              "Bot                              280\n",
              "Web Attack � Brute Force         209\n",
              "Web Attack � XSS                  93\n",
              "Web Attack � Sql Injection         7\n",
              "Heartbleed                         2\n",
              "Infiltration                       1\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0-lHNicZ04Q",
        "outputId": "949aeb2c-f5bb-4718-a8cd-06dd26658fcf"
      },
      "source": [
        "print('Validation set: ')\n",
        "df_val['Label'].value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        104952\n",
              "DoS Hulk                       34443\n",
              "PortScan                       23966\n",
              "DDoS                           19164\n",
              "DoS GoldenEye                   1531\n",
              "FTP-Patator                     1136\n",
              "SSH-Patator                      918\n",
              "DoS slowloris                    913\n",
              "DoS Slowhttptest                 838\n",
              "Bot                              275\n",
              "Web Attack � Brute Force         239\n",
              "Web Attack � XSS                  96\n",
              "Infiltration                      10\n",
              "Web Attack � Sql Injection         2\n",
              "Heartbleed                         1\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0O9ZuKoXhv3"
      },
      "source": [
        "## Step 3. Encode Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwXKhZOjXhv3"
      },
      "source": [
        "Encoding the labels, and generate numpy array. Note that the label has not been encoded as one-hot coding. We will use one-hot code later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyAnny9yZ04R"
      },
      "source": [
        "### Step 3.1 Encoding train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsR6_hJDZ04R"
      },
      "source": [
        "df_label = df_train['Label']\n",
        "data = df_train.drop(columns=['Label'])\n",
        "Xtrain = data.values\n",
        "y_train = encode_label(df_label.values)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XlV2AK3Z04S"
      },
      "source": [
        "### Step 3.2. Encoding test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVSrGExFZ04S",
        "scrolled": true
      },
      "source": [
        "df_label = df_test['Label']\n",
        "data = df_test.drop(columns=['Label'])\n",
        "Xtest = data.values\n",
        "y_test = encode_label(df_label.values)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO3PgredZ04T"
      },
      "source": [
        "### Step 3.3 Encoding validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ2wDKpZZ04T"
      },
      "source": [
        "df_label = df_val['Label']\n",
        "data = df_val.drop(columns=['Label'])\n",
        "Xval = data.values\n",
        "y_val = encode_label(df_label.values)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viMki-R0Z04Q"
      },
      "source": [
        "## Step 4. Normalization or Standardization\n",
        "\n",
        "The continuous feature values are normalized into the same feature space. This is important when using features that have different measurements, and is a general requirement of many machine learning algorithms. We implement the two methods to see the impact on the final classifications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8ERJidxXhv5"
      },
      "source": [
        "## Option 1. Normalization\n",
        "\n",
        "The values of the datasets are normalized using the Min-Max scaling technique, bringing them all within a range of [0,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c523vLd-Z04R"
      },
      "source": [
        "### Step 4.1 Normalizing train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5izaj07Z04R"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbYQFmfgZ04S",
        "scrolled": true,
        "outputId": "9c367b0f-6831-4d98-b7e4-065482de3b7f"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(Xtrain)\n",
        "X_train"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01010101, 1.        , 1.        , ..., 0.68650794, 0.71416667,\n",
              "        0.10166667],\n",
              "       [0.01010101, 0.22222222, 0.12121212, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.01010101, 0.02020202, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.12121212, 0.12121212, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.87878788, 0.87878788, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [1.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ5x1QxAXhv5"
      },
      "source": [
        "### Step 4.2. Normalizing validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-_BpWSsXhv6",
        "outputId": "33b35722-9f82-459e-f53d-d2cbef0687ce"
      },
      "source": [
        "X_val = scaler.fit_transform(Xval)\n",
        "X_val"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.32323232, 0.32323232, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.05050505, 0.05050505, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.02020202, 0.09090909, 0.02020202, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 1.        , 1.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 1.        , 1.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.96969697, ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TItkmTF1Z04S"
      },
      "source": [
        "### Step 4.3. Normalizing test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXmp2w2bZ04T",
        "outputId": "49d9cf30-8446-4c0b-fd85-900683b0e7f8"
      },
      "source": [
        "X_test = scaler.fit_transform(Xtest)\n",
        "X_test"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.96969697, 0.96969697, ..., 0.        , 0.71583333,\n",
              "        0.71583333],\n",
              "       [0.        , 0.96969697, 0.96969697, ..., 0.        , 0.6975    ,\n",
              "        0.6975    ],\n",
              "       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.44444444, 0.08080808, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.78787879, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.28282828, 0.28282828, ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge2XVkhTXhv6"
      },
      "source": [
        "## Option 2. Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu459dh3Xhv7"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOJud1h5Xhv7",
        "outputId": "b4614821-64c3-4de1-e8f9-3e6cf4430a36"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(Xtrain)\n",
        "X_val = scaler.fit_transform(Xval)\n",
        "X_test = scaler.fit_transform(Xtest)\n",
        "\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.46360917, -0.21367668, -0.60970034, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [-0.46360917,  1.19349981,  0.79867464, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [-0.46360917, -1.03452964, -1.43125242, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       ...,\n",
              "       [-0.32230411, -1.03452964, -1.31388783, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [-0.46360917, -0.82345316, -1.21999617, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976],\n",
              "       [-0.46360917, -0.28403551, -0.68011909, ..., -0.16609924,\n",
              "        -0.51046627, -0.47636976]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reoNDQZhZ04T"
      },
      "source": [
        "## Step 5 One-hot encoding for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So8gvIF8Z04T"
      },
      "source": [
        "y_train, y_test and y_val have to be one-hot-encoded. That means they must have dimension (number_of_samples, 15), where 15 denotes number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc97u4oZZ04U"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfeM_ZzsXhv8"
      },
      "source": [
        "Save the labels for AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N0GfC_zXhv8"
      },
      "source": [
        "y_train_ada = y_train\n",
        "y_test_ada = y_test\n",
        "y_val_ada = y_val"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQVqV19KZ04U"
      },
      "source": [
        "y_train = to_categorical(y_train, 15)\n",
        "y_test = to_categorical(y_test, 15)\n",
        "y_val = to_categorical(y_val, 15)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd9_XX_5Xhv8"
      },
      "source": [
        "## Step 6. Define the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOSi1KcXhv8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#importing confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#importing accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUup6sodXhv9"
      },
      "source": [
        "METRICS = [\n",
        "      tf.keras.metrics.TruePositives(name='tp'),\n",
        "      tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.AUC(name='auc'),\n",
        "]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJywwX9iXhv9"
      },
      "source": [
        "#  Model 1: PCA  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTYHuZtxXhv9"
      },
      "source": [
        "X_pca = df_train.drop('Label',axis=1)\n",
        "y_pca = df_train['Label']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XWs2_kXhv9"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_pca = scaler.fit_transform(X_pca)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGVxxHw2Xhv9"
      },
      "source": [
        "dfx = pd.DataFrame(data=X_pca,columns=df_train.columns[1:])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "2-gTg4_uXhv-",
        "outputId": "20cb6fc6-2b4e-4595-8fd9-bec6e543c505"
      },
      "source": [
        "dfx.head(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.938079</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>8.178046e-01</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>1.953868e-05</td>\n",
              "      <td>1.813646e-05</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.336096</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530855</td>\n",
              "      <td>0.362854</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>6.815038e-02</td>\n",
              "      <td>0.290094</td>\n",
              "      <td>7.141667e-01</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>8.158333e-01</td>\n",
              "      <td>1.166667e-01</td>\n",
              "      <td>0.381123</td>\n",
              "      <td>7.141667e-01</td>\n",
              "      <td>1.083333e-07</td>\n",
              "      <td>2.384917e-03</td>\n",
              "      <td>5.962292e-04</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>2.317825e-03</td>\n",
              "      <td>1.508333e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>2.717302e-08</td>\n",
              "      <td>2.547471e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.285298</td>\n",
              "      <td>0.375124</td>\n",
              "      <td>1.406381e-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.268839</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.530855</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>1.953868e-05</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>1.813646e-05</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.960000e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.960000e-06</td>\n",
              "      <td>9.960000e-06</td>\n",
              "      <td>0.407500</td>\n",
              "      <td>0.686508</td>\n",
              "      <td>0.714167</td>\n",
              "      <td>0.101667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.323232</td>\n",
              "      <td>0.757839</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.015983e-03</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>2.396977e-04</td>\n",
              "      <td>5.643706e-07</td>\n",
              "      <td>0.027438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038557</td>\n",
              "      <td>0.058490</td>\n",
              "      <td>0.020085</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020649</td>\n",
              "      <td>0.025689</td>\n",
              "      <td>0.005765</td>\n",
              "      <td>0.400011</td>\n",
              "      <td>1.693375e-04</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>5.154583e-04</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>7.597250e-04</td>\n",
              "      <td>3.798625e-04</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>5.154500e-04</td>\n",
              "      <td>2.443750e-04</td>\n",
              "      <td>7.724000e-04</td>\n",
              "      <td>2.574667e-04</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>7.629667e-04</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>8.202302e-06</td>\n",
              "      <td>1.640460e-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027438</td>\n",
              "      <td>0.044863</td>\n",
              "      <td>0.053490</td>\n",
              "      <td>2.859579e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.044863</td>\n",
              "      <td>0.038557</td>\n",
              "      <td>0.020649</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>2.396977e-04</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>5.643706e-07</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.077240</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.575758</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.585859</td>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.909255</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.564500e-04</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>2.581897e-05</td>\n",
              "      <td>4.095986e-07</td>\n",
              "      <td>0.001491</td>\n",
              "      <td>0.017918</td>\n",
              "      <td>0.006230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007539</td>\n",
              "      <td>0.066062</td>\n",
              "      <td>0.029972</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005766</td>\n",
              "      <td>0.400026</td>\n",
              "      <td>8.548889e-05</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>2.563917e-04</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>2.166401e-05</td>\n",
              "      <td>3.249602e-05</td>\n",
              "      <td>0.025552</td>\n",
              "      <td>0.005278</td>\n",
              "      <td>0.025548</td>\n",
              "      <td>0.010881</td>\n",
              "      <td>1.183393e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.027943</td>\n",
              "      <td>0.006230</td>\n",
              "      <td>0.029972</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>2.581897e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>4.095986e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.776971</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>9.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.403390</td>\n",
              "      <td>9.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.916667e-07</td>\n",
              "      <td>1.091667e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>2.824859e-03</td>\n",
              "      <td>4.237288e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.013824</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.854337</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.408511</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>4.999999e-07</td>\n",
              "      <td>3.916667e-07</td>\n",
              "      <td>3.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.916667e-07</td>\n",
              "      <td>4.916666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>1.418440e-02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003967</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.537087</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>5.259139e-01</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>8.750001e-02</td>\n",
              "      <td>0.140330</td>\n",
              "      <td>2.675000e-01</td>\n",
              "      <td>8.318082e-03</td>\n",
              "      <td>5.258333e-01</td>\n",
              "      <td>8.750000e-02</td>\n",
              "      <td>0.142174</td>\n",
              "      <td>2.675000e-01</td>\n",
              "      <td>8.318074e-03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995719</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>3.697268e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.445572</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.014080e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.014080e-02</td>\n",
              "      <td>7.014080e-02</td>\n",
              "      <td>0.155833</td>\n",
              "      <td>0.161376</td>\n",
              "      <td>0.267500</td>\n",
              "      <td>0.066799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.989899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>0.842161</td>\n",
              "      <td>0.050722</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>6.978099e-07</td>\n",
              "      <td>9.380121e-09</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000969</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.003026</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005827</td>\n",
              "      <td>0.406897</td>\n",
              "      <td>4.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.916667e-07</td>\n",
              "      <td>5.916666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>5.747126e-03</td>\n",
              "      <td>8.620690e-03</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>2.380952e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.001498</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.978099e-07</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>9.380121e-09</td>\n",
              "      <td>0.015640</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.879652</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>8.218116e-01</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>1.189766e-04</td>\n",
              "      <td>1.812708e-05</td>\n",
              "      <td>0.013739</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.020801</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530580</td>\n",
              "      <td>0.541087</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>8.218117e-02</td>\n",
              "      <td>0.367925</td>\n",
              "      <td>8.216667e-01</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>8.216667e-01</td>\n",
              "      <td>1.641667e-01</td>\n",
              "      <td>0.526882</td>\n",
              "      <td>8.216667e-01</td>\n",
              "      <td>1.083333e-07</td>\n",
              "      <td>3.930917e-04</td>\n",
              "      <td>9.827292e-05</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>2.981500e-04</td>\n",
              "      <td>1.516667e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>2.028040e-08</td>\n",
              "      <td>2.535050e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.350040</td>\n",
              "      <td>0.340639</td>\n",
              "      <td>0.525291</td>\n",
              "      <td>2.757738e-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.325156</td>\n",
              "      <td>0.009569</td>\n",
              "      <td>0.530580</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>1.189766e-04</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>1.812708e-05</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.000000e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.000000e-08</td>\n",
              "      <td>5.000000e-08</td>\n",
              "      <td>0.821667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.821667</td>\n",
              "      <td>0.821667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.720714</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.092667e-04</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>4.186860e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.002906</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400061</td>\n",
              "      <td>3.642778e-05</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>1.052250e-04</td>\n",
              "      <td>3.500000e-07</td>\n",
              "      <td>4.041667e-06</td>\n",
              "      <td>4.041667e-06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.041667e-06</td>\n",
              "      <td>4.141666e-06</td>\n",
              "      <td>1.090167e-04</td>\n",
              "      <td>1.090167e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.090167e-04</td>\n",
              "      <td>1.090167e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>5.084789e-05</td>\n",
              "      <td>7.627183e-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.001233</td>\n",
              "      <td>0.000695</td>\n",
              "      <td>4.821429e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.001348</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>4.186860e-06</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.821530</td>\n",
              "      <td>0.006760</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>5.015780e-01</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>2.093430e-04</td>\n",
              "      <td>7.871485e-06</td>\n",
              "      <td>0.009146</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009184</td>\n",
              "      <td>0.012029</td>\n",
              "      <td>0.092829</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.082285</td>\n",
              "      <td>0.093520</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>2.089909e-02</td>\n",
              "      <td>0.139151</td>\n",
              "      <td>4.825000e-01</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>4.866667e-01</td>\n",
              "      <td>4.870064e-02</td>\n",
              "      <td>0.218638</td>\n",
              "      <td>4.825000e-01</td>\n",
              "      <td>1.766666e-06</td>\n",
              "      <td>5.016667e-01</td>\n",
              "      <td>3.856817e-02</td>\n",
              "      <td>0.193991</td>\n",
              "      <td>4.841667e-01</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>6.091886e-08</td>\n",
              "      <td>1.162996e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064988</td>\n",
              "      <td>0.074223</td>\n",
              "      <td>0.101930</td>\n",
              "      <td>1.038387e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.067543</td>\n",
              "      <td>0.009184</td>\n",
              "      <td>0.082285</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>2.093430e-04</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>7.871485e-06</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.004440</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.864490e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.864490e-03</td>\n",
              "      <td>2.864490e-03</td>\n",
              "      <td>0.482500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.482500</td>\n",
              "      <td>0.482500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   src dport count  dst src count  dport count  ...  Idle Max  Idle Min     Label\n",
              "0         0.010101       1.000000     1.000000  ...  0.686508  0.714167  0.101667\n",
              "1         0.010101       0.222222     0.121212  ...  0.000000  0.000000  0.000000\n",
              "2         0.000000       0.010101     0.020202  ...  0.000000  0.000000  0.000000\n",
              "3         0.010101       0.000000     0.010101  ...  0.000000  0.000000  0.000000\n",
              "4         0.010101       0.030303     0.010101  ...  0.000000  0.000000  0.000000\n",
              "5         0.000000       1.000000     1.000000  ...  0.161376  0.267500  0.066799\n",
              "6         0.989899       0.000000     0.989899  ...  0.000000  0.000000  0.000000\n",
              "7         0.000000       0.969697     0.969697  ...  0.000000  0.821667  0.821667\n",
              "8         0.010101       0.939394     0.939394  ...  0.000000  0.000000  0.000000\n",
              "9         0.010101       0.070707     0.010101  ...  0.000000  0.482500  0.482500\n",
              "\n",
              "[10 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzLYPxy-Xhv-"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIE01O4yXhv-"
      },
      "source": [
        "pca = PCA(n_components=None)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd2crc5zXhv-"
      },
      "source": [
        "dfx_pca = pca.fit(dfx)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wPwtI_6RXhv-",
        "outputId": "d82172f4-9a70-4ec6-eb0e-c2cb6ea6ad80"
      },
      "source": [
        "plt.figure(figsize=(24,5))\n",
        "plt.scatter(x=[i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],\n",
        "            y=dfx_pca.explained_variance_ratio_,\n",
        "           s=200, alpha=0.75,c='orange',edgecolor='k')\n",
        "plt.grid(True)\n",
        "plt.title(\"Explained variance ratio of the \\nfitted principal component vector\\n\",fontsize=25)\n",
        "plt.xlabel(\"Principal components\",fontsize=15)\n",
        "plt.xticks([i+1 for i in range(len(dfx_pca.explained_variance_ratio_))],fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.ylabel(\"Explained variance ratio\",fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeVjWmuZXhv_"
      },
      "source": [
        "The above two plots means that the $1^{st}$ principal component explains about 38% of the total variance in the data and the $2^{nd}$ component explians further 20%. Therefore, if we just consider first two components, they together explain 58% of the total variance. Using the first 10 features should give very hight detection rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8xM86XhXhv_"
      },
      "source": [
        "Transform the scaled data set using the fitted PCA object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srySEJvRXhv_"
      },
      "source": [
        "dfx_trans = pca.transform(dfx)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COMFpsuyXhv_"
      },
      "source": [
        "Put it in a data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "rtptL8AMXhwA",
        "scrolled": true,
        "outputId": "3f9b5e00-5cc6-46b5-d984-24700a85b1d2"
      },
      "source": [
        "dfx_trans = pd.DataFrame(data=dfx_trans)\n",
        "dfx_trans.head(10)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.000408</td>\n",
              "      <td>0.456157</td>\n",
              "      <td>-0.193931</td>\n",
              "      <td>0.123428</td>\n",
              "      <td>-0.164004</td>\n",
              "      <td>-0.145284</td>\n",
              "      <td>-0.474488</td>\n",
              "      <td>-0.051236</td>\n",
              "      <td>0.117817</td>\n",
              "      <td>-0.036944</td>\n",
              "      <td>0.124120</td>\n",
              "      <td>0.315620</td>\n",
              "      <td>-0.097011</td>\n",
              "      <td>0.050066</td>\n",
              "      <td>-0.190276</td>\n",
              "      <td>0.108648</td>\n",
              "      <td>0.550164</td>\n",
              "      <td>0.112007</td>\n",
              "      <td>0.483786</td>\n",
              "      <td>0.072881</td>\n",
              "      <td>-0.023484</td>\n",
              "      <td>0.025039</td>\n",
              "      <td>-0.156377</td>\n",
              "      <td>0.011712</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>-0.008902</td>\n",
              "      <td>-0.001733</td>\n",
              "      <td>-0.011331</td>\n",
              "      <td>-0.008608</td>\n",
              "      <td>-0.021437</td>\n",
              "      <td>-0.022180</td>\n",
              "      <td>0.004493</td>\n",
              "      <td>0.002954</td>\n",
              "      <td>0.001991</td>\n",
              "      <td>-0.003361</td>\n",
              "      <td>0.001716</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>-0.002374</td>\n",
              "      <td>0.017730</td>\n",
              "      <td>-0.012738</td>\n",
              "      <td>-0.001900</td>\n",
              "      <td>0.002569</td>\n",
              "      <td>0.005968</td>\n",
              "      <td>-0.005596</td>\n",
              "      <td>-0.006042</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>-0.006021</td>\n",
              "      <td>0.002297</td>\n",
              "      <td>-0.004915</td>\n",
              "      <td>-0.008482</td>\n",
              "      <td>0.002636</td>\n",
              "      <td>0.004319</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>-0.002568</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>-0.000737</td>\n",
              "      <td>0.004008</td>\n",
              "      <td>-0.000079</td>\n",
              "      <td>-0.000031</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>-0.000847</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>-9.287958e-05</td>\n",
              "      <td>-0.001196</td>\n",
              "      <td>-0.000172</td>\n",
              "      <td>-2.666377e-07</td>\n",
              "      <td>1.876301e-06</td>\n",
              "      <td>2.866554e-05</td>\n",
              "      <td>-1.374226e-06</td>\n",
              "      <td>-5.330733e-08</td>\n",
              "      <td>-1.340074e-14</td>\n",
              "      <td>7.924770e-18</td>\n",
              "      <td>-1.821750e-16</td>\n",
              "      <td>7.865599e-17</td>\n",
              "      <td>2.879049e-17</td>\n",
              "      <td>-1.949659e-17</td>\n",
              "      <td>3.082198e-17</td>\n",
              "      <td>-7.641116e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.758438</td>\n",
              "      <td>-0.286833</td>\n",
              "      <td>0.531995</td>\n",
              "      <td>-0.055525</td>\n",
              "      <td>-0.372602</td>\n",
              "      <td>-0.161333</td>\n",
              "      <td>-0.027339</td>\n",
              "      <td>-0.036354</td>\n",
              "      <td>0.034841</td>\n",
              "      <td>0.089133</td>\n",
              "      <td>-0.168739</td>\n",
              "      <td>-0.175130</td>\n",
              "      <td>-0.006920</td>\n",
              "      <td>0.028832</td>\n",
              "      <td>0.009434</td>\n",
              "      <td>-0.250778</td>\n",
              "      <td>-0.014791</td>\n",
              "      <td>0.018521</td>\n",
              "      <td>0.102688</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>-0.045560</td>\n",
              "      <td>0.042128</td>\n",
              "      <td>0.078331</td>\n",
              "      <td>-0.055687</td>\n",
              "      <td>-0.025772</td>\n",
              "      <td>-0.022669</td>\n",
              "      <td>0.003055</td>\n",
              "      <td>-0.012343</td>\n",
              "      <td>0.008025</td>\n",
              "      <td>-0.006138</td>\n",
              "      <td>-0.009338</td>\n",
              "      <td>-0.004617</td>\n",
              "      <td>-0.002291</td>\n",
              "      <td>0.002559</td>\n",
              "      <td>-0.000371</td>\n",
              "      <td>0.003729</td>\n",
              "      <td>-0.004520</td>\n",
              "      <td>0.021719</td>\n",
              "      <td>-0.010093</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>-0.000744</td>\n",
              "      <td>-0.013623</td>\n",
              "      <td>0.005775</td>\n",
              "      <td>0.007258</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.003248</td>\n",
              "      <td>-0.007089</td>\n",
              "      <td>0.003018</td>\n",
              "      <td>-0.000076</td>\n",
              "      <td>-0.007684</td>\n",
              "      <td>-0.011160</td>\n",
              "      <td>0.001841</td>\n",
              "      <td>-0.002550</td>\n",
              "      <td>-0.001739</td>\n",
              "      <td>-0.001598</td>\n",
              "      <td>0.001802</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>1.061504e-04</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>2.535008e-06</td>\n",
              "      <td>3.381653e-06</td>\n",
              "      <td>1.600827e-05</td>\n",
              "      <td>-6.671434e-07</td>\n",
              "      <td>-4.225529e-09</td>\n",
              "      <td>1.336913e-14</td>\n",
              "      <td>1.554691e-17</td>\n",
              "      <td>5.380169e-17</td>\n",
              "      <td>-5.182148e-17</td>\n",
              "      <td>-1.393769e-17</td>\n",
              "      <td>1.937620e-17</td>\n",
              "      <td>8.659651e-17</td>\n",
              "      <td>5.998579e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.705317</td>\n",
              "      <td>0.246329</td>\n",
              "      <td>0.110341</td>\n",
              "      <td>-0.711713</td>\n",
              "      <td>0.381522</td>\n",
              "      <td>0.122896</td>\n",
              "      <td>-0.235481</td>\n",
              "      <td>0.010478</td>\n",
              "      <td>-0.037500</td>\n",
              "      <td>-0.003030</td>\n",
              "      <td>0.105523</td>\n",
              "      <td>0.148764</td>\n",
              "      <td>0.205875</td>\n",
              "      <td>-0.113824</td>\n",
              "      <td>-0.059042</td>\n",
              "      <td>-0.067677</td>\n",
              "      <td>-0.030813</td>\n",
              "      <td>0.036519</td>\n",
              "      <td>0.005807</td>\n",
              "      <td>-0.013207</td>\n",
              "      <td>0.130793</td>\n",
              "      <td>0.013535</td>\n",
              "      <td>-0.027019</td>\n",
              "      <td>-0.046712</td>\n",
              "      <td>-0.007212</td>\n",
              "      <td>0.002803</td>\n",
              "      <td>-0.001043</td>\n",
              "      <td>0.002211</td>\n",
              "      <td>-0.000531</td>\n",
              "      <td>-0.000670</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.000618</td>\n",
              "      <td>0.004156</td>\n",
              "      <td>-0.003454</td>\n",
              "      <td>0.005413</td>\n",
              "      <td>-0.000653</td>\n",
              "      <td>0.003335</td>\n",
              "      <td>0.003948</td>\n",
              "      <td>-0.001197</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>-0.001182</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>-0.002296</td>\n",
              "      <td>0.000748</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>-0.000443</td>\n",
              "      <td>-0.000510</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>-0.000097</td>\n",
              "      <td>-0.000141</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>-0.000170</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>-0.000013</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>-1.493894e-05</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>3.370267e-07</td>\n",
              "      <td>6.703074e-07</td>\n",
              "      <td>-7.556229e-07</td>\n",
              "      <td>-3.937491e-08</td>\n",
              "      <td>8.076255e-10</td>\n",
              "      <td>1.965755e-16</td>\n",
              "      <td>1.244710e-18</td>\n",
              "      <td>1.027711e-16</td>\n",
              "      <td>-4.379671e-17</td>\n",
              "      <td>-1.937365e-18</td>\n",
              "      <td>-5.589524e-18</td>\n",
              "      <td>-5.663658e-17</td>\n",
              "      <td>1.372642e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.097000</td>\n",
              "      <td>1.026849</td>\n",
              "      <td>-0.466014</td>\n",
              "      <td>0.769796</td>\n",
              "      <td>-0.284325</td>\n",
              "      <td>0.443920</td>\n",
              "      <td>0.231653</td>\n",
              "      <td>-0.427328</td>\n",
              "      <td>0.016124</td>\n",
              "      <td>0.081095</td>\n",
              "      <td>0.301759</td>\n",
              "      <td>-0.064301</td>\n",
              "      <td>-0.027400</td>\n",
              "      <td>0.060609</td>\n",
              "      <td>-0.050881</td>\n",
              "      <td>-0.101476</td>\n",
              "      <td>-0.023151</td>\n",
              "      <td>-0.137598</td>\n",
              "      <td>0.056145</td>\n",
              "      <td>-0.035790</td>\n",
              "      <td>-0.008768</td>\n",
              "      <td>-0.045601</td>\n",
              "      <td>-0.001778</td>\n",
              "      <td>0.014292</td>\n",
              "      <td>-0.005594</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>-0.005491</td>\n",
              "      <td>-0.001541</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>-0.002835</td>\n",
              "      <td>-0.001738</td>\n",
              "      <td>-0.031204</td>\n",
              "      <td>-0.005182</td>\n",
              "      <td>0.002629</td>\n",
              "      <td>-0.000094</td>\n",
              "      <td>-0.001397</td>\n",
              "      <td>-0.011319</td>\n",
              "      <td>0.002979</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>-0.001706</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>-0.001235</td>\n",
              "      <td>0.001271</td>\n",
              "      <td>-0.000074</td>\n",
              "      <td>-0.000605</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.001514</td>\n",
              "      <td>0.002105</td>\n",
              "      <td>-0.000290</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>-0.000223</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>-0.000378</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>-0.000031</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>2.275794e-05</td>\n",
              "      <td>-0.000048</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>2.737968e-06</td>\n",
              "      <td>1.159845e-05</td>\n",
              "      <td>3.537904e-06</td>\n",
              "      <td>-1.723659e-06</td>\n",
              "      <td>-1.121113e-08</td>\n",
              "      <td>-8.167227e-16</td>\n",
              "      <td>5.180455e-17</td>\n",
              "      <td>1.022034e-16</td>\n",
              "      <td>-2.658160e-18</td>\n",
              "      <td>3.496821e-17</td>\n",
              "      <td>1.868822e-17</td>\n",
              "      <td>-1.804450e-16</td>\n",
              "      <td>2.060025e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.883575</td>\n",
              "      <td>0.750642</td>\n",
              "      <td>-0.220291</td>\n",
              "      <td>0.089906</td>\n",
              "      <td>-0.137388</td>\n",
              "      <td>-0.354584</td>\n",
              "      <td>-0.244610</td>\n",
              "      <td>-0.044015</td>\n",
              "      <td>0.115940</td>\n",
              "      <td>0.346570</td>\n",
              "      <td>-0.269264</td>\n",
              "      <td>0.251641</td>\n",
              "      <td>-0.017005</td>\n",
              "      <td>-0.096137</td>\n",
              "      <td>0.232606</td>\n",
              "      <td>-0.045314</td>\n",
              "      <td>-0.037130</td>\n",
              "      <td>0.041674</td>\n",
              "      <td>0.029598</td>\n",
              "      <td>-0.079600</td>\n",
              "      <td>-0.071720</td>\n",
              "      <td>0.036267</td>\n",
              "      <td>-0.001167</td>\n",
              "      <td>-0.006967</td>\n",
              "      <td>-0.001204</td>\n",
              "      <td>0.013526</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.013734</td>\n",
              "      <td>-0.003108</td>\n",
              "      <td>-0.003876</td>\n",
              "      <td>0.003145</td>\n",
              "      <td>0.002278</td>\n",
              "      <td>-0.010702</td>\n",
              "      <td>0.001501</td>\n",
              "      <td>-0.003494</td>\n",
              "      <td>-0.001884</td>\n",
              "      <td>0.001071</td>\n",
              "      <td>-0.001006</td>\n",
              "      <td>0.003381</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.001113</td>\n",
              "      <td>-0.002917</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>-0.004871</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>-0.000853</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>-0.000801</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>-0.000792</td>\n",
              "      <td>-0.000382</td>\n",
              "      <td>-0.000894</td>\n",
              "      <td>-0.000219</td>\n",
              "      <td>-0.000372</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>-0.000136</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.000107</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>8.035994e-07</td>\n",
              "      <td>-0.000051</td>\n",
              "      <td>-0.000059</td>\n",
              "      <td>1.109809e-05</td>\n",
              "      <td>4.916593e-06</td>\n",
              "      <td>6.246139e-06</td>\n",
              "      <td>-7.740305e-07</td>\n",
              "      <td>-4.819689e-08</td>\n",
              "      <td>-4.638772e-15</td>\n",
              "      <td>1.376174e-17</td>\n",
              "      <td>1.493844e-16</td>\n",
              "      <td>-2.132667e-17</td>\n",
              "      <td>6.505406e-18</td>\n",
              "      <td>-1.584292e-18</td>\n",
              "      <td>4.212307e-17</td>\n",
              "      <td>8.283071e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.906004</td>\n",
              "      <td>-0.733026</td>\n",
              "      <td>0.148430</td>\n",
              "      <td>-0.075756</td>\n",
              "      <td>-0.194602</td>\n",
              "      <td>-0.402469</td>\n",
              "      <td>0.332488</td>\n",
              "      <td>-0.172710</td>\n",
              "      <td>-0.098643</td>\n",
              "      <td>-0.521091</td>\n",
              "      <td>0.099618</td>\n",
              "      <td>-0.333205</td>\n",
              "      <td>0.180124</td>\n",
              "      <td>-0.021190</td>\n",
              "      <td>-0.430085</td>\n",
              "      <td>-0.042203</td>\n",
              "      <td>0.276950</td>\n",
              "      <td>-0.122133</td>\n",
              "      <td>0.155934</td>\n",
              "      <td>0.023362</td>\n",
              "      <td>-0.097217</td>\n",
              "      <td>-0.070542</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>-0.061340</td>\n",
              "      <td>0.007793</td>\n",
              "      <td>0.019321</td>\n",
              "      <td>0.011040</td>\n",
              "      <td>0.044168</td>\n",
              "      <td>-0.031216</td>\n",
              "      <td>0.027433</td>\n",
              "      <td>-0.014642</td>\n",
              "      <td>0.010023</td>\n",
              "      <td>-0.002723</td>\n",
              "      <td>0.014066</td>\n",
              "      <td>0.006955</td>\n",
              "      <td>-0.031520</td>\n",
              "      <td>0.067692</td>\n",
              "      <td>0.016650</td>\n",
              "      <td>0.061013</td>\n",
              "      <td>0.108993</td>\n",
              "      <td>0.012872</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.006336</td>\n",
              "      <td>-0.015623</td>\n",
              "      <td>-0.022526</td>\n",
              "      <td>0.011228</td>\n",
              "      <td>-0.008603</td>\n",
              "      <td>-0.010924</td>\n",
              "      <td>-0.028336</td>\n",
              "      <td>0.011406</td>\n",
              "      <td>-0.007047</td>\n",
              "      <td>-0.012134</td>\n",
              "      <td>-0.003711</td>\n",
              "      <td>0.007950</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>0.004028</td>\n",
              "      <td>0.004844</td>\n",
              "      <td>-0.000084</td>\n",
              "      <td>-1.875047e-03</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>-0.000702</td>\n",
              "      <td>3.250905e-05</td>\n",
              "      <td>-1.783376e-05</td>\n",
              "      <td>2.876633e-05</td>\n",
              "      <td>3.172480e-06</td>\n",
              "      <td>-4.236382e-07</td>\n",
              "      <td>5.626355e-16</td>\n",
              "      <td>-4.427582e-17</td>\n",
              "      <td>-1.497967e-16</td>\n",
              "      <td>4.237967e-17</td>\n",
              "      <td>-1.404902e-17</td>\n",
              "      <td>-5.862961e-18</td>\n",
              "      <td>4.593071e-17</td>\n",
              "      <td>-1.468250e-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.269360</td>\n",
              "      <td>-1.427422</td>\n",
              "      <td>-0.022287</td>\n",
              "      <td>0.795755</td>\n",
              "      <td>0.676687</td>\n",
              "      <td>-0.004024</td>\n",
              "      <td>-0.302277</td>\n",
              "      <td>0.108517</td>\n",
              "      <td>0.323924</td>\n",
              "      <td>0.103873</td>\n",
              "      <td>-0.092631</td>\n",
              "      <td>-0.045926</td>\n",
              "      <td>0.030301</td>\n",
              "      <td>-0.057931</td>\n",
              "      <td>0.003269</td>\n",
              "      <td>0.019289</td>\n",
              "      <td>0.006769</td>\n",
              "      <td>0.003090</td>\n",
              "      <td>-0.016583</td>\n",
              "      <td>-0.007482</td>\n",
              "      <td>-0.019938</td>\n",
              "      <td>-0.022593</td>\n",
              "      <td>-0.012416</td>\n",
              "      <td>-0.005646</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>0.002342</td>\n",
              "      <td>-0.002875</td>\n",
              "      <td>0.002046</td>\n",
              "      <td>-0.002247</td>\n",
              "      <td>-0.001114</td>\n",
              "      <td>0.002456</td>\n",
              "      <td>-0.000159</td>\n",
              "      <td>0.003029</td>\n",
              "      <td>-0.004901</td>\n",
              "      <td>-0.000856</td>\n",
              "      <td>-0.001014</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>-0.003070</td>\n",
              "      <td>-0.000250</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>-0.000141</td>\n",
              "      <td>-0.001208</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>-0.000216</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>-0.000015</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>-1.215735e-05</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>2.306841e-06</td>\n",
              "      <td>-2.065234e-06</td>\n",
              "      <td>1.548875e-06</td>\n",
              "      <td>1.510208e-07</td>\n",
              "      <td>-2.777240e-09</td>\n",
              "      <td>-4.136765e-16</td>\n",
              "      <td>3.709522e-18</td>\n",
              "      <td>-8.421800e-18</td>\n",
              "      <td>-1.062519e-16</td>\n",
              "      <td>5.384828e-18</td>\n",
              "      <td>-1.407710e-17</td>\n",
              "      <td>-3.709330e-17</td>\n",
              "      <td>1.324261e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.363014</td>\n",
              "      <td>0.698347</td>\n",
              "      <td>0.073176</td>\n",
              "      <td>0.274132</td>\n",
              "      <td>-0.132425</td>\n",
              "      <td>0.045027</td>\n",
              "      <td>-0.668782</td>\n",
              "      <td>-0.069744</td>\n",
              "      <td>-0.125251</td>\n",
              "      <td>0.074095</td>\n",
              "      <td>-0.015165</td>\n",
              "      <td>0.039565</td>\n",
              "      <td>-0.038962</td>\n",
              "      <td>0.042716</td>\n",
              "      <td>-0.025120</td>\n",
              "      <td>-0.010657</td>\n",
              "      <td>-0.050406</td>\n",
              "      <td>-0.005320</td>\n",
              "      <td>-0.078858</td>\n",
              "      <td>0.013993</td>\n",
              "      <td>0.036207</td>\n",
              "      <td>-0.046769</td>\n",
              "      <td>0.103983</td>\n",
              "      <td>-0.009584</td>\n",
              "      <td>-0.019586</td>\n",
              "      <td>0.059521</td>\n",
              "      <td>0.012007</td>\n",
              "      <td>-0.016557</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>-0.022936</td>\n",
              "      <td>0.032721</td>\n",
              "      <td>-0.010135</td>\n",
              "      <td>0.002399</td>\n",
              "      <td>-0.008567</td>\n",
              "      <td>0.013699</td>\n",
              "      <td>0.005632</td>\n",
              "      <td>-0.016115</td>\n",
              "      <td>-0.006741</td>\n",
              "      <td>0.007939</td>\n",
              "      <td>0.011853</td>\n",
              "      <td>0.001808</td>\n",
              "      <td>0.007065</td>\n",
              "      <td>-0.003584</td>\n",
              "      <td>-0.001911</td>\n",
              "      <td>-0.002094</td>\n",
              "      <td>0.000825</td>\n",
              "      <td>-0.003243</td>\n",
              "      <td>-0.001390</td>\n",
              "      <td>0.006592</td>\n",
              "      <td>-0.000149</td>\n",
              "      <td>-0.000650</td>\n",
              "      <td>-0.001007</td>\n",
              "      <td>-0.001907</td>\n",
              "      <td>0.000397</td>\n",
              "      <td>-0.000910</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>-0.002648</td>\n",
              "      <td>-0.000447</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>3.149584e-04</td>\n",
              "      <td>-0.000694</td>\n",
              "      <td>-0.000039</td>\n",
              "      <td>-1.643623e-05</td>\n",
              "      <td>1.565507e-06</td>\n",
              "      <td>4.406435e-06</td>\n",
              "      <td>1.740552e-07</td>\n",
              "      <td>-1.592051e-08</td>\n",
              "      <td>4.952606e-15</td>\n",
              "      <td>-3.806585e-17</td>\n",
              "      <td>-9.933668e-17</td>\n",
              "      <td>4.772081e-17</td>\n",
              "      <td>1.358903e-17</td>\n",
              "      <td>-2.476525e-17</td>\n",
              "      <td>1.911723e-17</td>\n",
              "      <td>-4.198538e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.485298</td>\n",
              "      <td>-0.452846</td>\n",
              "      <td>-0.516268</td>\n",
              "      <td>-0.611352</td>\n",
              "      <td>0.314698</td>\n",
              "      <td>0.103022</td>\n",
              "      <td>0.835726</td>\n",
              "      <td>0.060756</td>\n",
              "      <td>0.015481</td>\n",
              "      <td>0.390987</td>\n",
              "      <td>-0.301307</td>\n",
              "      <td>-0.059095</td>\n",
              "      <td>-0.064001</td>\n",
              "      <td>-0.063584</td>\n",
              "      <td>-0.354922</td>\n",
              "      <td>-0.083940</td>\n",
              "      <td>0.293437</td>\n",
              "      <td>-0.151907</td>\n",
              "      <td>-0.135756</td>\n",
              "      <td>-0.139980</td>\n",
              "      <td>0.019655</td>\n",
              "      <td>0.035230</td>\n",
              "      <td>0.059897</td>\n",
              "      <td>-0.009439</td>\n",
              "      <td>-0.064176</td>\n",
              "      <td>-0.057387</td>\n",
              "      <td>-0.006318</td>\n",
              "      <td>0.013059</td>\n",
              "      <td>0.002570</td>\n",
              "      <td>0.004373</td>\n",
              "      <td>-0.017866</td>\n",
              "      <td>-0.002858</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>0.006319</td>\n",
              "      <td>-0.004676</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.008414</td>\n",
              "      <td>0.020366</td>\n",
              "      <td>-0.008274</td>\n",
              "      <td>-0.018523</td>\n",
              "      <td>-0.002926</td>\n",
              "      <td>-0.000737</td>\n",
              "      <td>0.006030</td>\n",
              "      <td>0.009198</td>\n",
              "      <td>0.010084</td>\n",
              "      <td>-0.002369</td>\n",
              "      <td>-0.002307</td>\n",
              "      <td>-0.002068</td>\n",
              "      <td>-0.004368</td>\n",
              "      <td>-0.001572</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>-0.000562</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>-0.000708</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>-0.000524</td>\n",
              "      <td>-0.000493</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>-0.000018</td>\n",
              "      <td>-0.000991</td>\n",
              "      <td>-0.000115</td>\n",
              "      <td>-0.000033</td>\n",
              "      <td>3.549956e-04</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>-0.000057</td>\n",
              "      <td>-4.356167e-06</td>\n",
              "      <td>1.234088e-06</td>\n",
              "      <td>1.990480e-06</td>\n",
              "      <td>-2.269772e-07</td>\n",
              "      <td>-7.228749e-09</td>\n",
              "      <td>5.796567e-15</td>\n",
              "      <td>-2.502870e-17</td>\n",
              "      <td>-1.069897e-16</td>\n",
              "      <td>4.620496e-17</td>\n",
              "      <td>-2.095386e-17</td>\n",
              "      <td>-7.428734e-18</td>\n",
              "      <td>1.432901e-16</td>\n",
              "      <td>-7.421773e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.053763</td>\n",
              "      <td>0.431512</td>\n",
              "      <td>1.470356</td>\n",
              "      <td>0.338930</td>\n",
              "      <td>-0.030375</td>\n",
              "      <td>-0.276931</td>\n",
              "      <td>-0.049404</td>\n",
              "      <td>-0.061485</td>\n",
              "      <td>0.025761</td>\n",
              "      <td>0.034717</td>\n",
              "      <td>-0.065724</td>\n",
              "      <td>-0.140164</td>\n",
              "      <td>0.192608</td>\n",
              "      <td>-0.007704</td>\n",
              "      <td>0.167306</td>\n",
              "      <td>-0.288536</td>\n",
              "      <td>-0.261066</td>\n",
              "      <td>0.047316</td>\n",
              "      <td>0.233377</td>\n",
              "      <td>0.059993</td>\n",
              "      <td>-0.116945</td>\n",
              "      <td>-0.079269</td>\n",
              "      <td>0.042427</td>\n",
              "      <td>-0.004294</td>\n",
              "      <td>0.028465</td>\n",
              "      <td>-0.109752</td>\n",
              "      <td>0.022337</td>\n",
              "      <td>0.013891</td>\n",
              "      <td>-0.047396</td>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.037853</td>\n",
              "      <td>0.058525</td>\n",
              "      <td>-0.011176</td>\n",
              "      <td>-0.013955</td>\n",
              "      <td>0.017766</td>\n",
              "      <td>0.006997</td>\n",
              "      <td>-0.035563</td>\n",
              "      <td>0.016486</td>\n",
              "      <td>-0.013138</td>\n",
              "      <td>0.014975</td>\n",
              "      <td>0.001579</td>\n",
              "      <td>-0.009302</td>\n",
              "      <td>-0.000441</td>\n",
              "      <td>-0.007538</td>\n",
              "      <td>-0.004675</td>\n",
              "      <td>0.003696</td>\n",
              "      <td>0.003128</td>\n",
              "      <td>0.000670</td>\n",
              "      <td>-0.001083</td>\n",
              "      <td>-0.004846</td>\n",
              "      <td>0.004904</td>\n",
              "      <td>-0.003757</td>\n",
              "      <td>0.005552</td>\n",
              "      <td>-0.000715</td>\n",
              "      <td>-0.000893</td>\n",
              "      <td>0.000799</td>\n",
              "      <td>-0.001270</td>\n",
              "      <td>-0.000160</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>-0.002537</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>-1.302672e-03</td>\n",
              "      <td>0.001071</td>\n",
              "      <td>-0.000046</td>\n",
              "      <td>2.874768e-05</td>\n",
              "      <td>1.920554e-06</td>\n",
              "      <td>-6.106031e-06</td>\n",
              "      <td>6.118902e-07</td>\n",
              "      <td>-7.607018e-08</td>\n",
              "      <td>-1.820610e-14</td>\n",
              "      <td>-9.096018e-18</td>\n",
              "      <td>1.008635e-16</td>\n",
              "      <td>-6.912240e-17</td>\n",
              "      <td>-1.655960e-17</td>\n",
              "      <td>9.001405e-18</td>\n",
              "      <td>1.838770e-16</td>\n",
              "      <td>-6.226768e-16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...            75            76            77\n",
              "0  2.000408  0.456157 -0.193931  ... -1.949659e-17  3.082198e-17 -7.641116e-16\n",
              "1 -0.758438 -0.286833  0.531995  ...  1.937620e-17  8.659651e-17  5.998579e-16\n",
              "2 -0.705317  0.246329  0.110341  ... -5.589524e-18 -5.663658e-17  1.372642e-16\n",
              "3 -1.097000  1.026849 -0.466014  ...  1.868822e-17 -1.804450e-16  2.060025e-16\n",
              "4 -0.883575  0.750642 -0.220291  ... -1.584292e-18  4.212307e-17  8.283071e-17\n",
              "5  0.906004 -0.733026  0.148430  ... -5.862961e-18  4.593071e-17 -1.468250e-15\n",
              "6 -0.269360 -1.427422 -0.022287  ... -1.407710e-17 -3.709330e-17  1.324261e-16\n",
              "7  2.363014  0.698347  0.073176  ... -2.476525e-17  1.911723e-17 -4.198538e-17\n",
              "8  0.485298 -0.452846 -0.516268  ... -7.428734e-18  1.432901e-16 -7.421773e-17\n",
              "9 -0.053763  0.431512  1.470356  ...  9.001405e-18  1.838770e-16 -6.226768e-16\n",
              "\n",
              "[10 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIDYI_dXhwA"
      },
      "source": [
        "## Training and Making Predictions\n",
        "\n",
        "In this case we'll use random forest classification for making the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swXJ6_ydXhwA"
      },
      "source": [
        "pca = PCA(n_components=12)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Yt7PnkXhwA"
      },
      "source": [
        "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "classifier.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test_pca)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7X48ybRXhwA",
        "outputId": "b5b85b59-ff9c-4274-dc9d-39bbf53df1b4"
      },
      "source": [
        "print('Accuracy:%f' %accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:%f\" %metrics.average_precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall:%f\" %metrics.recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1-score:%f\" %metrics.f1_score(y_test, y_pred,average='weighted'))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.724893\n",
            "Precision:0.734083\n",
            "Recall:0.724893\n",
            "F1-score:0.755240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfibn9gAXhwB",
        "scrolled": true,
        "outputId": "be99767f-51c7-4306-cdb3-c104d7f00ce7"
      },
      "source": [
        "#The confusion matrix takes a vector of labels (not the one-hot encoding). \n",
        "\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[138946      0      0      0      0      0      0      0      0      0\n",
            "     189      0      0      0      0]\n",
            " [   489      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [ 31990      0      0      0      0      0      0      0      0      0\n",
            "      16      0      0      0      0]\n",
            " [  2573      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [ 57531      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [  1373      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]\n",
            " [  1444      0      0      0      0      0      0      0      0      0\n",
            "       5      0      0      0      0]\n",
            " [  1983      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     2      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     9      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [  1667      0      0      0      0      0      0      0      0      0\n",
            "   38034      0      0      0      0]\n",
            " [  1474      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   376      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     5      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   162      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxRNywIXXhwC"
      },
      "source": [
        "Get the attacks' names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9AlIiTKXhwC"
      },
      "source": [
        "labels_d = make_value2index(df_test['Label'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1vqAhE1XhwC",
        "outputId": "2d5dcf46-f63e-43cd-ebe9-1c37f1b2ca06"
      },
      "source": [
        "print(labels_d)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'BENIGN': 105018, 'Bot': 105298, 'DDoS': 124569, 'DoS GoldenEye': 126111, 'DoS Hulk': 160658, 'DoS Slowhttptest': 161486, 'DoS slowloris': 162320, 'FTP-Patator': 163498, 'Heartbleed': 163500, 'Infiltration': 163501, 'PortScan': 187347, 'SSH-Patator': 188173, 'Web Attack � Brute Force': 188382, 'Web Attack � Sql Injection': 188389, 'Web Attack � XSS': 188482}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO4EmilxXhwD",
        "outputId": "afce2f53-02f1-46ff-b344-fd79a2c21dbf"
      },
      "source": [
        "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1), target_names=labels_d))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.69      0.99      0.81    105019\n",
            "                       Bot       0.00      0.00      0.00       280\n",
            "                      DDoS       0.00      0.00      0.00     19271\n",
            "             DoS GoldenEye       0.00      0.00      0.00      1542\n",
            "                  DoS Hulk       0.98      0.43      0.59     34547\n",
            "          DoS Slowhttptest       0.00      0.00      0.00       828\n",
            "             DoS slowloris       0.00      0.00      0.00       834\n",
            "               FTP-Patator       0.00      0.00      0.00      1178\n",
            "                Heartbleed       0.00      0.00      0.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.96      0.92      0.94     23846\n",
            "               SSH-Patator       0.00      0.00      0.00       826\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.00      0.00      0.00        93\n",
            "\n",
            "                  accuracy                           0.75    188483\n",
            "                 macro avg       0.18      0.16      0.16    188483\n",
            "              weighted avg       0.69      0.75      0.68    188483\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtGAy_3gY2n"
      },
      "source": [
        "# Model : Naive Bayes model (GaussianNB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxNz3yQch35o"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izw8QWLIgcTx",
        "outputId": "7ede9a07-ed9f-429e-b6f2-704eb5dd439c"
      },
      "source": [
        "model_gaussian = GaussianNB()\r\n",
        "model_gaussian.fit(X_train, y_train_ada)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c15lYNIdgs9V"
      },
      "source": [
        "# make predictions\r\n",
        "y_pred = model_gaussian.predict(X_test)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ6JwTSDg6iE",
        "outputId": "9663a9ba-74b7-495f-b8f9-4c0010caf1cb"
      },
      "source": [
        "display_metrics(y_test_ada, y_pred, labels_d)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.56\n",
            "\n",
            "Micro Precision: 0.56\n",
            "Micro Recall: 0.56\n",
            "Micro F1-score: 0.56\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.04\n",
            "Macro Recall: 0.07\n",
            "Macro F1-score: 0.05\n",
            "\n",
            "Weighted Precision: 0.31\n",
            "Weighted Recall: 0.56\n",
            "Weighted F1-score: 0.40\n",
            "\n",
            "Classification Report\n",
            "\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.56      1.00      0.72    105019\n",
            "                       Bot       0.00      0.00      0.00       280\n",
            "                      DDoS       0.00      0.00      0.00     19271\n",
            "             DoS GoldenEye       0.00      0.00      0.00      1542\n",
            "                  DoS Hulk       0.00      0.00      0.00     34547\n",
            "          DoS Slowhttptest       0.00      0.00      0.00       828\n",
            "             DoS slowloris       0.00      0.00      0.00       834\n",
            "               FTP-Patator       0.00      0.00      0.00      1178\n",
            "                Heartbleed       0.00      0.00      0.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.00      0.00      0.00     23846\n",
            "               SSH-Patator       0.00      0.00      0.00       826\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.00      0.00      0.00        93\n",
            "\n",
            "                  accuracy                           0.56    188483\n",
            "                 macro avg       0.04      0.07      0.05    188483\n",
            "              weighted avg       0.31      0.56      0.40    188483\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXea3TMVXhwD"
      },
      "source": [
        "# Model 3: Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOUaeJwiXhwD"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRMY85VyXhwE"
      },
      "source": [
        "model_dec = DecisionTreeClassifier()\n",
        "model_dec.fit(X_train, y_train_ada)\n",
        "y_pred = model_dec.predict(X_test)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFQ6iCJjXhwE",
        "outputId": "550f88b2-dc9a-4234-cbb6-4b20eca16881"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\n",
        " \n",
        "print('\\nClassification Report\\n')\n",
        "\n",
        "print(classification_report(y_test_ada , y_pred, target_names=labels_d))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.99\n",
            "\n",
            "Micro Precision: 0.99\n",
            "Micro Recall: 0.99\n",
            "Micro F1-score: 0.99\n",
            "\n",
            "Macro Precision: 0.85\n",
            "Macro Recall: 0.92\n",
            "Macro F1-score: 0.88\n",
            "\n",
            "Weighted Precision: 0.99\n",
            "Weighted Recall: 0.99\n",
            "Weighted F1-score: 0.99\n",
            "\n",
            "Classification Report\n",
            "\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.99      0.99      0.99    105019\n",
            "                       Bot       0.69      0.96      0.80       280\n",
            "                      DDoS       1.00      0.97      0.99     19271\n",
            "             DoS GoldenEye       0.71      1.00      0.83      1542\n",
            "                  DoS Hulk       1.00      0.98      0.99     34547\n",
            "          DoS Slowhttptest       0.95      0.96      0.95       828\n",
            "             DoS slowloris       0.87      0.88      0.88       834\n",
            "               FTP-Patator       1.00      1.00      1.00      1178\n",
            "                Heartbleed       0.67      1.00      0.80         2\n",
            "              Infiltration       0.50      1.00      0.67         1\n",
            "                  PortScan       0.97      0.99      0.98     23846\n",
            "               SSH-Patator       1.00      1.00      1.00       826\n",
            "  Web Attack � Brute Force       0.90      0.88      0.89       209\n",
            "Web Attack � Sql Injection       0.75      0.43      0.55         7\n",
            "          Web Attack � XSS       0.83      0.83      0.83        93\n",
            "\n",
            "                  accuracy                           0.99    188483\n",
            "                 macro avg       0.85      0.92      0.88    188483\n",
            "              weighted avg       0.99      0.99      0.99    188483\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABYQjuBCXhwE"
      },
      "source": [
        "# Model 4: Random Foresty with DecisionTree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI6QGw9LXhwF"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "clf.fit(X_train,y_train)\n",
        "    \n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFh6H0d3lNmz",
        "outputId": "47a3e523-9564-4bed-f2fa-d212c07d4abc"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, np.argmax(y_pred, axis = 1))))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, np.argmax(y_pred, axis = 1), average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')\r\n",
        "\r\n",
        "print(classification_report(y_test_ada ,np.argmax(y_pred, axis = 1), target_names=labels_d))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.92\n",
            "\n",
            "Micro Precision: 0.92\n",
            "Micro Recall: 0.92\n",
            "Micro F1-score: 0.92\n",
            "\n",
            "Macro Precision: 0.91\n",
            "Macro Recall: 0.75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.80\n",
            "\n",
            "Weighted Precision: 0.93\n",
            "Weighted Recall: 0.92\n",
            "Weighted F1-score: 0.91\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.88      1.00      0.93    105019\n",
            "                       Bot       0.94      0.18      0.31       280\n",
            "                      DDoS       1.00      0.91      0.95     19271\n",
            "             DoS GoldenEye       1.00      0.92      0.96      1542\n",
            "                  DoS Hulk       1.00      1.00      1.00     34547\n",
            "          DoS Slowhttptest       1.00      0.99      0.99       828\n",
            "             DoS slowloris       1.00      0.90      0.95       834\n",
            "               FTP-Patator       1.00      0.53      0.69      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       1.00      1.00      1.00         1\n",
            "                  PortScan       1.00      0.51      0.68     23846\n",
            "               SSH-Patator       1.00      0.99      1.00       826\n",
            "  Web Attack � Brute Force       0.91      0.75      0.82       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.87      0.56      0.68        93\n",
            "\n",
            "                  accuracy                           0.92    188483\n",
            "                 macro avg       0.91      0.75      0.80    188483\n",
            "              weighted avg       0.93      0.92      0.91    188483\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xi2INVUk9p9"
      },
      "source": [
        "# Model 5: Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_AoQZpOnxuD"
      },
      "source": [
        "from sklearn import linear_model"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqJDnChnmxJ"
      },
      "source": [
        "attack_classifier = linear_model.LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=10000)\r\n",
        "attack_classifier.fit(X_train, y_train_ada)\r\n",
        "\r\n",
        "y_pred = attack_classifier.predict(X_test)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3x76HdDorw7",
        "outputId": "7d88966a-be59-4e6a-8e36-42de212c9ab0"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')\r\n",
        "\r\n",
        "print(classification_report(y_test_ada , y_pred, target_names=labels_d))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.91\n",
            "\n",
            "Micro Precision: 0.91\n",
            "Micro Recall: 0.91\n",
            "Micro F1-score: 0.91\n",
            "\n",
            "Macro Precision: 0.53\n",
            "Macro Recall: 0.90\n",
            "Macro F1-score: 0.58\n",
            "\n",
            "Weighted Precision: 0.96\n",
            "Weighted Recall: 0.91\n",
            "Weighted F1-score: 0.93\n",
            "\n",
            "Classification Report\n",
            "\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       1.00      0.87      0.93    105019\n",
            "                       Bot       0.04      0.96      0.08       280\n",
            "                      DDoS       0.87      0.98      0.92     19271\n",
            "             DoS GoldenEye       0.83      0.97      0.89      1542\n",
            "                  DoS Hulk       0.99      0.92      0.95     34547\n",
            "          DoS Slowhttptest       0.96      0.99      0.97       828\n",
            "             DoS slowloris       0.65      0.96      0.78       834\n",
            "               FTP-Patator       0.85      0.99      0.91      1178\n",
            "                Heartbleed       0.33      1.00      0.50         2\n",
            "              Infiltration       0.00      1.00      0.00         1\n",
            "                  PortScan       0.91      1.00      0.95     23846\n",
            "               SSH-Patator       0.36      0.99      0.53       826\n",
            "  Web Attack � Brute Force       0.12      0.55      0.19       209\n",
            "Web Attack � Sql Injection       0.01      0.71      0.01         7\n",
            "          Web Attack � XSS       0.07      0.68      0.13        93\n",
            "\n",
            "                  accuracy                           0.91    188483\n",
            "                 macro avg       0.53      0.90      0.58    188483\n",
            "              weighted avg       0.96      0.91      0.93    188483\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXJKq1zXhwF"
      },
      "source": [
        "# Model 6: AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqqrLskIXhwF"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT8egNOEXhwG"
      },
      "source": [
        "model_ada = AdaBoostClassifier(n_estimators=100)\n",
        "model_ada.fit(X_train, y_train_ada)\n",
        "\n",
        "# make predictions\n",
        "expected = y_test_ada\n",
        "predicted = model_ada.predict(X_test)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJCcrCS4XhwG",
        "outputId": "ef24334c-ebd0-4169-b7a1-b8d57c5d80fe"
      },
      "source": [
        "y_pred = predicted\n",
        "\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\n",
        "\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\n",
        "\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\n",
        "\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\n",
        " \n",
        "print('\\nClassification Report\\n')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.70\n",
            "\n",
            "Micro Precision: 0.70\n",
            "Micro Recall: 0.70\n",
            "Micro F1-score: 0.70\n",
            "\n",
            "Macro Precision: 0.15\n",
            "Macro Recall: 0.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.11\n",
            "\n",
            "Weighted Precision: 0.71\n",
            "Weighted Recall: 0.70\n",
            "Weighted F1-score: 0.63\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_lvcZVXXhwG",
        "scrolled": true,
        "outputId": "14e3b3e6-71e7-41eb-8665-feaa44edec25"
      },
      "source": [
        "print(classification_report(y_test_ada, predicted, target_names=labels_d))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.93      0.94      0.94    105019\n",
            "                       Bot       0.00      0.00      0.00       280\n",
            "                      DDoS       0.00      0.00      0.00     19271\n",
            "             DoS GoldenEye       0.00      0.00      0.00      1542\n",
            "                  DoS Hulk       0.41      0.97      0.57     34547\n",
            "          DoS Slowhttptest       0.00      0.00      0.00       828\n",
            "             DoS slowloris       0.00      0.00      0.00       834\n",
            "               FTP-Patator       0.00      0.00      0.00      1178\n",
            "                Heartbleed       0.03      1.00      0.05         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.92      0.01      0.02     23846\n",
            "               SSH-Patator       0.00      0.00      0.00       826\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.00      0.00      0.00        93\n",
            "\n",
            "                  accuracy                           0.70    188483\n",
            "                 macro avg       0.15      0.19      0.11    188483\n",
            "              weighted avg       0.71      0.70      0.63    188483\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn7bQX_Gvusc"
      },
      "source": [
        "# Model 6: XGBClassifier (very slow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ttK187BvuLn"
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "53QHIzuAv5z9",
        "outputId": "0e392ce3-2300-4f4f-a1e0-44bdc7723ac0"
      },
      "source": [
        "xgboostc = XGBClassifier(learning_rate = 0.1, max_depth = 5,n_estimators = 1165, subsample=0.8,colsample_bytree=0.8,seed=27)\r\n",
        "xgboostc.fit(X_train,y_train)\r\n",
        "    \r\n",
        "y_pred = xgboostc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-07eedb02b743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgboostc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1165\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgboostc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboostc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz8IN3Jfwav0"
      },
      "source": [
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_ada, y_pred)))\r\n",
        "\r\n",
        "print('Micro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='micro')))\r\n",
        "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='micro')))\r\n",
        "\r\n",
        "print('Macro Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='macro')))\r\n",
        "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_ada, y_pred, average='macro')))\r\n",
        "\r\n",
        "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_ada, y_pred, average='weighted')))\r\n",
        "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_ada, y_pred, average='weighted')))\r\n",
        " \r\n",
        "print('\\nClassification Report\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KznBGxa2yiLk"
      },
      "source": [
        "# Model 7: Voting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfa2lmvytnh"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIYUdP7iyuRG",
        "outputId": "a621c6ae-133c-459c-c365-77064a1b89bd"
      },
      "source": [
        "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=35, criterion=\"entropy\")\r\n",
        "ada = AdaBoostClassifier(n_estimators=75, learning_rate=1.5)\r\n",
        "etc = ExtraTreesClassifier(n_jobs=-1, criterion=\"entropy\", n_estimators=5)\r\n",
        "eclf = VotingClassifier(estimators=[('ada', ada), ('rfc', rfc), ('etc', etc)], voting='soft', weights=[2, 1, 3],n_jobs=1)\r\n",
        "eclf.fit(X_train,y_train_ada)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('ada',\n",
              "                              AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                                                 base_estimator=None,\n",
              "                                                 learning_rate=1.5,\n",
              "                                                 n_estimators=75,\n",
              "                                                 random_state=None)),\n",
              "                             ('rfc',\n",
              "                              RandomForestClassifier(bootstrap=True,\n",
              "                                                     ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     criterion='entropy',\n",
              "                                                     max_depth=None,\n",
              "                                                     max_features='auto',\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     max_samples=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_im...\n",
              "                                                   criterion='entropy',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   max_samples=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=5, n_jobs=-1,\n",
              "                                                   oob_score=False,\n",
              "                                                   random_state=None, verbose=0,\n",
              "                                                   warm_start=False))],\n",
              "                 flatten_transform=True, n_jobs=1, voting='soft',\n",
              "                 weights=[2, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rig3c_bqzpim"
      },
      "source": [
        "y_pred = eclf.predict(X_test)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuI-C6lZzqeV",
        "outputId": "d385a06a-b6b1-4a2a-aa9f-a74fca9a8276"
      },
      "source": [
        "display_metrics(y_test_ada, y_pred,labels_d)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 1.00\n",
            "\n",
            "Micro Precision: 1.00\n",
            "Micro Recall: 1.00\n",
            "Micro F1-score: 1.00\n",
            "\n",
            "Macro Precision: 0.87\n",
            "Macro Recall: 0.76\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.80\n",
            "\n",
            "Weighted Precision: 1.00\n",
            "Weighted Recall: 1.00\n",
            "Weighted F1-score: 1.00\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.99      1.00      1.00    105019\n",
            "                       Bot       0.97      0.34      0.51       280\n",
            "                      DDoS       1.00      1.00      1.00     19271\n",
            "             DoS GoldenEye       1.00      1.00      1.00      1542\n",
            "                  DoS Hulk       1.00      1.00      1.00     34547\n",
            "          DoS Slowhttptest       0.99      0.99      0.99       828\n",
            "             DoS slowloris       1.00      0.98      0.99       834\n",
            "               FTP-Patator       1.00      0.88      0.94      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       1.00      0.99      0.99     23846\n",
            "               SSH-Patator       1.00      0.97      0.99       826\n",
            "  Web Attack � Brute Force       0.86      0.59      0.70       209\n",
            "Web Attack � Sql Injection       0.50      0.14      0.22         7\n",
            "          Web Attack � XSS       0.78      0.54      0.64        93\n",
            "\n",
            "                  accuracy                           1.00    188483\n",
            "                 macro avg       0.87      0.76      0.80    188483\n",
            "              weighted avg       1.00      1.00      1.00    188483\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAuDnGKYXhwH"
      },
      "source": [
        "# Model 8: KNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7fba3ZuXhwH"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgx8g7vuXhwH"
      },
      "source": [
        "features_order = ['dst sport count', 'src dport count', 'dst src count', 'dport count', 'sport count', 'dst host count','src host count', 'Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min']"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyhoOfFgXhwH"
      },
      "source": [
        "features=['dst sport count', 'src dport count', 'dst src count', 'dport count', 'sport count', 'dst host count','src host count', \"Fwd Packet Length Max\",\"Flow IAT Std\",\"Fwd Packet Length Std\" ,\"Fwd IAT Total\",'Flow Packets/s', \"Fwd Packet Length Mean\",  \"Flow Bytes/s\",  \"Flow IAT Mean\", \"Bwd Packet Length Mean\",  \"Flow IAT Max\", \"Bwd Packet Length Std\", ]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "GlhN1BDdXhwI",
        "outputId": "00e22cb8-fa9e-41ac-963a-bc064db727c8"
      },
      "source": [
        "df_knn_train = pd.DataFrame(X_train, columns = features_order)\n",
        "df_knn_train.head()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.938079</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>8.178046e-01</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>1.813646e-05</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.336096</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530855</td>\n",
              "      <td>0.362854</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>6.815038e-02</td>\n",
              "      <td>0.290094</td>\n",
              "      <td>7.141667e-01</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>8.158333e-01</td>\n",
              "      <td>1.166667e-01</td>\n",
              "      <td>0.381123</td>\n",
              "      <td>7.141667e-01</td>\n",
              "      <td>1.083333e-07</td>\n",
              "      <td>2.384917e-03</td>\n",
              "      <td>5.962292e-04</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>2.317825e-03</td>\n",
              "      <td>1.508333e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>2.717302e-08</td>\n",
              "      <td>2.547471e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.285298</td>\n",
              "      <td>0.375124</td>\n",
              "      <td>0.140638</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.268839</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.530855</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>1.813646e-05</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.00351</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.4075</td>\n",
              "      <td>0.686508</td>\n",
              "      <td>0.714167</td>\n",
              "      <td>0.101667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.323232</td>\n",
              "      <td>0.757839</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.015983e-03</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>5.643706e-07</td>\n",
              "      <td>0.027438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038557</td>\n",
              "      <td>0.058490</td>\n",
              "      <td>0.020085</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020649</td>\n",
              "      <td>0.025689</td>\n",
              "      <td>0.005765</td>\n",
              "      <td>0.400011</td>\n",
              "      <td>1.693375e-04</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>5.154583e-04</td>\n",
              "      <td>1.166667e-07</td>\n",
              "      <td>7.597250e-04</td>\n",
              "      <td>3.798625e-04</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>5.154500e-04</td>\n",
              "      <td>2.443750e-04</td>\n",
              "      <td>7.724000e-04</td>\n",
              "      <td>2.574667e-04</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>7.629667e-04</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>8.202302e-06</td>\n",
              "      <td>1.640460e-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027438</td>\n",
              "      <td>0.044863</td>\n",
              "      <td>0.053490</td>\n",
              "      <td>0.002860</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.044863</td>\n",
              "      <td>0.038557</td>\n",
              "      <td>0.020649</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>5.643706e-07</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.07724</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.575758</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.585859</td>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.909255</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.564500e-04</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>4.095986e-07</td>\n",
              "      <td>0.001491</td>\n",
              "      <td>0.017918</td>\n",
              "      <td>0.006230</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007539</td>\n",
              "      <td>0.066062</td>\n",
              "      <td>0.029972</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005766</td>\n",
              "      <td>0.400026</td>\n",
              "      <td>8.548889e-05</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>2.563917e-04</td>\n",
              "      <td>1.333333e-07</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>1.250000e-07</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>3.333333e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>2.166401e-05</td>\n",
              "      <td>3.249602e-05</td>\n",
              "      <td>0.025552</td>\n",
              "      <td>0.005278</td>\n",
              "      <td>0.025548</td>\n",
              "      <td>0.010881</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.027943</td>\n",
              "      <td>0.006230</td>\n",
              "      <td>0.029972</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>4.095986e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.776971</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>9.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.403390</td>\n",
              "      <td>9.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.916667e-07</td>\n",
              "      <td>1.091667e-06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.999999e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>2.824859e-03</td>\n",
              "      <td>4.237288e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.013824</td>\n",
              "      <td>0.00351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.854337</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.408511</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>4.999999e-07</td>\n",
              "      <td>3.916667e-07</td>\n",
              "      <td>3.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.916667e-07</td>\n",
              "      <td>4.916666e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.995718</td>\n",
              "      <td>0.994635</td>\n",
              "      <td>1.418440e-02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003967</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  ...  Idle Max  Idle Min\n",
              "0         0.010101         1.000000  ...  0.714167  0.101667\n",
              "1         0.010101         0.222222  ...  0.000000  0.000000\n",
              "2         0.000000         0.010101  ...  0.000000  0.000000\n",
              "3         0.010101         0.000000  ...  0.000000  0.000000\n",
              "4         0.010101         0.030303  ...  0.000000  0.000000\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eda-FM60XhwI"
      },
      "source": [
        "df_knn_test = pd.DataFrame(X_test, columns = features_order)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "iv6nljzO5_Wd",
        "outputId": "11c42bd0-3009-4658-8c9a-9cedef69525e"
      },
      "source": [
        "df_knn_test.head()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.664337</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>7.168427e-01</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>1.849282e-05</td>\n",
              "      <td>0.013417</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.014166</td>\n",
              "      <td>0.023453</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.492073</td>\n",
              "      <td>0.324761</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>6.627202e-02</td>\n",
              "      <td>0.305425</td>\n",
              "      <td>7.158333e-01</td>\n",
              "      <td>1.271186e-07</td>\n",
              "      <td>0.715833</td>\n",
              "      <td>0.143333</td>\n",
              "      <td>0.460432</td>\n",
              "      <td>0.715833</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.001143</td>\n",
              "      <td>3.833333e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.981499</td>\n",
              "      <td>0.979061</td>\n",
              "      <td>2.325012e-08</td>\n",
              "      <td>3.487518e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.233360</td>\n",
              "      <td>0.432854</td>\n",
              "      <td>0.362787</td>\n",
              "      <td>1.315395e-01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.421723</td>\n",
              "      <td>0.014166</td>\n",
              "      <td>0.492073</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>1.849163e-05</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.715833</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.715833</td>\n",
              "      <td>0.715833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.629271</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>6.987776e-01</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>1.849282e-05</td>\n",
              "      <td>0.013175</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017305</td>\n",
              "      <td>0.025003</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.492073</td>\n",
              "      <td>0.352229</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>7.106208e-02</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>6.975000e-01</td>\n",
              "      <td>1.694915e-07</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>0.174167</td>\n",
              "      <td>0.501199</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>5.000000e-08</td>\n",
              "      <td>0.699167</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.452215</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>1.750000e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.981499</td>\n",
              "      <td>0.979061</td>\n",
              "      <td>1.987600e-08</td>\n",
              "      <td>3.577679e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.233360</td>\n",
              "      <td>0.469397</td>\n",
              "      <td>0.395969</td>\n",
              "      <td>1.567024e-01</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.460524</td>\n",
              "      <td>0.017305</td>\n",
              "      <td>0.492073</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>1.849163e-05</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>0.697500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.857247</td>\n",
              "      <td>0.089692</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>6.416672e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.569378e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.003525</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005799</td>\n",
              "      <td>0.337778</td>\n",
              "      <td>6.525424e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.416667e-07</td>\n",
              "      <td>7.542372e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.981498</td>\n",
              "      <td>0.979060</td>\n",
              "      <td>4.444444e-03</td>\n",
              "      <td>6.666667e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000944</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>5.357143e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.001273</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>9.568759e-09</td>\n",
              "      <td>0.445572</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.101010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.622190</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>1.420557e-01</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.440678e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.416667e-01</td>\n",
              "      <td>1.440679e-01</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>1.416667e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.981499</td>\n",
              "      <td>0.979060</td>\n",
              "      <td>3.910831e-08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.006271</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>0.141667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.282828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.282828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.813331</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>4.070336e-04</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>4.497608e-07</td>\n",
              "      <td>0.018251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023124</td>\n",
              "      <td>0.034950</td>\n",
              "      <td>0.019475</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023935</td>\n",
              "      <td>0.024243</td>\n",
              "      <td>0.005768</td>\n",
              "      <td>0.333361</td>\n",
              "      <td>5.914770e-05</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>1.938250e-04</td>\n",
              "      <td>6.779660e-07</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>1.600000e-06</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>1.262500e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.981499</td>\n",
              "      <td>0.979060</td>\n",
              "      <td>3.412364e-05</td>\n",
              "      <td>3.071127e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.018251</td>\n",
              "      <td>0.038527</td>\n",
              "      <td>0.035421</td>\n",
              "      <td>1.253951e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038980</td>\n",
              "      <td>0.023124</td>\n",
              "      <td>0.023935</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>4.497317e-07</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.014343</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  ...  Idle Max  Idle Min\n",
              "0              0.0         0.969697  ...  0.715833  0.715833\n",
              "1              0.0         0.969697  ...  0.697500  0.697500\n",
              "2              0.0         0.000000  ...  0.000000  0.000000\n",
              "3              0.0         0.070707  ...  0.141667  0.141667\n",
              "4              0.0         0.282828  ...  0.000000  0.000000\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T8Abk2XhwI"
      },
      "source": [
        "Select a subset of features from the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "LFABov84XhwK",
        "outputId": "a7ff97cf-9558-430d-c1c1-9009762adf4d"
      },
      "source": [
        "df_knn_sub=df_knn_train.loc[:, features]\n",
        "df_knn_sub.head()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.290094</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>8.158333e-01</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>6.815038e-02</td>\n",
              "      <td>0.530855</td>\n",
              "      <td>7.141667e-01</td>\n",
              "      <td>0.362854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.323232</td>\n",
              "      <td>0.027438</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.058490</td>\n",
              "      <td>7.597250e-04</td>\n",
              "      <td>0.400011</td>\n",
              "      <td>0.038557</td>\n",
              "      <td>0.005765</td>\n",
              "      <td>1.693375e-04</td>\n",
              "      <td>0.020649</td>\n",
              "      <td>5.154583e-04</td>\n",
              "      <td>0.025689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.575758</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.585859</td>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.001491</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000e-08</td>\n",
              "      <td>0.400026</td>\n",
              "      <td>0.006230</td>\n",
              "      <td>0.005766</td>\n",
              "      <td>8.548889e-05</td>\n",
              "      <td>0.029972</td>\n",
              "      <td>2.563917e-04</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.403390</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>9.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.916667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.916667e-07</td>\n",
              "      <td>0.408511</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000e-07</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0         0.010101         1.000000  ...  7.141667e-01               0.362854\n",
              "1         0.010101         0.222222  ...  5.154583e-04               0.025689\n",
              "2         0.000000         0.010101  ...  2.563917e-04               0.000000\n",
              "3         0.010101         0.000000  ...  9.916667e-07               0.000000\n",
              "4         0.010101         0.030303  ...  4.000000e-07               0.000000\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "zhVGlgN5XhwK",
        "outputId": "b9aee704-8323-463b-83fc-e0e03b3d99ad"
      },
      "source": [
        "df_knn_test_sub=df_knn_test.loc[:, features]\n",
        "df_knn_test_sub.head()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dst sport count</th>\n",
              "      <th>src dport count</th>\n",
              "      <th>dst src count</th>\n",
              "      <th>dport count</th>\n",
              "      <th>sport count</th>\n",
              "      <th>dst host count</th>\n",
              "      <th>src host count</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.013417</td>\n",
              "      <td>0.305425</td>\n",
              "      <td>0.023453</td>\n",
              "      <td>0.715833</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.014166</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>6.627202e-02</td>\n",
              "      <td>0.492073</td>\n",
              "      <td>7.158333e-01</td>\n",
              "      <td>0.324761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.969697</td>\n",
              "      <td>0.013175</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.025003</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.017305</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>7.106208e-02</td>\n",
              "      <td>0.492073</td>\n",
              "      <td>6.975000e-01</td>\n",
              "      <td>0.352229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.337778</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005799</td>\n",
              "      <td>6.525424e-07</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>6.416667e-07</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.101010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.070707</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>1.440678e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.416667e-01</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.282828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.282828</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.018251</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.034950</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>0.333361</td>\n",
              "      <td>0.023124</td>\n",
              "      <td>0.005768</td>\n",
              "      <td>5.914770e-05</td>\n",
              "      <td>0.023935</td>\n",
              "      <td>1.938250e-04</td>\n",
              "      <td>0.024243</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dst sport count  src dport count  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0              0.0         0.969697  ...  7.158333e-01               0.324761\n",
              "1              0.0         0.969697  ...  6.975000e-01               0.352229\n",
              "2              0.0         0.000000  ...  6.416667e-07               0.000000\n",
              "3              0.0         0.070707  ...  1.416667e-01               0.000000\n",
              "4              0.0         0.282828  ...  1.938250e-04               0.024243\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59G_b79XhwK"
      },
      "source": [
        "Convert dataframes to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJHMos8rXhwK"
      },
      "source": [
        "X_train_knn = df_knn_sub.to_numpy()\n",
        "X_test_knn = df_knn_test_sub.to_numpy()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hAzqyPsXhwK",
        "outputId": "bbe9fe67-662c-4d8e-d917-169987a4ba62"
      },
      "source": [
        "for i in range(5,X_train_knn.shape[1]+1):\n",
        "    knn=KNeighborsClassifier(n_neighbors=i)\n",
        "    model_knn=knn.fit(X_train_knn,y_train)\n",
        "    y_pred=model_knn.predict(X_test_knn)\n",
        "    print(\"for \" , i,  \" as K, accuracy is : \", accuracy_score(y_test, y_pred))\n",
        "    display_metrics(y_test, y_pred, labels_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for  5  as K, accuracy is :  0.9596674501148644\n",
            "\n",
            "Accuracy: 0.96\n",
            "\n",
            "Micro Precision: 0.96\n",
            "Micro Recall: 0.96\n",
            "Micro F1-score: 0.96\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.79\n",
            "Macro Recall: 0.75\n",
            "Macro F1-score: 0.77\n",
            "\n",
            "Weighted Precision: 0.96\n",
            "Weighted Recall: 0.96\n",
            "Weighted F1-score: 0.96\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.95      0.99      0.97    105019\n",
            "                       Bot       0.75      0.63      0.68       280\n",
            "                      DDoS       0.98      1.00      0.99     19271\n",
            "             DoS GoldenEye       0.95      0.98      0.97      1542\n",
            "                  DoS Hulk       1.00      0.98      0.99     34547\n",
            "          DoS Slowhttptest       0.98      0.99      0.98       828\n",
            "             DoS slowloris       0.78      0.89      0.83       834\n",
            "               FTP-Patator       0.94      0.82      0.88      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.94      0.81      0.87     23846\n",
            "               SSH-Patator       0.95      0.59      0.73       826\n",
            "  Web Attack � Brute Force       0.86      0.78      0.82       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.84      0.78      0.81        93\n",
            "\n",
            "                 micro avg       0.96      0.96      0.96    188483\n",
            "                 macro avg       0.79      0.75      0.77    188483\n",
            "              weighted avg       0.96      0.96      0.96    188483\n",
            "               samples avg       0.96      0.96      0.96    188483\n",
            "\n",
            "for  6  as K, accuracy is :  0.9583039319195895\n",
            "\n",
            "Accuracy: 0.96\n",
            "\n",
            "Micro Precision: 0.96\n",
            "Micro Recall: 0.96\n",
            "Micro F1-score: 0.96\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.81\n",
            "Macro Recall: 0.73\n",
            "Macro F1-score: 0.76\n",
            "\n",
            "Weighted Precision: 0.96\n",
            "Weighted Recall: 0.96\n",
            "Weighted F1-score: 0.96\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.95      0.98      0.97    105019\n",
            "                       Bot       0.79      0.59      0.68       280\n",
            "                      DDoS       0.98      1.00      0.99     19271\n",
            "             DoS GoldenEye       0.96      0.98      0.97      1542\n",
            "                  DoS Hulk       1.00      0.98      0.99     34547\n",
            "          DoS Slowhttptest       1.00      0.99      0.99       828\n",
            "             DoS slowloris       0.79      0.87      0.83       834\n",
            "               FTP-Patator       0.95      0.79      0.86      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.94      0.81      0.87     23846\n",
            "               SSH-Patator       0.96      0.58      0.72       826\n",
            "  Web Attack � Brute Force       0.89      0.71      0.79       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.92      0.72      0.81        93\n",
            "\n",
            "                 micro avg       0.96      0.96      0.96    188483\n",
            "                 macro avg       0.81      0.73      0.76    188483\n",
            "              weighted avg       0.96      0.96      0.96    188483\n",
            "               samples avg       0.96      0.96      0.96    188483\n",
            "\n",
            "for  7  as K, accuracy is :  0.9595560342312038\n",
            "\n",
            "Accuracy: 0.96\n",
            "\n",
            "Micro Precision: 0.96\n",
            "Micro Recall: 0.96\n",
            "Micro F1-score: 0.96\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.79\n",
            "Macro Recall: 0.74\n",
            "Macro F1-score: 0.76\n",
            "\n",
            "Weighted Precision: 0.96\n",
            "Weighted Recall: 0.96\n",
            "Weighted F1-score: 0.96\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.95      0.99      0.97    105019\n",
            "                       Bot       0.77      0.63      0.69       280\n",
            "                      DDoS       0.98      1.00      0.99     19271\n",
            "             DoS GoldenEye       0.94      0.98      0.96      1542\n",
            "                  DoS Hulk       1.00      0.98      0.99     34547\n",
            "          DoS Slowhttptest       0.99      0.99      0.99       828\n",
            "             DoS slowloris       0.77      0.89      0.83       834\n",
            "               FTP-Patator       0.92      0.85      0.88      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.94      0.81      0.87     23846\n",
            "               SSH-Patator       0.95      0.58      0.72       826\n",
            "  Web Attack � Brute Force       0.83      0.74      0.78       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.86      0.72      0.78        93\n",
            "\n",
            "                 micro avg       0.96      0.96      0.96    188483\n",
            "                 macro avg       0.79      0.74      0.76    188483\n",
            "              weighted avg       0.96      0.96      0.96    188483\n",
            "               samples avg       0.96      0.96      0.96    188483\n",
            "\n",
            "for  8  as K, accuracy is :  0.9581712939628507\n",
            "\n",
            "Accuracy: 0.96\n",
            "\n",
            "Micro Precision: 0.96\n",
            "Micro Recall: 0.96\n",
            "Micro F1-score: 0.96\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.80\n",
            "Macro Recall: 0.73\n",
            "Macro F1-score: 0.76\n",
            "\n",
            "Weighted Precision: 0.96\n",
            "Weighted Recall: 0.96\n",
            "Weighted F1-score: 0.96\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.95      0.98      0.97    105019\n",
            "                       Bot       0.79      0.60      0.68       280\n",
            "                      DDoS       0.98      1.00      0.99     19271\n",
            "             DoS GoldenEye       0.96      0.98      0.97      1542\n",
            "                  DoS Hulk       1.00      0.98      0.99     34547\n",
            "          DoS Slowhttptest       0.99      0.99      0.99       828\n",
            "             DoS slowloris       0.79      0.88      0.83       834\n",
            "               FTP-Patator       0.92      0.81      0.86      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.94      0.81      0.87     23846\n",
            "               SSH-Patator       0.96      0.55      0.70       826\n",
            "  Web Attack � Brute Force       0.85      0.73      0.79       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.91      0.67      0.77        93\n",
            "\n",
            "                 micro avg       0.96      0.96      0.96    188483\n",
            "                 macro avg       0.80      0.73      0.76    188483\n",
            "              weighted avg       0.96      0.96      0.96    188483\n",
            "               samples avg       0.96      0.96      0.96    188483\n",
            "\n",
            "for  9  as K, accuracy is :  0.9593278969456132\n",
            "\n",
            "Accuracy: 0.96\n",
            "\n",
            "Micro Precision: 0.96\n",
            "Micro Recall: 0.96\n",
            "Micro F1-score: 0.96\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.79\n",
            "Macro Recall: 0.74\n",
            "Macro F1-score: 0.76\n",
            "\n",
            "Weighted Precision: 0.96\n",
            "Weighted Recall: 0.96\n",
            "Weighted F1-score: 0.96\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.95      0.98      0.97    105019\n",
            "                       Bot       0.75      0.66      0.70       280\n",
            "                      DDoS       0.98      1.00      0.99     19271\n",
            "             DoS GoldenEye       0.96      0.98      0.97      1542\n",
            "                  DoS Hulk       1.00      0.98      0.99     34547\n",
            "          DoS Slowhttptest       0.99      0.99      0.99       828\n",
            "             DoS slowloris       0.78      0.88      0.83       834\n",
            "               FTP-Patator       0.91      0.88      0.90      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.93      0.81      0.87     23846\n",
            "               SSH-Patator       0.95      0.55      0.70       826\n",
            "  Web Attack � Brute Force       0.79      0.76      0.77       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.85      0.68      0.75        93\n",
            "\n",
            "                 micro avg       0.96      0.96      0.96    188483\n",
            "                 macro avg       0.79      0.74      0.76    188483\n",
            "              weighted avg       0.96      0.96      0.96    188483\n",
            "               samples avg       0.96      0.96      0.96    188483\n",
            "\n",
            "for  10  as K, accuracy is :  0.9581872105176594\n",
            "\n",
            "Accuracy: 0.96\n",
            "\n",
            "Micro Precision: 0.96\n",
            "Micro Recall: 0.96\n",
            "Micro F1-score: 0.96\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro Precision: 0.80\n",
            "Macro Recall: 0.73\n",
            "Macro F1-score: 0.76\n",
            "\n",
            "Weighted Precision: 0.96\n",
            "Weighted Recall: 0.96\n",
            "Weighted F1-score: 0.96\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.95      0.98      0.97    105019\n",
            "                       Bot       0.76      0.64      0.69       280\n",
            "                      DDoS       0.98      1.00      0.99     19271\n",
            "             DoS GoldenEye       0.96      0.98      0.97      1542\n",
            "                  DoS Hulk       1.00      0.98      0.99     34547\n",
            "          DoS Slowhttptest       1.00      0.99      0.99       828\n",
            "             DoS slowloris       0.78      0.87      0.82       834\n",
            "               FTP-Patator       0.91      0.85      0.88      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       0.00      0.00      0.00         1\n",
            "                  PortScan       0.94      0.81      0.87     23846\n",
            "               SSH-Patator       0.95      0.54      0.69       826\n",
            "  Web Attack � Brute Force       0.84      0.73      0.78       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.91      0.62      0.74        93\n",
            "\n",
            "                 micro avg       0.96      0.96      0.96    188483\n",
            "                 macro avg       0.80      0.73      0.76    188483\n",
            "              weighted avg       0.96      0.96      0.96    188483\n",
            "               samples avg       0.96      0.96      0.96    188483\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQvcVlbmXhwL"
      },
      "source": [
        "# Model 9: DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tL-xBt_XhwL"
      },
      "source": [
        "def make_model(metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          256, activation='relu',\n",
        "          input_shape=(X_train.shape[-1],)),\n",
        "      #tf.keras.layers.Dropout(0.9),\n",
        "      tf.keras.layers.Dense(256, activation ='relu'),\n",
        "      #tf.keras.layers.Dropout(0.4),\n",
        "      #tf.keras.layers.Dense(256, activation ='relu'),\n",
        "      #tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(128, activation ='relu'),\n",
        "      #tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(64, activation ='relu'),\n",
        "      tf.keras.layers.Dense(y_train.shape[-1], activation='softmax',\n",
        "                         bias_initializer=output_bias),\n",
        "  ])\n",
        " \n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAzpGMEoXhwL"
      },
      "source": [
        "EPOCHS = 300\n",
        "BATCH_SIZE = 9000\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWmjTb9kXhwL",
        "outputId": "130c973d-5cd0-4ec4-d669-f2d01f5d2c45"
      },
      "source": [
        "model_dnn = make_model()\n",
        "model_dnn.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               20224     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 15)                975       \n",
            "=================================================================\n",
            "Total params: 128,143\n",
            "Trainable params: 128,143\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajWocDr4_30a"
      },
      "source": [
        "If loading the validation dataset has an issue, please load the csv files again, and encode it again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeRlOQj6XhwL",
        "outputId": "16714968-75a2-4450-8f13-9ee5a1a19578"
      },
      "source": [
        "baseline_history = model_dnn.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    #callbacks=[early_stopping],\n",
        "    validation_data=(X_val, y_val))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "98/98 [==============================] - 6s 33ms/step - loss: 0.6860 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 6298045.3737 - fn: 449860.3838 - accuracy: 0.9333 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5193 - val_loss: 0.3195 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 2638776.0000 - val_fn: 188484.0000 - val_accuracy: 0.9333 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8474\n",
            "Epoch 2/300\n",
            "98/98 [==============================] - 2s 24ms/step - loss: 0.2280 - tp: 59558.2323 - fp: 10830.2222 - tn: 6287215.1515 - fn: 390302.1515 - accuracy: 0.9377 - precision: 0.5770 - recall: 0.0821 - auc: 0.8953 - val_loss: 0.1067 - val_tp: 123107.0000 - val_fp: 11771.0000 - val_tn: 2627005.0000 - val_fn: 65377.0000 - val_accuracy: 0.9727 - val_precision: 0.9127 - val_recall: 0.6531 - val_auc: 0.9718\n",
            "Epoch 3/300\n",
            "98/98 [==============================] - 2s 24ms/step - loss: 0.0896 - tp: 316744.0707 - fp: 27208.0707 - tn: 6270837.3030 - fn: 133116.3131 - accuracy: 0.9752 - precision: 0.9208 - recall: 0.6868 - auc: 0.9751 - val_loss: 0.0590 - val_tp: 156242.0000 - val_fp: 16767.0000 - val_tn: 2622009.0000 - val_fn: 32242.0000 - val_accuracy: 0.9827 - val_precision: 0.9031 - val_recall: 0.8289 - val_auc: 0.9800\n",
            "Epoch 4/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0550 - tp: 380242.5253 - fp: 42832.2929 - tn: 6255213.0808 - fn: 69617.8586 - accuracy: 0.9831 - precision: 0.8993 - recall: 0.8411 - auc: 0.9805 - val_loss: 0.0470 - val_tp: 163232.0000 - val_fp: 18364.0000 - val_tn: 2620412.0000 - val_fn: 25252.0000 - val_accuracy: 0.9846 - val_precision: 0.8989 - val_recall: 0.8660 - val_auc: 0.9824\n",
            "Epoch 5/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0453 - tp: 393932.6970 - fp: 47337.0707 - tn: 6250708.3030 - fn: 55927.6869 - accuracy: 0.9846 - precision: 0.8938 - recall: 0.8730 - auc: 0.9832 - val_loss: 0.0410 - val_tp: 167300.0000 - val_fp: 20477.0000 - val_tn: 2618299.0000 - val_fn: 21184.0000 - val_accuracy: 0.9853 - val_precision: 0.8910 - val_recall: 0.8876 - val_auc: 0.9857\n",
            "Epoch 6/300\n",
            "98/98 [==============================] - 2s 24ms/step - loss: 0.0397 - tp: 400977.2626 - fp: 46689.8586 - tn: 6251355.5152 - fn: 48883.1212 - accuracy: 0.9857 - precision: 0.8948 - recall: 0.8909 - auc: 0.9868 - val_loss: 0.0365 - val_tp: 168242.0000 - val_fp: 17973.0000 - val_tn: 2620803.0000 - val_fn: 20242.0000 - val_accuracy: 0.9865 - val_precision: 0.9035 - val_recall: 0.8926 - val_auc: 0.9901\n",
            "Epoch 7/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0354 - tp: 409431.3737 - fp: 34109.8687 - tn: 6263935.5051 - fn: 40429.0101 - accuracy: 0.9885 - precision: 0.9199 - recall: 0.9059 - auc: 0.9916 - val_loss: 0.0321 - val_tp: 173858.0000 - val_fp: 12965.0000 - val_tn: 2625811.0000 - val_fn: 14626.0000 - val_accuracy: 0.9902 - val_precision: 0.9306 - val_recall: 0.9224 - val_auc: 0.9937\n",
            "Epoch 8/300\n",
            "98/98 [==============================] - 2s 24ms/step - loss: 0.0309 - tp: 415775.6566 - fp: 30459.1313 - tn: 6267586.2424 - fn: 34084.7273 - accuracy: 0.9904 - precision: 0.9314 - recall: 0.9237 - auc: 0.9940 - val_loss: 0.0280 - val_tp: 175834.0000 - val_fp: 11567.0000 - val_tn: 2627209.0000 - val_fn: 12650.0000 - val_accuracy: 0.9914 - val_precision: 0.9383 - val_recall: 0.9329 - val_auc: 0.9943\n",
            "Epoch 9/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0265 - tp: 420697.2929 - fp: 26780.8788 - tn: 6271264.4949 - fn: 29163.0909 - accuracy: 0.9916 - precision: 0.9394 - recall: 0.9342 - auc: 0.9948 - val_loss: 0.0238 - val_tp: 176888.0000 - val_fp: 10698.0000 - val_tn: 2628078.0000 - val_fn: 11596.0000 - val_accuracy: 0.9921 - val_precision: 0.9430 - val_recall: 0.9385 - val_auc: 0.9964\n",
            "Epoch 10/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0225 - tp: 424873.1919 - fp: 22564.1717 - tn: 6275481.2020 - fn: 24987.1919 - accuracy: 0.9928 - precision: 0.9486 - recall: 0.9435 - auc: 0.9969 - val_loss: 0.0206 - val_tp: 178752.0000 - val_fp: 8807.0000 - val_tn: 2629969.0000 - val_fn: 9732.0000 - val_accuracy: 0.9934 - val_precision: 0.9530 - val_recall: 0.9484 - val_auc: 0.9970\n",
            "Epoch 11/300\n",
            "98/98 [==============================] - 2s 24ms/step - loss: 0.0192 - tp: 429521.8182 - fp: 17769.7576 - tn: 6280275.6162 - fn: 20338.5657 - accuracy: 0.9943 - precision: 0.9595 - recall: 0.9540 - auc: 0.9977 - val_loss: 0.0179 - val_tp: 180015.0000 - val_fp: 7154.0000 - val_tn: 2631622.0000 - val_fn: 8469.0000 - val_accuracy: 0.9945 - val_precision: 0.9618 - val_recall: 0.9551 - val_auc: 0.9972\n",
            "Epoch 12/300\n",
            "98/98 [==============================] - 2s 24ms/step - loss: 0.0167 - tp: 432026.8586 - fp: 14997.1818 - tn: 6283048.1919 - fn: 17833.5253 - accuracy: 0.9951 - precision: 0.9660 - recall: 0.9599 - auc: 0.9978 - val_loss: 0.0163 - val_tp: 180747.0000 - val_fp: 6531.0000 - val_tn: 2632245.0000 - val_fn: 7737.0000 - val_accuracy: 0.9950 - val_precision: 0.9651 - val_recall: 0.9590 - val_auc: 0.9972\n",
            "Epoch 13/300\n",
            "98/98 [==============================] - 2s 24ms/step - loss: 0.0150 - tp: 433422.3737 - fp: 13068.8384 - tn: 6284976.5354 - fn: 16438.0101 - accuracy: 0.9956 - precision: 0.9703 - recall: 0.9632 - auc: 0.9978 - val_loss: 0.0150 - val_tp: 181379.0000 - val_fp: 5685.0000 - val_tn: 2633091.0000 - val_fn: 7105.0000 - val_accuracy: 0.9955 - val_precision: 0.9696 - val_recall: 0.9623 - val_auc: 0.9973\n",
            "Epoch 14/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0137 - tp: 434870.4949 - fp: 11181.2121 - tn: 6286864.1616 - fn: 14989.8889 - accuracy: 0.9961 - precision: 0.9748 - recall: 0.9668 - auc: 0.9978 - val_loss: 0.0137 - val_tp: 181763.0000 - val_fp: 5029.0000 - val_tn: 2633747.0000 - val_fn: 6721.0000 - val_accuracy: 0.9958 - val_precision: 0.9731 - val_recall: 0.9643 - val_auc: 0.9974\n",
            "Epoch 15/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0127 - tp: 435213.9697 - fp: 10376.8687 - tn: 6287668.5051 - fn: 14646.4141 - accuracy: 0.9963 - precision: 0.9766 - recall: 0.9674 - auc: 0.9979 - val_loss: 0.0130 - val_tp: 181816.0000 - val_fp: 4822.0000 - val_tn: 2633954.0000 - val_fn: 6668.0000 - val_accuracy: 0.9959 - val_precision: 0.9742 - val_recall: 0.9646 - val_auc: 0.9974\n",
            "Epoch 16/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0118 - tp: 435696.4949 - fp: 9878.6364 - tn: 6288166.7374 - fn: 14163.8889 - accuracy: 0.9964 - precision: 0.9778 - recall: 0.9685 - auc: 0.9980 - val_loss: 0.0126 - val_tp: 182042.0000 - val_fp: 4919.0000 - val_tn: 2633857.0000 - val_fn: 6442.0000 - val_accuracy: 0.9960 - val_precision: 0.9737 - val_recall: 0.9658 - val_auc: 0.9975\n",
            "Epoch 17/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0110 - tp: 436896.8081 - fp: 9575.7273 - tn: 6288469.6465 - fn: 12963.5758 - accuracy: 0.9967 - precision: 0.9785 - recall: 0.9711 - auc: 0.9983 - val_loss: 0.0117 - val_tp: 182515.0000 - val_fp: 4614.0000 - val_tn: 2634162.0000 - val_fn: 5969.0000 - val_accuracy: 0.9963 - val_precision: 0.9753 - val_recall: 0.9683 - val_auc: 0.9977\n",
            "Epoch 18/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0105 - tp: 437453.6970 - fp: 9336.0505 - tn: 6288709.3232 - fn: 12406.6869 - accuracy: 0.9968 - precision: 0.9792 - recall: 0.9725 - auc: 0.9983 - val_loss: 0.0109 - val_tp: 182908.0000 - val_fp: 4356.0000 - val_tn: 2634420.0000 - val_fn: 5576.0000 - val_accuracy: 0.9965 - val_precision: 0.9767 - val_recall: 0.9704 - val_auc: 0.9980\n",
            "Epoch 19/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0100 - tp: 437884.8990 - fp: 9203.6162 - tn: 6288841.7576 - fn: 11975.4848 - accuracy: 0.9968 - precision: 0.9793 - recall: 0.9731 - auc: 0.9984 - val_loss: 0.0108 - val_tp: 183095.0000 - val_fp: 4295.0000 - val_tn: 2634481.0000 - val_fn: 5389.0000 - val_accuracy: 0.9966 - val_precision: 0.9771 - val_recall: 0.9714 - val_auc: 0.9975\n",
            "Epoch 20/300\n",
            "98/98 [==============================] - 2s 24ms/step - loss: 0.0095 - tp: 438521.0606 - fp: 8951.6869 - tn: 6289093.6869 - fn: 11339.3232 - accuracy: 0.9970 - precision: 0.9800 - recall: 0.9748 - auc: 0.9983 - val_loss: 0.0102 - val_tp: 183222.0000 - val_fp: 4166.0000 - val_tn: 2634610.0000 - val_fn: 5262.0000 - val_accuracy: 0.9967 - val_precision: 0.9778 - val_recall: 0.9721 - val_auc: 0.9978\n",
            "Epoch 21/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0092 - tp: 438830.4242 - fp: 8699.4545 - tn: 6289345.9192 - fn: 11029.9596 - accuracy: 0.9970 - precision: 0.9803 - recall: 0.9752 - auc: 0.9984 - val_loss: 0.0098 - val_tp: 183512.0000 - val_fp: 3925.0000 - val_tn: 2634851.0000 - val_fn: 4972.0000 - val_accuracy: 0.9969 - val_precision: 0.9791 - val_recall: 0.9736 - val_auc: 0.9979\n",
            "Epoch 22/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0087 - tp: 439434.1919 - fp: 8245.2828 - tn: 6289800.0909 - fn: 10426.1919 - accuracy: 0.9972 - precision: 0.9817 - recall: 0.9769 - auc: 0.9984 - val_loss: 0.0095 - val_tp: 183674.0000 - val_fp: 3929.0000 - val_tn: 2634847.0000 - val_fn: 4810.0000 - val_accuracy: 0.9969 - val_precision: 0.9791 - val_recall: 0.9745 - val_auc: 0.9980\n",
            "Epoch 23/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0084 - tp: 439979.9697 - fp: 7956.3838 - tn: 6290088.9899 - fn: 9880.4141 - accuracy: 0.9974 - precision: 0.9823 - recall: 0.9781 - auc: 0.9984 - val_loss: 0.0093 - val_tp: 184063.0000 - val_fp: 3547.0000 - val_tn: 2635229.0000 - val_fn: 4421.0000 - val_accuracy: 0.9972 - val_precision: 0.9811 - val_recall: 0.9765 - val_auc: 0.9977\n",
            "Epoch 24/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0081 - tp: 440366.4949 - fp: 7633.8687 - tn: 6290411.5051 - fn: 9493.8889 - accuracy: 0.9975 - precision: 0.9831 - recall: 0.9790 - auc: 0.9985 - val_loss: 0.0087 - val_tp: 184090.0000 - val_fp: 3658.0000 - val_tn: 2635118.0000 - val_fn: 4394.0000 - val_accuracy: 0.9972 - val_precision: 0.9805 - val_recall: 0.9767 - val_auc: 0.9982\n",
            "Epoch 25/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0080 - tp: 440566.1313 - fp: 7604.4747 - tn: 6290440.8990 - fn: 9294.2525 - accuracy: 0.9975 - precision: 0.9829 - recall: 0.9792 - auc: 0.9984 - val_loss: 0.0085 - val_tp: 184546.0000 - val_fp: 3268.0000 - val_tn: 2635508.0000 - val_fn: 3938.0000 - val_accuracy: 0.9975 - val_precision: 0.9826 - val_recall: 0.9791 - val_auc: 0.9979\n",
            "Epoch 26/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0077 - tp: 440931.1313 - fp: 7346.0404 - tn: 6290699.3333 - fn: 8929.2525 - accuracy: 0.9976 - precision: 0.9836 - recall: 0.9801 - auc: 0.9984 - val_loss: 0.0083 - val_tp: 184468.0000 - val_fp: 3336.0000 - val_tn: 2635440.0000 - val_fn: 4016.0000 - val_accuracy: 0.9974 - val_precision: 0.9822 - val_recall: 0.9787 - val_auc: 0.9979\n",
            "Epoch 27/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0074 - tp: 441265.3131 - fp: 7097.0606 - tn: 6290948.3131 - fn: 8595.0707 - accuracy: 0.9977 - precision: 0.9844 - recall: 0.9810 - auc: 0.9984 - val_loss: 0.0080 - val_tp: 184644.0000 - val_fp: 3199.0000 - val_tn: 2635577.0000 - val_fn: 3840.0000 - val_accuracy: 0.9975 - val_precision: 0.9830 - val_recall: 0.9796 - val_auc: 0.9979\n",
            "Epoch 28/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0073 - tp: 441494.5758 - fp: 6931.5253 - tn: 6291113.8485 - fn: 8365.8081 - accuracy: 0.9977 - precision: 0.9845 - recall: 0.9814 - auc: 0.9984 - val_loss: 0.0079 - val_tp: 184753.0000 - val_fp: 3109.0000 - val_tn: 2635667.0000 - val_fn: 3731.0000 - val_accuracy: 0.9976 - val_precision: 0.9835 - val_recall: 0.9802 - val_auc: 0.9978\n",
            "Epoch 29/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0071 - tp: 441710.9293 - fp: 6805.2020 - tn: 6291240.1717 - fn: 8149.4545 - accuracy: 0.9978 - precision: 0.9848 - recall: 0.9818 - auc: 0.9984 - val_loss: 0.0076 - val_tp: 184804.0000 - val_fp: 3087.0000 - val_tn: 2635689.0000 - val_fn: 3680.0000 - val_accuracy: 0.9976 - val_precision: 0.9836 - val_recall: 0.9805 - val_auc: 0.9984\n",
            "Epoch 30/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0069 - tp: 441875.2929 - fp: 6616.5253 - tn: 6291428.8485 - fn: 7985.0909 - accuracy: 0.9978 - precision: 0.9852 - recall: 0.9822 - auc: 0.9984 - val_loss: 0.0075 - val_tp: 184789.0000 - val_fp: 3106.0000 - val_tn: 2635670.0000 - val_fn: 3695.0000 - val_accuracy: 0.9976 - val_precision: 0.9835 - val_recall: 0.9804 - val_auc: 0.9980\n",
            "Epoch 31/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0067 - tp: 442034.0606 - fp: 6481.8990 - tn: 6291563.4747 - fn: 7826.3232 - accuracy: 0.9979 - precision: 0.9857 - recall: 0.9827 - auc: 0.9984 - val_loss: 0.0073 - val_tp: 184923.0000 - val_fp: 2994.0000 - val_tn: 2635782.0000 - val_fn: 3561.0000 - val_accuracy: 0.9977 - val_precision: 0.9841 - val_recall: 0.9811 - val_auc: 0.9980\n",
            "Epoch 32/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0065 - tp: 442229.8081 - fp: 6349.6566 - tn: 6291695.7172 - fn: 7630.5758 - accuracy: 0.9979 - precision: 0.9858 - recall: 0.9830 - auc: 0.9984 - val_loss: 0.0073 - val_tp: 184976.0000 - val_fp: 2958.0000 - val_tn: 2635818.0000 - val_fn: 3508.0000 - val_accuracy: 0.9977 - val_precision: 0.9843 - val_recall: 0.9814 - val_auc: 0.9978\n",
            "Epoch 33/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0064 - tp: 442510.5152 - fp: 6079.2929 - tn: 6291966.0808 - fn: 7349.8687 - accuracy: 0.9980 - precision: 0.9864 - recall: 0.9836 - auc: 0.9985 - val_loss: 0.0069 - val_tp: 185093.0000 - val_fp: 2850.0000 - val_tn: 2635926.0000 - val_fn: 3391.0000 - val_accuracy: 0.9978 - val_precision: 0.9848 - val_recall: 0.9820 - val_auc: 0.9981\n",
            "Epoch 34/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0062 - tp: 442682.6061 - fp: 5910.5051 - tn: 6292134.8687 - fn: 7177.7778 - accuracy: 0.9981 - precision: 0.9869 - recall: 0.9841 - auc: 0.9985 - val_loss: 0.0070 - val_tp: 185048.0000 - val_fp: 2871.0000 - val_tn: 2635905.0000 - val_fn: 3436.0000 - val_accuracy: 0.9978 - val_precision: 0.9847 - val_recall: 0.9818 - val_auc: 0.9978\n",
            "Epoch 35/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0061 - tp: 442774.0101 - fp: 5868.9192 - tn: 6292176.4545 - fn: 7086.3737 - accuracy: 0.9981 - precision: 0.9868 - recall: 0.9841 - auc: 0.9985 - val_loss: 0.0065 - val_tp: 185243.0000 - val_fp: 2710.0000 - val_tn: 2636066.0000 - val_fn: 3241.0000 - val_accuracy: 0.9979 - val_precision: 0.9856 - val_recall: 0.9828 - val_auc: 0.9982\n",
            "Epoch 36/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0059 - tp: 443155.0707 - fp: 5731.4343 - tn: 6292313.9394 - fn: 6705.3131 - accuracy: 0.9982 - precision: 0.9873 - recall: 0.9850 - auc: 0.9985 - val_loss: 0.0064 - val_tp: 185508.0000 - val_fp: 2649.0000 - val_tn: 2636127.0000 - val_fn: 2976.0000 - val_accuracy: 0.9980 - val_precision: 0.9859 - val_recall: 0.9842 - val_auc: 0.9982\n",
            "Epoch 37/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0059 - tp: 443430.1515 - fp: 5690.9899 - tn: 6292354.3838 - fn: 6430.2323 - accuracy: 0.9982 - precision: 0.9873 - recall: 0.9856 - auc: 0.9985 - val_loss: 0.0064 - val_tp: 185438.0000 - val_fp: 2783.0000 - val_tn: 2635993.0000 - val_fn: 3046.0000 - val_accuracy: 0.9979 - val_precision: 0.9852 - val_recall: 0.9838 - val_auc: 0.9981\n",
            "Epoch 38/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0056 - tp: 443736.8283 - fp: 5534.8990 - tn: 6292510.4747 - fn: 6123.5556 - accuracy: 0.9983 - precision: 0.9877 - recall: 0.9864 - auc: 0.9985 - val_loss: 0.0062 - val_tp: 185584.0000 - val_fp: 2695.0000 - val_tn: 2636081.0000 - val_fn: 2900.0000 - val_accuracy: 0.9980 - val_precision: 0.9857 - val_recall: 0.9846 - val_auc: 0.9983\n",
            "Epoch 39/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0055 - tp: 443891.6768 - fp: 5444.4141 - tn: 6292600.9596 - fn: 5968.7071 - accuracy: 0.9983 - precision: 0.9880 - recall: 0.9868 - auc: 0.9985 - val_loss: 0.0061 - val_tp: 185683.0000 - val_fp: 2616.0000 - val_tn: 2636160.0000 - val_fn: 2801.0000 - val_accuracy: 0.9981 - val_precision: 0.9861 - val_recall: 0.9851 - val_auc: 0.9982\n",
            "Epoch 40/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0055 - tp: 443952.6970 - fp: 5429.1010 - tn: 6292616.2727 - fn: 5907.6869 - accuracy: 0.9983 - precision: 0.9879 - recall: 0.9869 - auc: 0.9986 - val_loss: 0.0059 - val_tp: 185835.0000 - val_fp: 2475.0000 - val_tn: 2636301.0000 - val_fn: 2649.0000 - val_accuracy: 0.9982 - val_precision: 0.9869 - val_recall: 0.9859 - val_auc: 0.9984\n",
            "Epoch 41/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0054 - tp: 444122.7071 - fp: 5339.9091 - tn: 6292705.4646 - fn: 5737.6768 - accuracy: 0.9984 - precision: 0.9881 - recall: 0.9873 - auc: 0.9985 - val_loss: 0.0058 - val_tp: 185814.0000 - val_fp: 2512.0000 - val_tn: 2636264.0000 - val_fn: 2670.0000 - val_accuracy: 0.9982 - val_precision: 0.9867 - val_recall: 0.9858 - val_auc: 0.9983\n",
            "Epoch 42/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0053 - tp: 444235.5152 - fp: 5249.1818 - tn: 6292796.1919 - fn: 5624.8687 - accuracy: 0.9984 - precision: 0.9883 - recall: 0.9875 - auc: 0.9986 - val_loss: 0.0064 - val_tp: 185397.0000 - val_fp: 2945.0000 - val_tn: 2635831.0000 - val_fn: 3087.0000 - val_accuracy: 0.9979 - val_precision: 0.9844 - val_recall: 0.9836 - val_auc: 0.9983\n",
            "Epoch 43/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0052 - tp: 444264.2929 - fp: 5282.1919 - tn: 6292763.1818 - fn: 5596.0909 - accuracy: 0.9984 - precision: 0.9881 - recall: 0.9874 - auc: 0.9986 - val_loss: 0.0055 - val_tp: 186017.0000 - val_fp: 2347.0000 - val_tn: 2636429.0000 - val_fn: 2467.0000 - val_accuracy: 0.9983 - val_precision: 0.9875 - val_recall: 0.9869 - val_auc: 0.9984\n",
            "Epoch 44/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0052 - tp: 444417.4141 - fp: 5156.2323 - tn: 6292889.1414 - fn: 5442.9697 - accuracy: 0.9984 - precision: 0.9885 - recall: 0.9878 - auc: 0.9986 - val_loss: 0.0055 - val_tp: 186011.0000 - val_fp: 2359.0000 - val_tn: 2636417.0000 - val_fn: 2473.0000 - val_accuracy: 0.9983 - val_precision: 0.9875 - val_recall: 0.9869 - val_auc: 0.9983\n",
            "Epoch 45/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0050 - tp: 444531.1414 - fp: 5069.1212 - tn: 6292976.2525 - fn: 5329.2424 - accuracy: 0.9985 - precision: 0.9887 - recall: 0.9881 - auc: 0.9986 - val_loss: 0.0054 - val_tp: 186003.0000 - val_fp: 2360.0000 - val_tn: 2636416.0000 - val_fn: 2481.0000 - val_accuracy: 0.9983 - val_precision: 0.9875 - val_recall: 0.9868 - val_auc: 0.9983\n",
            "Epoch 46/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0049 - tp: 444662.0909 - fp: 4945.9697 - tn: 6293099.4040 - fn: 5198.2929 - accuracy: 0.9985 - precision: 0.9890 - recall: 0.9884 - auc: 0.9986 - val_loss: 0.0054 - val_tp: 186022.0000 - val_fp: 2353.0000 - val_tn: 2636423.0000 - val_fn: 2462.0000 - val_accuracy: 0.9983 - val_precision: 0.9875 - val_recall: 0.9869 - val_auc: 0.9984\n",
            "Epoch 47/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0049 - tp: 444775.4444 - fp: 4860.0000 - tn: 6293185.3737 - fn: 5084.9394 - accuracy: 0.9985 - precision: 0.9891 - recall: 0.9886 - auc: 0.9987 - val_loss: 0.0053 - val_tp: 186052.0000 - val_fp: 2352.0000 - val_tn: 2636424.0000 - val_fn: 2432.0000 - val_accuracy: 0.9983 - val_precision: 0.9875 - val_recall: 0.9871 - val_auc: 0.9983\n",
            "Epoch 48/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0048 - tp: 444781.1616 - fp: 4870.5657 - tn: 6293174.8081 - fn: 5079.2222 - accuracy: 0.9985 - precision: 0.9891 - recall: 0.9887 - auc: 0.9986 - val_loss: 0.0053 - val_tp: 186061.0000 - val_fp: 2330.0000 - val_tn: 2636446.0000 - val_fn: 2423.0000 - val_accuracy: 0.9983 - val_precision: 0.9876 - val_recall: 0.9871 - val_auc: 0.9984\n",
            "Epoch 49/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0047 - tp: 444834.9495 - fp: 4820.0606 - tn: 6293225.3131 - fn: 5025.4343 - accuracy: 0.9985 - precision: 0.9893 - recall: 0.9888 - auc: 0.9986 - val_loss: 0.0051 - val_tp: 186212.0000 - val_fp: 2192.0000 - val_tn: 2636584.0000 - val_fn: 2272.0000 - val_accuracy: 0.9984 - val_precision: 0.9884 - val_recall: 0.9879 - val_auc: 0.9984\n",
            "Epoch 50/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0047 - tp: 444970.8586 - fp: 4685.4343 - tn: 6293359.9394 - fn: 4889.5253 - accuracy: 0.9986 - precision: 0.9895 - recall: 0.9891 - auc: 0.9986 - val_loss: 0.0051 - val_tp: 186239.0000 - val_fp: 2175.0000 - val_tn: 2636601.0000 - val_fn: 2245.0000 - val_accuracy: 0.9984 - val_precision: 0.9885 - val_recall: 0.9881 - val_auc: 0.9983\n",
            "Epoch 51/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0046 - tp: 444982.0000 - fp: 4693.0808 - tn: 6293352.2929 - fn: 4878.3838 - accuracy: 0.9986 - precision: 0.9896 - recall: 0.9892 - auc: 0.9987 - val_loss: 0.0050 - val_tp: 186206.0000 - val_fp: 2200.0000 - val_tn: 2636576.0000 - val_fn: 2278.0000 - val_accuracy: 0.9984 - val_precision: 0.9883 - val_recall: 0.9879 - val_auc: 0.9985\n",
            "Epoch 52/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0046 - tp: 445063.1111 - fp: 4631.6364 - tn: 6293413.7374 - fn: 4797.2727 - accuracy: 0.9986 - precision: 0.9897 - recall: 0.9893 - auc: 0.9987 - val_loss: 0.0049 - val_tp: 186294.0000 - val_fp: 2116.0000 - val_tn: 2636660.0000 - val_fn: 2190.0000 - val_accuracy: 0.9985 - val_precision: 0.9888 - val_recall: 0.9884 - val_auc: 0.9986\n",
            "Epoch 53/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0045 - tp: 445155.9091 - fp: 4542.5051 - tn: 6293502.8687 - fn: 4704.4747 - accuracy: 0.9986 - precision: 0.9898 - recall: 0.9895 - auc: 0.9987 - val_loss: 0.0048 - val_tp: 186342.0000 - val_fp: 2061.0000 - val_tn: 2636715.0000 - val_fn: 2142.0000 - val_accuracy: 0.9985 - val_precision: 0.9891 - val_recall: 0.9886 - val_auc: 0.9986\n",
            "Epoch 54/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0045 - tp: 445167.2121 - fp: 4532.6869 - tn: 6293512.6869 - fn: 4693.1717 - accuracy: 0.9986 - precision: 0.9898 - recall: 0.9895 - auc: 0.9987 - val_loss: 0.0049 - val_tp: 186235.0000 - val_fp: 2192.0000 - val_tn: 2636584.0000 - val_fn: 2249.0000 - val_accuracy: 0.9984 - val_precision: 0.9884 - val_recall: 0.9881 - val_auc: 0.9986\n",
            "Epoch 55/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0044 - tp: 445211.5051 - fp: 4520.2929 - tn: 6293525.0808 - fn: 4648.8788 - accuracy: 0.9986 - precision: 0.9899 - recall: 0.9896 - auc: 0.9987 - val_loss: 0.0048 - val_tp: 186351.0000 - val_fp: 2082.0000 - val_tn: 2636694.0000 - val_fn: 2133.0000 - val_accuracy: 0.9985 - val_precision: 0.9890 - val_recall: 0.9887 - val_auc: 0.9985\n",
            "Epoch 56/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0044 - tp: 445269.5455 - fp: 4447.3333 - tn: 6293598.0404 - fn: 4590.8384 - accuracy: 0.9987 - precision: 0.9901 - recall: 0.9898 - auc: 0.9987 - val_loss: 0.0050 - val_tp: 186229.0000 - val_fp: 2191.0000 - val_tn: 2636585.0000 - val_fn: 2255.0000 - val_accuracy: 0.9984 - val_precision: 0.9884 - val_recall: 0.9880 - val_auc: 0.9986\n",
            "Epoch 57/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0044 - tp: 445292.7576 - fp: 4429.4040 - tn: 6293615.9697 - fn: 4567.6263 - accuracy: 0.9987 - precision: 0.9901 - recall: 0.9898 - auc: 0.9987 - val_loss: 0.0046 - val_tp: 186399.0000 - val_fp: 2005.0000 - val_tn: 2636771.0000 - val_fn: 2085.0000 - val_accuracy: 0.9986 - val_precision: 0.9894 - val_recall: 0.9889 - val_auc: 0.9985\n",
            "Epoch 58/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0043 - tp: 445385.0303 - fp: 4337.0606 - tn: 6293708.3131 - fn: 4475.3535 - accuracy: 0.9987 - precision: 0.9903 - recall: 0.9900 - auc: 0.9987 - val_loss: 0.0049 - val_tp: 186281.0000 - val_fp: 2143.0000 - val_tn: 2636633.0000 - val_fn: 2203.0000 - val_accuracy: 0.9985 - val_precision: 0.9886 - val_recall: 0.9883 - val_auc: 0.9987\n",
            "Epoch 59/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0043 - tp: 445395.2424 - fp: 4336.1010 - tn: 6293709.2727 - fn: 4465.1414 - accuracy: 0.9987 - precision: 0.9903 - recall: 0.9900 - auc: 0.9988 - val_loss: 0.0046 - val_tp: 186436.0000 - val_fp: 1989.0000 - val_tn: 2636787.0000 - val_fn: 2048.0000 - val_accuracy: 0.9986 - val_precision: 0.9894 - val_recall: 0.9891 - val_auc: 0.9987\n",
            "Epoch 60/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0042 - tp: 445526.4242 - fp: 4206.2424 - tn: 6293839.1313 - fn: 4333.9596 - accuracy: 0.9987 - precision: 0.9906 - recall: 0.9903 - auc: 0.9987 - val_loss: 0.0044 - val_tp: 186600.0000 - val_fp: 1828.0000 - val_tn: 2636948.0000 - val_fn: 1884.0000 - val_accuracy: 0.9987 - val_precision: 0.9903 - val_recall: 0.9900 - val_auc: 0.9987\n",
            "Epoch 61/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0042 - tp: 445486.2626 - fp: 4259.0808 - tn: 6293786.2929 - fn: 4374.1212 - accuracy: 0.9987 - precision: 0.9905 - recall: 0.9902 - auc: 0.9987 - val_loss: 0.0045 - val_tp: 186507.0000 - val_fp: 1892.0000 - val_tn: 2636884.0000 - val_fn: 1977.0000 - val_accuracy: 0.9986 - val_precision: 0.9900 - val_recall: 0.9895 - val_auc: 0.9985\n",
            "Epoch 62/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0041 - tp: 445507.9091 - fp: 4233.8586 - tn: 6293811.5152 - fn: 4352.4747 - accuracy: 0.9987 - precision: 0.9907 - recall: 0.9904 - auc: 0.9987 - val_loss: 0.0044 - val_tp: 186493.0000 - val_fp: 1928.0000 - val_tn: 2636848.0000 - val_fn: 1991.0000 - val_accuracy: 0.9986 - val_precision: 0.9898 - val_recall: 0.9894 - val_auc: 0.9986\n",
            "Epoch 63/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0040 - tp: 445591.0505 - fp: 4169.8182 - tn: 6293875.5556 - fn: 4269.3333 - accuracy: 0.9988 - precision: 0.9908 - recall: 0.9905 - auc: 0.9988 - val_loss: 0.0045 - val_tp: 186501.0000 - val_fp: 1911.0000 - val_tn: 2636865.0000 - val_fn: 1983.0000 - val_accuracy: 0.9986 - val_precision: 0.9899 - val_recall: 0.9895 - val_auc: 0.9986\n",
            "Epoch 64/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0040 - tp: 445674.4040 - fp: 4091.7778 - tn: 6293953.5960 - fn: 4185.9798 - accuracy: 0.9988 - precision: 0.9910 - recall: 0.9907 - auc: 0.9988 - val_loss: 0.0046 - val_tp: 186480.0000 - val_fp: 1947.0000 - val_tn: 2636829.0000 - val_fn: 2004.0000 - val_accuracy: 0.9986 - val_precision: 0.9897 - val_recall: 0.9894 - val_auc: 0.9984\n",
            "Epoch 65/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0040 - tp: 445637.8586 - fp: 4117.6667 - tn: 6293927.7071 - fn: 4222.5253 - accuracy: 0.9988 - precision: 0.9908 - recall: 0.9906 - auc: 0.9987 - val_loss: 0.0046 - val_tp: 186441.0000 - val_fp: 1984.0000 - val_tn: 2636792.0000 - val_fn: 2043.0000 - val_accuracy: 0.9986 - val_precision: 0.9895 - val_recall: 0.9892 - val_auc: 0.9986\n",
            "Epoch 66/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0040 - tp: 445684.0909 - fp: 4073.2222 - tn: 6293972.1515 - fn: 4176.2929 - accuracy: 0.9988 - precision: 0.9910 - recall: 0.9908 - auc: 0.9988 - val_loss: 0.0042 - val_tp: 186619.0000 - val_fp: 1810.0000 - val_tn: 2636966.0000 - val_fn: 1865.0000 - val_accuracy: 0.9987 - val_precision: 0.9904 - val_recall: 0.9901 - val_auc: 0.9986\n",
            "Epoch 67/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0040 - tp: 445702.1212 - fp: 4056.4040 - tn: 6293988.9697 - fn: 4158.2626 - accuracy: 0.9988 - precision: 0.9910 - recall: 0.9908 - auc: 0.9987 - val_loss: 0.0045 - val_tp: 186467.0000 - val_fp: 1967.0000 - val_tn: 2636809.0000 - val_fn: 2017.0000 - val_accuracy: 0.9986 - val_precision: 0.9896 - val_recall: 0.9893 - val_auc: 0.9987\n",
            "Epoch 68/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0039 - tp: 445718.4949 - fp: 4043.0707 - tn: 6294002.3030 - fn: 4141.8889 - accuracy: 0.9988 - precision: 0.9910 - recall: 0.9908 - auc: 0.9988 - val_loss: 0.0042 - val_tp: 186621.0000 - val_fp: 1817.0000 - val_tn: 2636959.0000 - val_fn: 1863.0000 - val_accuracy: 0.9987 - val_precision: 0.9904 - val_recall: 0.9901 - val_auc: 0.9988\n",
            "Epoch 69/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0038 - tp: 445781.3030 - fp: 3979.1818 - tn: 6294066.1919 - fn: 4079.0808 - accuracy: 0.9988 - precision: 0.9911 - recall: 0.9909 - auc: 0.9988 - val_loss: 0.0042 - val_tp: 186596.0000 - val_fp: 1847.0000 - val_tn: 2636929.0000 - val_fn: 1888.0000 - val_accuracy: 0.9987 - val_precision: 0.9902 - val_recall: 0.9900 - val_auc: 0.9986\n",
            "Epoch 70/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0039 - tp: 445769.5051 - fp: 3999.0707 - tn: 6294046.3030 - fn: 4090.8788 - accuracy: 0.9988 - precision: 0.9910 - recall: 0.9908 - auc: 0.9988 - val_loss: 0.0042 - val_tp: 186577.0000 - val_fp: 1866.0000 - val_tn: 2636910.0000 - val_fn: 1907.0000 - val_accuracy: 0.9987 - val_precision: 0.9901 - val_recall: 0.9899 - val_auc: 0.9985\n",
            "Epoch 71/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0038 - tp: 445747.4343 - fp: 4015.2424 - tn: 6294030.1313 - fn: 4112.9495 - accuracy: 0.9988 - precision: 0.9910 - recall: 0.9908 - auc: 0.9988 - val_loss: 0.0041 - val_tp: 186637.0000 - val_fp: 1817.0000 - val_tn: 2636959.0000 - val_fn: 1847.0000 - val_accuracy: 0.9987 - val_precision: 0.9904 - val_recall: 0.9902 - val_auc: 0.9986\n",
            "Epoch 72/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0037 - tp: 445898.0606 - fp: 3885.0505 - tn: 6294160.3232 - fn: 3962.3232 - accuracy: 0.9988 - precision: 0.9913 - recall: 0.9912 - auc: 0.9989 - val_loss: 0.0041 - val_tp: 186622.0000 - val_fp: 1800.0000 - val_tn: 2636976.0000 - val_fn: 1862.0000 - val_accuracy: 0.9987 - val_precision: 0.9904 - val_recall: 0.9901 - val_auc: 0.9989\n",
            "Epoch 73/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0037 - tp: 445849.3030 - fp: 3918.4040 - tn: 6294126.9697 - fn: 4011.0808 - accuracy: 0.9988 - precision: 0.9912 - recall: 0.9910 - auc: 0.9989 - val_loss: 0.0040 - val_tp: 186678.0000 - val_fp: 1763.0000 - val_tn: 2637013.0000 - val_fn: 1806.0000 - val_accuracy: 0.9987 - val_precision: 0.9906 - val_recall: 0.9904 - val_auc: 0.9986\n",
            "Epoch 74/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0038 - tp: 445883.0808 - fp: 3881.7576 - tn: 6294163.6162 - fn: 3977.3030 - accuracy: 0.9988 - precision: 0.9914 - recall: 0.9912 - auc: 0.9988 - val_loss: 0.0039 - val_tp: 186724.0000 - val_fp: 1712.0000 - val_tn: 2637064.0000 - val_fn: 1760.0000 - val_accuracy: 0.9988 - val_precision: 0.9909 - val_recall: 0.9907 - val_auc: 0.9986\n",
            "Epoch 75/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0036 - tp: 445987.2020 - fp: 3791.7677 - tn: 6294253.6061 - fn: 3873.1818 - accuracy: 0.9989 - precision: 0.9916 - recall: 0.9915 - auc: 0.9989 - val_loss: 0.0039 - val_tp: 186726.0000 - val_fp: 1697.0000 - val_tn: 2637079.0000 - val_fn: 1758.0000 - val_accuracy: 0.9988 - val_precision: 0.9910 - val_recall: 0.9907 - val_auc: 0.9988\n",
            "Epoch 76/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0037 - tp: 445852.5556 - fp: 3904.9394 - tn: 6294140.4343 - fn: 4007.8283 - accuracy: 0.9988 - precision: 0.9912 - recall: 0.9909 - auc: 0.9989 - val_loss: 0.0040 - val_tp: 186555.0000 - val_fp: 1884.0000 - val_tn: 2636892.0000 - val_fn: 1929.0000 - val_accuracy: 0.9987 - val_precision: 0.9900 - val_recall: 0.9898 - val_auc: 0.9986\n",
            "Epoch 77/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0036 - tp: 445990.3838 - fp: 3776.4444 - tn: 6294268.9293 - fn: 3870.0000 - accuracy: 0.9989 - precision: 0.9916 - recall: 0.9914 - auc: 0.9989 - val_loss: 0.0038 - val_tp: 186829.0000 - val_fp: 1610.0000 - val_tn: 2637166.0000 - val_fn: 1655.0000 - val_accuracy: 0.9988 - val_precision: 0.9915 - val_recall: 0.9912 - val_auc: 0.9987\n",
            "Epoch 78/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0035 - tp: 446043.0909 - fp: 3733.3737 - tn: 6294312.0000 - fn: 3817.2929 - accuracy: 0.9989 - precision: 0.9918 - recall: 0.9916 - auc: 0.9989 - val_loss: 0.0039 - val_tp: 186731.0000 - val_fp: 1708.0000 - val_tn: 2637068.0000 - val_fn: 1753.0000 - val_accuracy: 0.9988 - val_precision: 0.9909 - val_recall: 0.9907 - val_auc: 0.9988\n",
            "Epoch 79/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0036 - tp: 446074.0202 - fp: 3701.1414 - tn: 6294344.2323 - fn: 3786.3636 - accuracy: 0.9989 - precision: 0.9917 - recall: 0.9915 - auc: 0.9989 - val_loss: 0.0039 - val_tp: 186760.0000 - val_fp: 1662.0000 - val_tn: 2637114.0000 - val_fn: 1724.0000 - val_accuracy: 0.9988 - val_precision: 0.9912 - val_recall: 0.9909 - val_auc: 0.9987\n",
            "Epoch 80/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0035 - tp: 446108.1313 - fp: 3651.4141 - tn: 6294393.9596 - fn: 3752.2525 - accuracy: 0.9989 - precision: 0.9919 - recall: 0.9917 - auc: 0.9989 - val_loss: 0.0038 - val_tp: 186775.0000 - val_fp: 1669.0000 - val_tn: 2637107.0000 - val_fn: 1709.0000 - val_accuracy: 0.9988 - val_precision: 0.9911 - val_recall: 0.9909 - val_auc: 0.9989\n",
            "Epoch 81/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0035 - tp: 446132.3232 - fp: 3646.3939 - tn: 6294398.9798 - fn: 3728.0606 - accuracy: 0.9989 - precision: 0.9919 - recall: 0.9917 - auc: 0.9989 - val_loss: 0.0038 - val_tp: 186735.0000 - val_fp: 1693.0000 - val_tn: 2637083.0000 - val_fn: 1749.0000 - val_accuracy: 0.9988 - val_precision: 0.9910 - val_recall: 0.9907 - val_auc: 0.9989\n",
            "Epoch 82/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0035 - tp: 446101.5253 - fp: 3679.5253 - tn: 6294365.8485 - fn: 3758.8586 - accuracy: 0.9989 - precision: 0.9918 - recall: 0.9916 - auc: 0.9990 - val_loss: 0.0036 - val_tp: 186873.0000 - val_fp: 1560.0000 - val_tn: 2637216.0000 - val_fn: 1611.0000 - val_accuracy: 0.9989 - val_precision: 0.9917 - val_recall: 0.9915 - val_auc: 0.9989\n",
            "Epoch 83/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0035 - tp: 446108.6970 - fp: 3667.7677 - tn: 6294377.6061 - fn: 3751.6869 - accuracy: 0.9989 - precision: 0.9919 - recall: 0.9917 - auc: 0.9989 - val_loss: 0.0037 - val_tp: 186816.0000 - val_fp: 1611.0000 - val_tn: 2637165.0000 - val_fn: 1668.0000 - val_accuracy: 0.9988 - val_precision: 0.9915 - val_recall: 0.9912 - val_auc: 0.9987\n",
            "Epoch 84/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0034 - tp: 446125.3131 - fp: 3651.8687 - tn: 6294393.5051 - fn: 3735.0707 - accuracy: 0.9989 - precision: 0.9919 - recall: 0.9917 - auc: 0.9989 - val_loss: 0.0036 - val_tp: 186846.0000 - val_fp: 1587.0000 - val_tn: 2637189.0000 - val_fn: 1638.0000 - val_accuracy: 0.9989 - val_precision: 0.9916 - val_recall: 0.9913 - val_auc: 0.9990\n",
            "Epoch 85/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0034 - tp: 446207.1212 - fp: 3567.8586 - tn: 6294477.5152 - fn: 3653.2626 - accuracy: 0.9989 - precision: 0.9921 - recall: 0.9919 - auc: 0.9990 - val_loss: 0.0037 - val_tp: 186794.0000 - val_fp: 1642.0000 - val_tn: 2637134.0000 - val_fn: 1690.0000 - val_accuracy: 0.9988 - val_precision: 0.9913 - val_recall: 0.9910 - val_auc: 0.9987\n",
            "Epoch 86/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0034 - tp: 446194.3232 - fp: 3591.9798 - tn: 6294453.3939 - fn: 3666.0606 - accuracy: 0.9989 - precision: 0.9920 - recall: 0.9919 - auc: 0.9989 - val_loss: 0.0038 - val_tp: 186624.0000 - val_fp: 1813.0000 - val_tn: 2636963.0000 - val_fn: 1860.0000 - val_accuracy: 0.9987 - val_precision: 0.9904 - val_recall: 0.9901 - val_auc: 0.9986\n",
            "Epoch 87/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0033 - tp: 446194.7879 - fp: 3577.1111 - tn: 6294468.2626 - fn: 3665.5960 - accuracy: 0.9989 - precision: 0.9920 - recall: 0.9918 - auc: 0.9990 - val_loss: 0.0036 - val_tp: 186760.0000 - val_fp: 1681.0000 - val_tn: 2637095.0000 - val_fn: 1724.0000 - val_accuracy: 0.9988 - val_precision: 0.9911 - val_recall: 0.9909 - val_auc: 0.9987\n",
            "Epoch 88/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0033 - tp: 446233.7374 - fp: 3526.4646 - tn: 6294518.9091 - fn: 3626.6465 - accuracy: 0.9989 - precision: 0.9921 - recall: 0.9919 - auc: 0.9990 - val_loss: 0.0037 - val_tp: 186661.0000 - val_fp: 1765.0000 - val_tn: 2637011.0000 - val_fn: 1823.0000 - val_accuracy: 0.9987 - val_precision: 0.9906 - val_recall: 0.9903 - val_auc: 0.9987\n",
            "Epoch 89/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0033 - tp: 446210.8586 - fp: 3571.7374 - tn: 6294473.6364 - fn: 3649.5253 - accuracy: 0.9989 - precision: 0.9920 - recall: 0.9918 - auc: 0.9989 - val_loss: 0.0037 - val_tp: 186797.0000 - val_fp: 1636.0000 - val_tn: 2637140.0000 - val_fn: 1687.0000 - val_accuracy: 0.9988 - val_precision: 0.9913 - val_recall: 0.9910 - val_auc: 0.9986\n",
            "Epoch 90/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0033 - tp: 446294.9495 - fp: 3483.3434 - tn: 6294562.0303 - fn: 3565.4343 - accuracy: 0.9990 - precision: 0.9923 - recall: 0.9921 - auc: 0.9989 - val_loss: 0.0036 - val_tp: 186748.0000 - val_fp: 1688.0000 - val_tn: 2637088.0000 - val_fn: 1736.0000 - val_accuracy: 0.9988 - val_precision: 0.9910 - val_recall: 0.9908 - val_auc: 0.9987\n",
            "Epoch 91/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0033 - tp: 446216.6566 - fp: 3568.4343 - tn: 6294476.9394 - fn: 3643.7273 - accuracy: 0.9989 - precision: 0.9920 - recall: 0.9919 - auc: 0.9990 - val_loss: 0.0035 - val_tp: 186941.0000 - val_fp: 1493.0000 - val_tn: 2637283.0000 - val_fn: 1543.0000 - val_accuracy: 0.9989 - val_precision: 0.9921 - val_recall: 0.9918 - val_auc: 0.9989\n",
            "Epoch 92/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0032 - tp: 446349.0505 - fp: 3411.4040 - tn: 6294633.9697 - fn: 3511.3333 - accuracy: 0.9990 - precision: 0.9925 - recall: 0.9923 - auc: 0.9990 - val_loss: 0.0036 - val_tp: 186842.0000 - val_fp: 1596.0000 - val_tn: 2637180.0000 - val_fn: 1642.0000 - val_accuracy: 0.9989 - val_precision: 0.9915 - val_recall: 0.9913 - val_auc: 0.9990\n",
            "Epoch 93/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0032 - tp: 446342.0909 - fp: 3422.0000 - tn: 6294623.3737 - fn: 3518.2929 - accuracy: 0.9990 - precision: 0.9924 - recall: 0.9922 - auc: 0.9990 - val_loss: 0.0034 - val_tp: 186939.0000 - val_fp: 1511.0000 - val_tn: 2637265.0000 - val_fn: 1545.0000 - val_accuracy: 0.9989 - val_precision: 0.9920 - val_recall: 0.9918 - val_auc: 0.9989\n",
            "Epoch 94/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0032 - tp: 446355.1010 - fp: 3422.0303 - tn: 6294623.3434 - fn: 3505.2828 - accuracy: 0.9990 - precision: 0.9924 - recall: 0.9922 - auc: 0.9990 - val_loss: 0.0036 - val_tp: 186735.0000 - val_fp: 1716.0000 - val_tn: 2637060.0000 - val_fn: 1749.0000 - val_accuracy: 0.9988 - val_precision: 0.9909 - val_recall: 0.9907 - val_auc: 0.9987\n",
            "Epoch 95/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0032 - tp: 446355.6061 - fp: 3432.2020 - tn: 6294613.1717 - fn: 3504.7778 - accuracy: 0.9990 - precision: 0.9924 - recall: 0.9922 - auc: 0.9990 - val_loss: 0.0034 - val_tp: 186897.0000 - val_fp: 1549.0000 - val_tn: 2637227.0000 - val_fn: 1587.0000 - val_accuracy: 0.9989 - val_precision: 0.9918 - val_recall: 0.9916 - val_auc: 0.9991\n",
            "Epoch 96/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0032 - tp: 446407.4040 - fp: 3382.3030 - tn: 6294663.0707 - fn: 3452.9798 - accuracy: 0.9990 - precision: 0.9925 - recall: 0.9924 - auc: 0.9990 - val_loss: 0.0034 - val_tp: 186946.0000 - val_fp: 1520.0000 - val_tn: 2637256.0000 - val_fn: 1538.0000 - val_accuracy: 0.9989 - val_precision: 0.9919 - val_recall: 0.9918 - val_auc: 0.9990\n",
            "Epoch 97/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0032 - tp: 446415.6970 - fp: 3376.0303 - tn: 6294669.3434 - fn: 3444.6869 - accuracy: 0.9990 - precision: 0.9925 - recall: 0.9924 - auc: 0.9990 - val_loss: 0.0033 - val_tp: 187024.0000 - val_fp: 1426.0000 - val_tn: 2637350.0000 - val_fn: 1460.0000 - val_accuracy: 0.9990 - val_precision: 0.9924 - val_recall: 0.9923 - val_auc: 0.9989\n",
            "Epoch 98/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0032 - tp: 446436.4949 - fp: 3356.6364 - tn: 6294688.7374 - fn: 3423.8889 - accuracy: 0.9990 - precision: 0.9925 - recall: 0.9923 - auc: 0.9990 - val_loss: 0.0033 - val_tp: 186950.0000 - val_fp: 1505.0000 - val_tn: 2637271.0000 - val_fn: 1534.0000 - val_accuracy: 0.9989 - val_precision: 0.9920 - val_recall: 0.9919 - val_auc: 0.9989\n",
            "Epoch 99/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0031 - tp: 446463.0707 - fp: 3331.1717 - tn: 6294714.2020 - fn: 3397.3131 - accuracy: 0.9990 - precision: 0.9926 - recall: 0.9924 - auc: 0.9990 - val_loss: 0.0034 - val_tp: 186952.0000 - val_fp: 1503.0000 - val_tn: 2637273.0000 - val_fn: 1532.0000 - val_accuracy: 0.9989 - val_precision: 0.9920 - val_recall: 0.9919 - val_auc: 0.9991\n",
            "Epoch 100/300\n",
            "98/98 [==============================] - 2s 25ms/step - loss: 0.0031 - tp: 446484.7273 - fp: 3302.6566 - tn: 6294742.7172 - fn: 3375.6566 - accuracy: 0.9990 - precision: 0.9927 - recall: 0.9925 - auc: 0.9990 - val_loss: 0.0034 - val_tp: 186951.0000 - val_fp: 1513.0000 - val_tn: 2637263.0000 - val_fn: 1533.0000 - val_accuracy: 0.9989 - val_precision: 0.9920 - val_recall: 0.9919 - val_auc: 0.9990\n",
            "Epoch 101/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0031 - tp: 446483.1919 - fp: 3312.2525 - tn: 6294733.1212 - fn: 3377.1919 - accuracy: 0.9990 - precision: 0.9927 - recall: 0.9925 - auc: 0.9990 - val_loss: 0.0033 - val_tp: 186910.0000 - val_fp: 1546.0000 - val_tn: 2637230.0000 - val_fn: 1574.0000 - val_accuracy: 0.9989 - val_precision: 0.9918 - val_recall: 0.9916 - val_auc: 0.9988\n",
            "Epoch 102/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0031 - tp: 446517.5354 - fp: 3276.3131 - tn: 6294769.0606 - fn: 3342.8485 - accuracy: 0.9990 - precision: 0.9927 - recall: 0.9925 - auc: 0.9990 - val_loss: 0.0033 - val_tp: 186974.0000 - val_fp: 1484.0000 - val_tn: 2637292.0000 - val_fn: 1510.0000 - val_accuracy: 0.9989 - val_precision: 0.9921 - val_recall: 0.9920 - val_auc: 0.9989\n",
            "Epoch 103/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0030 - tp: 446489.0000 - fp: 3305.2727 - tn: 6294740.1010 - fn: 3371.3838 - accuracy: 0.9990 - precision: 0.9926 - recall: 0.9925 - auc: 0.9991 - val_loss: 0.0033 - val_tp: 186984.0000 - val_fp: 1476.0000 - val_tn: 2637300.0000 - val_fn: 1500.0000 - val_accuracy: 0.9989 - val_precision: 0.9922 - val_recall: 0.9920 - val_auc: 0.9989\n",
            "Epoch 104/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0030 - tp: 446539.2121 - fp: 3245.9394 - tn: 6294799.4343 - fn: 3321.1717 - accuracy: 0.9990 - precision: 0.9927 - recall: 0.9925 - auc: 0.9990 - val_loss: 0.0033 - val_tp: 187018.0000 - val_fp: 1449.0000 - val_tn: 2637327.0000 - val_fn: 1466.0000 - val_accuracy: 0.9990 - val_precision: 0.9923 - val_recall: 0.9922 - val_auc: 0.9988\n",
            "Epoch 105/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0030 - tp: 446590.3434 - fp: 3203.6970 - tn: 6294841.6768 - fn: 3270.0404 - accuracy: 0.9990 - precision: 0.9929 - recall: 0.9928 - auc: 0.9990 - val_loss: 0.0032 - val_tp: 187019.0000 - val_fp: 1436.0000 - val_tn: 2637340.0000 - val_fn: 1465.0000 - val_accuracy: 0.9990 - val_precision: 0.9924 - val_recall: 0.9922 - val_auc: 0.9989\n",
            "Epoch 106/300\n",
            "98/98 [==============================] - 2s 26ms/step - loss: 0.0031 - tp: 446556.0707 - fp: 3248.2424 - tn: 6294797.1313 - fn: 3304.3131 - accuracy: 0.9990 - precision: 0.9928 - recall: 0.9926 - auc: 0.9990 - val_loss: 0.0034 - val_tp: 186958.0000 - val_fp: 1506.0000 - val_tn: 2637270.0000 - val_fn: 1526.0000 - val_accuracy: 0.9989 - val_precision: 0.9920 - val_recall: 0.9919 - val_auc: 0.9989\n",
            "Epoch 107/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0030 - tp: 446531.0606 - fp: 3269.2323 - tn: 6294776.1414 - fn: 3329.3232 - accuracy: 0.9990 - precision: 0.9928 - recall: 0.9927 - auc: 0.9990 - val_loss: 0.0032 - val_tp: 187021.0000 - val_fp: 1443.0000 - val_tn: 2637333.0000 - val_fn: 1463.0000 - val_accuracy: 0.9990 - val_precision: 0.9923 - val_recall: 0.9922 - val_auc: 0.9990\n",
            "Epoch 108/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0030 - tp: 446559.6263 - fp: 3245.1414 - tn: 6294800.2323 - fn: 3300.7576 - accuracy: 0.9990 - precision: 0.9927 - recall: 0.9926 - auc: 0.9990 - val_loss: 0.0032 - val_tp: 187055.0000 - val_fp: 1399.0000 - val_tn: 2637377.0000 - val_fn: 1429.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9924 - val_auc: 0.9990\n",
            "Epoch 109/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446594.5253 - fp: 3210.3131 - tn: 6294835.0606 - fn: 3265.8586 - accuracy: 0.9990 - precision: 0.9929 - recall: 0.9928 - auc: 0.9990 - val_loss: 0.0033 - val_tp: 186978.0000 - val_fp: 1488.0000 - val_tn: 2637288.0000 - val_fn: 1506.0000 - val_accuracy: 0.9989 - val_precision: 0.9921 - val_recall: 0.9920 - val_auc: 0.9988\n",
            "Epoch 110/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0030 - tp: 446562.0101 - fp: 3245.3333 - tn: 6294800.0404 - fn: 3298.3737 - accuracy: 0.9990 - precision: 0.9928 - recall: 0.9926 - auc: 0.9991 - val_loss: 0.0032 - val_tp: 187000.0000 - val_fp: 1465.0000 - val_tn: 2637311.0000 - val_fn: 1484.0000 - val_accuracy: 0.9990 - val_precision: 0.9922 - val_recall: 0.9921 - val_auc: 0.9988\n",
            "Epoch 111/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446609.1414 - fp: 3189.8081 - tn: 6294855.5657 - fn: 3251.2424 - accuracy: 0.9990 - precision: 0.9929 - recall: 0.9928 - auc: 0.9990 - val_loss: 0.0032 - val_tp: 187029.0000 - val_fp: 1435.0000 - val_tn: 2637341.0000 - val_fn: 1455.0000 - val_accuracy: 0.9990 - val_precision: 0.9924 - val_recall: 0.9923 - val_auc: 0.9990\n",
            "Epoch 112/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446577.9495 - fp: 3221.1010 - tn: 6294824.2727 - fn: 3282.4343 - accuracy: 0.9990 - precision: 0.9928 - recall: 0.9927 - auc: 0.9990 - val_loss: 0.0031 - val_tp: 187081.0000 - val_fp: 1385.0000 - val_tn: 2637391.0000 - val_fn: 1403.0000 - val_accuracy: 0.9990 - val_precision: 0.9927 - val_recall: 0.9926 - val_auc: 0.9989\n",
            "Epoch 113/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446654.1919 - fp: 3157.9293 - tn: 6294887.4444 - fn: 3206.1919 - accuracy: 0.9991 - precision: 0.9931 - recall: 0.9930 - auc: 0.9990 - val_loss: 0.0032 - val_tp: 187001.0000 - val_fp: 1460.0000 - val_tn: 2637316.0000 - val_fn: 1483.0000 - val_accuracy: 0.9990 - val_precision: 0.9923 - val_recall: 0.9921 - val_auc: 0.9988\n",
            "Epoch 114/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0030 - tp: 446628.2525 - fp: 3187.9596 - tn: 6294857.4141 - fn: 3232.1313 - accuracy: 0.9990 - precision: 0.9929 - recall: 0.9928 - auc: 0.9990 - val_loss: 0.0031 - val_tp: 187005.0000 - val_fp: 1458.0000 - val_tn: 2637318.0000 - val_fn: 1479.0000 - val_accuracy: 0.9990 - val_precision: 0.9923 - val_recall: 0.9922 - val_auc: 0.9989\n",
            "Epoch 115/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446645.5758 - fp: 3162.3636 - tn: 6294883.0101 - fn: 3214.8081 - accuracy: 0.9991 - precision: 0.9930 - recall: 0.9929 - auc: 0.9991 - val_loss: 0.0031 - val_tp: 187069.0000 - val_fp: 1398.0000 - val_tn: 2637378.0000 - val_fn: 1415.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9925 - val_auc: 0.9990\n",
            "Epoch 116/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446624.2626 - fp: 3184.1212 - tn: 6294861.2525 - fn: 3236.1212 - accuracy: 0.9990 - precision: 0.9929 - recall: 0.9928 - auc: 0.9991 - val_loss: 0.0031 - val_tp: 187088.0000 - val_fp: 1374.0000 - val_tn: 2637402.0000 - val_fn: 1396.0000 - val_accuracy: 0.9990 - val_precision: 0.9927 - val_recall: 0.9926 - val_auc: 0.9990\n",
            "Epoch 117/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446633.8889 - fp: 3181.9798 - tn: 6294863.3939 - fn: 3226.4949 - accuracy: 0.9991 - precision: 0.9929 - recall: 0.9928 - auc: 0.9990 - val_loss: 0.0031 - val_tp: 187045.0000 - val_fp: 1418.0000 - val_tn: 2637358.0000 - val_fn: 1439.0000 - val_accuracy: 0.9990 - val_precision: 0.9925 - val_recall: 0.9924 - val_auc: 0.9990\n",
            "Epoch 118/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446632.3939 - fp: 3180.1010 - tn: 6294865.2727 - fn: 3227.9899 - accuracy: 0.9990 - precision: 0.9929 - recall: 0.9928 - auc: 0.9991 - val_loss: 0.0032 - val_tp: 186938.0000 - val_fp: 1525.0000 - val_tn: 2637251.0000 - val_fn: 1546.0000 - val_accuracy: 0.9989 - val_precision: 0.9919 - val_recall: 0.9918 - val_auc: 0.9988\n",
            "Epoch 119/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446712.2424 - fp: 3091.7879 - tn: 6294953.5859 - fn: 3148.1414 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9990 - val_loss: 0.0031 - val_tp: 187073.0000 - val_fp: 1391.0000 - val_tn: 2637385.0000 - val_fn: 1411.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9925 - val_auc: 0.9990\n",
            "Epoch 120/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446662.5253 - fp: 3150.0707 - tn: 6294895.3030 - fn: 3197.8586 - accuracy: 0.9991 - precision: 0.9930 - recall: 0.9929 - auc: 0.9991 - val_loss: 0.0031 - val_tp: 187047.0000 - val_fp: 1416.0000 - val_tn: 2637360.0000 - val_fn: 1437.0000 - val_accuracy: 0.9990 - val_precision: 0.9925 - val_recall: 0.9924 - val_auc: 0.9992\n",
            "Epoch 121/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446675.2727 - fp: 3137.2424 - tn: 6294908.1313 - fn: 3185.1111 - accuracy: 0.9991 - precision: 0.9930 - recall: 0.9929 - auc: 0.9990 - val_loss: 0.0030 - val_tp: 187102.0000 - val_fp: 1366.0000 - val_tn: 2637410.0000 - val_fn: 1382.0000 - val_accuracy: 0.9990 - val_precision: 0.9928 - val_recall: 0.9927 - val_auc: 0.9991\n",
            "Epoch 122/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446603.6566 - fp: 3198.7475 - tn: 6294846.6263 - fn: 3256.7273 - accuracy: 0.9990 - precision: 0.9929 - recall: 0.9928 - auc: 0.9991 - val_loss: 0.0031 - val_tp: 187094.0000 - val_fp: 1369.0000 - val_tn: 2637407.0000 - val_fn: 1390.0000 - val_accuracy: 0.9990 - val_precision: 0.9927 - val_recall: 0.9926 - val_auc: 0.9989\n",
            "Epoch 123/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446726.5152 - fp: 3085.5556 - tn: 6294959.8182 - fn: 3133.8687 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9991 - val_loss: 0.0030 - val_tp: 187059.0000 - val_fp: 1404.0000 - val_tn: 2637372.0000 - val_fn: 1425.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9924 - val_auc: 0.9989\n",
            "Epoch 124/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446667.6667 - fp: 3142.1616 - tn: 6294903.2121 - fn: 3192.7172 - accuracy: 0.9990 - precision: 0.9929 - recall: 0.9928 - auc: 0.9990 - val_loss: 0.0031 - val_tp: 186997.0000 - val_fp: 1463.0000 - val_tn: 2637313.0000 - val_fn: 1487.0000 - val_accuracy: 0.9990 - val_precision: 0.9922 - val_recall: 0.9921 - val_auc: 0.9988\n",
            "Epoch 125/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446727.3838 - fp: 3085.8687 - tn: 6294959.5051 - fn: 3133.0000 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9990 - val_loss: 0.0030 - val_tp: 187066.0000 - val_fp: 1393.0000 - val_tn: 2637383.0000 - val_fn: 1418.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9925 - val_auc: 0.9991\n",
            "Epoch 126/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446696.4646 - fp: 3113.3939 - tn: 6294931.9798 - fn: 3163.9192 - accuracy: 0.9991 - precision: 0.9930 - recall: 0.9929 - auc: 0.9990 - val_loss: 0.0030 - val_tp: 187133.0000 - val_fp: 1333.0000 - val_tn: 2637443.0000 - val_fn: 1351.0000 - val_accuracy: 0.9991 - val_precision: 0.9929 - val_recall: 0.9928 - val_auc: 0.9990\n",
            "Epoch 127/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446731.4040 - fp: 3088.4949 - tn: 6294956.8788 - fn: 3128.9798 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9991 - val_loss: 0.0030 - val_tp: 187157.0000 - val_fp: 1312.0000 - val_tn: 2637464.0000 - val_fn: 1327.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9989\n",
            "Epoch 128/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446725.5960 - fp: 3087.4242 - tn: 6294957.9495 - fn: 3134.7879 - accuracy: 0.9991 - precision: 0.9931 - recall: 0.9930 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187128.0000 - val_fp: 1336.0000 - val_tn: 2637440.0000 - val_fn: 1356.0000 - val_accuracy: 0.9990 - val_precision: 0.9929 - val_recall: 0.9928 - val_auc: 0.9990\n",
            "Epoch 129/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0029 - tp: 446685.1111 - fp: 3131.1212 - tn: 6294914.2525 - fn: 3175.2727 - accuracy: 0.9991 - precision: 0.9930 - recall: 0.9929 - auc: 0.9990 - val_loss: 0.0030 - val_tp: 187036.0000 - val_fp: 1428.0000 - val_tn: 2637348.0000 - val_fn: 1448.0000 - val_accuracy: 0.9990 - val_precision: 0.9924 - val_recall: 0.9923 - val_auc: 0.9990\n",
            "Epoch 130/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446727.6566 - fp: 3098.0000 - tn: 6294947.3737 - fn: 3132.7273 - accuracy: 0.9991 - precision: 0.9931 - recall: 0.9931 - auc: 0.9991 - val_loss: 0.0032 - val_tp: 186924.0000 - val_fp: 1537.0000 - val_tn: 2637239.0000 - val_fn: 1560.0000 - val_accuracy: 0.9989 - val_precision: 0.9918 - val_recall: 0.9917 - val_auc: 0.9988\n",
            "Epoch 131/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446773.0505 - fp: 3045.6263 - tn: 6294999.7475 - fn: 3087.3333 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9990 - val_loss: 0.0031 - val_tp: 187101.0000 - val_fp: 1365.0000 - val_tn: 2637411.0000 - val_fn: 1383.0000 - val_accuracy: 0.9990 - val_precision: 0.9928 - val_recall: 0.9927 - val_auc: 0.9990\n",
            "Epoch 132/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446707.0808 - fp: 3118.4646 - tn: 6294926.9091 - fn: 3153.3030 - accuracy: 0.9991 - precision: 0.9931 - recall: 0.9930 - auc: 0.9990 - val_loss: 0.0030 - val_tp: 187078.0000 - val_fp: 1389.0000 - val_tn: 2637387.0000 - val_fn: 1406.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9925 - val_auc: 0.9989\n",
            "Epoch 133/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446712.3030 - fp: 3107.9596 - tn: 6294937.4141 - fn: 3148.0808 - accuracy: 0.9991 - precision: 0.9931 - recall: 0.9930 - auc: 0.9990 - val_loss: 0.0029 - val_tp: 187162.0000 - val_fp: 1308.0000 - val_tn: 2637468.0000 - val_fn: 1322.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9930 - val_auc: 0.9989\n",
            "Epoch 134/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446716.2929 - fp: 3099.8182 - tn: 6294945.5556 - fn: 3144.0909 - accuracy: 0.9991 - precision: 0.9931 - recall: 0.9930 - auc: 0.9990 - val_loss: 0.0031 - val_tp: 187036.0000 - val_fp: 1425.0000 - val_tn: 2637351.0000 - val_fn: 1448.0000 - val_accuracy: 0.9990 - val_precision: 0.9924 - val_recall: 0.9923 - val_auc: 0.9990\n",
            "Epoch 135/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446744.7475 - fp: 3075.5758 - tn: 6294969.7980 - fn: 3115.6364 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187158.0000 - val_fp: 1318.0000 - val_tn: 2637458.0000 - val_fn: 1326.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9990\n",
            "Epoch 136/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446773.2828 - fp: 3053.5455 - tn: 6294991.8283 - fn: 3087.1010 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9990 - val_loss: 0.0030 - val_tp: 187122.0000 - val_fp: 1345.0000 - val_tn: 2637431.0000 - val_fn: 1362.0000 - val_accuracy: 0.9990 - val_precision: 0.9929 - val_recall: 0.9928 - val_auc: 0.9988\n",
            "Epoch 137/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446784.0101 - fp: 3034.7475 - tn: 6295010.6263 - fn: 3076.3737 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9991 - val_loss: 0.0030 - val_tp: 187052.0000 - val_fp: 1416.0000 - val_tn: 2637360.0000 - val_fn: 1432.0000 - val_accuracy: 0.9990 - val_precision: 0.9925 - val_recall: 0.9924 - val_auc: 0.9988\n",
            "Epoch 138/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446759.2525 - fp: 3062.6768 - tn: 6294982.6970 - fn: 3101.1313 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9991 - val_loss: 0.0030 - val_tp: 187040.0000 - val_fp: 1433.0000 - val_tn: 2637343.0000 - val_fn: 1444.0000 - val_accuracy: 0.9990 - val_precision: 0.9924 - val_recall: 0.9923 - val_auc: 0.9988\n",
            "Epoch 139/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446784.4747 - fp: 3044.7980 - tn: 6295000.5758 - fn: 3075.9091 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9932 - auc: 0.9990 - val_loss: 0.0028 - val_tp: 187190.0000 - val_fp: 1278.0000 - val_tn: 2637498.0000 - val_fn: 1294.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9931 - val_auc: 0.9990\n",
            "Epoch 140/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446806.3939 - fp: 3021.6667 - tn: 6295023.7071 - fn: 3053.9899 - accuracy: 0.9991 - precision: 0.9933 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187208.0000 - val_fp: 1268.0000 - val_tn: 2637508.0000 - val_fn: 1276.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9991\n",
            "Epoch 141/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446786.9899 - fp: 3040.1818 - tn: 6295005.1919 - fn: 3073.3939 - accuracy: 0.9991 - precision: 0.9932 - recall: 0.9931 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187152.0000 - val_fp: 1320.0000 - val_tn: 2637456.0000 - val_fn: 1332.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9929 - val_auc: 0.9989\n",
            "Epoch 142/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446823.7374 - fp: 3005.1111 - tn: 6295040.2626 - fn: 3036.6465 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187161.0000 - val_fp: 1313.0000 - val_tn: 2637463.0000 - val_fn: 1323.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9990\n",
            "Epoch 143/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0027 - tp: 446829.0000 - fp: 2994.2727 - tn: 6295051.1010 - fn: 3031.3838 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0030 - val_tp: 187067.0000 - val_fp: 1406.0000 - val_tn: 2637370.0000 - val_fn: 1417.0000 - val_accuracy: 0.9990 - val_precision: 0.9925 - val_recall: 0.9925 - val_auc: 0.9988\n",
            "Epoch 144/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446759.7980 - fp: 3061.2626 - tn: 6294984.1111 - fn: 3100.5859 - accuracy: 0.9991 - precision: 0.9931 - recall: 0.9931 - auc: 0.9991 - val_loss: 0.0030 - val_tp: 187091.0000 - val_fp: 1374.0000 - val_tn: 2637402.0000 - val_fn: 1393.0000 - val_accuracy: 0.9990 - val_precision: 0.9927 - val_recall: 0.9926 - val_auc: 0.9991\n",
            "Epoch 145/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446812.2727 - fp: 3016.4040 - tn: 6295028.9697 - fn: 3048.1111 - accuracy: 0.9991 - precision: 0.9933 - recall: 0.9932 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187085.0000 - val_fp: 1388.0000 - val_tn: 2637388.0000 - val_fn: 1399.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9926 - val_auc: 0.9989\n",
            "Epoch 146/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0027 - tp: 446821.1717 - fp: 3006.8990 - tn: 6295038.4747 - fn: 3039.2121 - accuracy: 0.9991 - precision: 0.9933 - recall: 0.9932 - auc: 0.9990 - val_loss: 0.0028 - val_tp: 187203.0000 - val_fp: 1262.0000 - val_tn: 2637514.0000 - val_fn: 1281.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9990\n",
            "Epoch 147/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446853.7677 - fp: 2976.0303 - tn: 6295069.3434 - fn: 3006.6162 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187123.0000 - val_fp: 1349.0000 - val_tn: 2637427.0000 - val_fn: 1361.0000 - val_accuracy: 0.9990 - val_precision: 0.9928 - val_recall: 0.9928 - val_auc: 0.9991\n",
            "Epoch 148/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0028 - tp: 446831.2626 - fp: 2994.3030 - tn: 6295051.0707 - fn: 3029.1212 - accuracy: 0.9991 - precision: 0.9933 - recall: 0.9932 - auc: 0.9990 - val_loss: 0.0028 - val_tp: 187200.0000 - val_fp: 1276.0000 - val_tn: 2637500.0000 - val_fn: 1284.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9932 - val_auc: 0.9989\n",
            "Epoch 149/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446824.3535 - fp: 2997.9394 - tn: 6295047.4343 - fn: 3036.0303 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187208.0000 - val_fp: 1266.0000 - val_tn: 2637510.0000 - val_fn: 1276.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9989\n",
            "Epoch 150/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446857.8788 - fp: 2972.1818 - tn: 6295073.1919 - fn: 3002.5051 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9934 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187161.0000 - val_fp: 1308.0000 - val_tn: 2637468.0000 - val_fn: 1323.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9930 - val_auc: 0.9991\n",
            "Epoch 151/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446847.1616 - fp: 2977.4343 - tn: 6295067.9394 - fn: 3013.2222 - accuracy: 0.9991 - precision: 0.9933 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187196.0000 - val_fp: 1270.0000 - val_tn: 2637506.0000 - val_fn: 1288.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9990\n",
            "Epoch 152/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446882.5859 - fp: 2951.9798 - tn: 6295093.3939 - fn: 2977.7980 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9934 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187116.0000 - val_fp: 1351.0000 - val_tn: 2637425.0000 - val_fn: 1368.0000 - val_accuracy: 0.9990 - val_precision: 0.9928 - val_recall: 0.9927 - val_auc: 0.9991\n",
            "Epoch 153/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446860.6162 - fp: 2968.0202 - tn: 6295077.3535 - fn: 2999.7677 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9934 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187161.0000 - val_fp: 1320.0000 - val_tn: 2637456.0000 - val_fn: 1323.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9989\n",
            "Epoch 154/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446907.9091 - fp: 2928.2121 - tn: 6295117.1616 - fn: 2952.4747 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187166.0000 - val_fp: 1305.0000 - val_tn: 2637471.0000 - val_fn: 1318.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9930 - val_auc: 0.9989\n",
            "Epoch 155/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446811.1010 - fp: 3028.2020 - tn: 6295017.1717 - fn: 3049.2828 - accuracy: 0.9991 - precision: 0.9933 - recall: 0.9932 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187196.0000 - val_fp: 1274.0000 - val_tn: 2637502.0000 - val_fn: 1288.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9932 - val_auc: 0.9989\n",
            "Epoch 156/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446941.7273 - fp: 2892.2828 - tn: 6295153.0909 - fn: 2918.6566 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187159.0000 - val_fp: 1313.0000 - val_tn: 2637463.0000 - val_fn: 1325.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9989\n",
            "Epoch 157/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446915.3030 - fp: 2920.9697 - tn: 6295124.4040 - fn: 2945.0808 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187212.0000 - val_fp: 1259.0000 - val_tn: 2637517.0000 - val_fn: 1272.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9933 - val_auc: 0.9991\n",
            "Epoch 158/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0026 - tp: 446873.5657 - fp: 2960.2222 - tn: 6295085.1515 - fn: 2986.8182 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187192.0000 - val_fp: 1288.0000 - val_tn: 2637488.0000 - val_fn: 1292.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9931 - val_auc: 0.9990\n",
            "Epoch 159/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446940.7172 - fp: 2892.6667 - tn: 6295152.7071 - fn: 2919.6667 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187228.0000 - val_fp: 1251.0000 - val_tn: 2637525.0000 - val_fn: 1256.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9933 - val_auc: 0.9991\n",
            "Epoch 160/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446864.6263 - fp: 2972.8687 - tn: 6295072.5051 - fn: 2995.7576 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187179.0000 - val_fp: 1296.0000 - val_tn: 2637480.0000 - val_fn: 1305.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9931 - val_auc: 0.9989\n",
            "Epoch 161/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446895.5455 - fp: 2944.4848 - tn: 6295100.8889 - fn: 2964.8384 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9934 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187186.0000 - val_fp: 1288.0000 - val_tn: 2637488.0000 - val_fn: 1298.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9931 - val_auc: 0.9989\n",
            "Epoch 162/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446858.9091 - fp: 2976.6061 - tn: 6295068.7677 - fn: 3001.4747 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9934 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187207.0000 - val_fp: 1270.0000 - val_tn: 2637506.0000 - val_fn: 1277.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9991\n",
            "Epoch 163/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446938.4646 - fp: 2892.4545 - tn: 6295152.9192 - fn: 2921.9192 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187144.0000 - val_fp: 1326.0000 - val_tn: 2637450.0000 - val_fn: 1340.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9929 - val_auc: 0.9992\n",
            "Epoch 164/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446905.3535 - fp: 2936.6768 - tn: 6295108.6970 - fn: 2955.0303 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9934 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187234.0000 - val_fp: 1242.0000 - val_tn: 2637534.0000 - val_fn: 1250.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9991\n",
            "Epoch 165/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446971.0303 - fp: 2868.3434 - tn: 6295177.0303 - fn: 2889.3535 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187069.0000 - val_fp: 1401.0000 - val_tn: 2637375.0000 - val_fn: 1415.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9925 - val_auc: 0.9988\n",
            "Epoch 166/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446832.8586 - fp: 3003.7071 - tn: 6295041.6667 - fn: 3027.5253 - accuracy: 0.9991 - precision: 0.9933 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187081.0000 - val_fp: 1394.0000 - val_tn: 2637382.0000 - val_fn: 1403.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9926 - val_auc: 0.9988\n",
            "Epoch 167/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0027 - tp: 446875.6061 - fp: 2953.2222 - tn: 6295092.1515 - fn: 2984.7778 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9933 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187182.0000 - val_fp: 1293.0000 - val_tn: 2637483.0000 - val_fn: 1302.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9931 - val_auc: 0.9989\n",
            "Epoch 168/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446963.0000 - fp: 2873.0303 - tn: 6295172.3434 - fn: 2897.3838 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187194.0000 - val_fp: 1282.0000 - val_tn: 2637494.0000 - val_fn: 1290.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9932 - val_auc: 0.9989\n",
            "Epoch 169/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446945.5152 - fp: 2894.3333 - tn: 6295151.0404 - fn: 2914.8687 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187212.0000 - val_fp: 1260.0000 - val_tn: 2637516.0000 - val_fn: 1272.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9933 - val_auc: 0.9989\n",
            "Epoch 170/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446909.9091 - fp: 2927.8687 - tn: 6295117.5051 - fn: 2950.4747 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9934 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187265.0000 - val_fp: 1212.0000 - val_tn: 2637564.0000 - val_fn: 1219.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9935 - val_auc: 0.9991\n",
            "Epoch 171/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446927.0202 - fp: 2910.5455 - tn: 6295134.8283 - fn: 2933.3636 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187157.0000 - val_fp: 1317.0000 - val_tn: 2637459.0000 - val_fn: 1327.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9991\n",
            "Epoch 172/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446932.4040 - fp: 2904.3636 - tn: 6295141.0101 - fn: 2927.9798 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187174.0000 - val_fp: 1300.0000 - val_tn: 2637476.0000 - val_fn: 1310.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9930 - val_auc: 0.9989\n",
            "Epoch 173/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446947.3434 - fp: 2889.4444 - tn: 6295155.9293 - fn: 2913.0404 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0029 - val_tp: 187142.0000 - val_fp: 1331.0000 - val_tn: 2637445.0000 - val_fn: 1342.0000 - val_accuracy: 0.9991 - val_precision: 0.9929 - val_recall: 0.9929 - val_auc: 0.9991\n",
            "Epoch 174/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446949.0202 - fp: 2892.2828 - tn: 6295153.0909 - fn: 2911.3636 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187172.0000 - val_fp: 1304.0000 - val_tn: 2637472.0000 - val_fn: 1312.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9930 - val_auc: 0.9991\n",
            "Epoch 175/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446940.4747 - fp: 2896.6465 - tn: 6295148.7273 - fn: 2919.9091 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187233.0000 - val_fp: 1240.0000 - val_tn: 2637536.0000 - val_fn: 1251.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 176/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446923.0606 - fp: 2912.8081 - tn: 6295132.5657 - fn: 2937.3232 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187230.0000 - val_fp: 1246.0000 - val_tn: 2637530.0000 - val_fn: 1254.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9933 - val_auc: 0.9990\n",
            "Epoch 177/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446941.9192 - fp: 2896.2929 - tn: 6295149.0808 - fn: 2918.4646 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187163.0000 - val_fp: 1317.0000 - val_tn: 2637459.0000 - val_fn: 1321.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9989\n",
            "Epoch 178/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446983.8283 - fp: 2855.2626 - tn: 6295190.1111 - fn: 2876.5556 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187138.0000 - val_fp: 1339.0000 - val_tn: 2637437.0000 - val_fn: 1346.0000 - val_accuracy: 0.9991 - val_precision: 0.9929 - val_recall: 0.9929 - val_auc: 0.9989\n",
            "Epoch 179/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446938.6263 - fp: 2898.1414 - tn: 6295147.2323 - fn: 2921.7576 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187197.0000 - val_fp: 1280.0000 - val_tn: 2637496.0000 - val_fn: 1287.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9932 - val_auc: 0.9991\n",
            "Epoch 180/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0026 - tp: 446915.7778 - fp: 2915.2222 - tn: 6295130.1515 - fn: 2944.6061 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9934 - auc: 0.9991 - val_loss: 0.0030 - val_tp: 187037.0000 - val_fp: 1441.0000 - val_tn: 2637335.0000 - val_fn: 1447.0000 - val_accuracy: 0.9990 - val_precision: 0.9924 - val_recall: 0.9923 - val_auc: 0.9987\n",
            "Epoch 181/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0026 - tp: 446955.8990 - fp: 2885.2727 - tn: 6295160.1010 - fn: 2904.4848 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9935 - auc: 0.9990 - val_loss: 0.0027 - val_tp: 187260.0000 - val_fp: 1217.0000 - val_tn: 2637559.0000 - val_fn: 1224.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9991\n",
            "Epoch 182/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446980.8081 - fp: 2860.5657 - tn: 6295184.8081 - fn: 2879.5758 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187264.0000 - val_fp: 1215.0000 - val_tn: 2637561.0000 - val_fn: 1220.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9935 - val_auc: 0.9991\n",
            "Epoch 183/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446983.8182 - fp: 2857.2121 - tn: 6295188.1616 - fn: 2876.5657 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187248.0000 - val_fp: 1225.0000 - val_tn: 2637551.0000 - val_fn: 1236.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 184/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447012.3838 - fp: 2827.1313 - tn: 6295218.2424 - fn: 2848.0000 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187204.0000 - val_fp: 1271.0000 - val_tn: 2637505.0000 - val_fn: 1280.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9989\n",
            "Epoch 185/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446985.4848 - fp: 2852.8384 - tn: 6295192.5354 - fn: 2874.8990 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187269.0000 - val_fp: 1207.0000 - val_tn: 2637569.0000 - val_fn: 1215.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9991\n",
            "Epoch 186/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446959.5152 - fp: 2877.4747 - tn: 6295167.8990 - fn: 2900.8687 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187255.0000 - val_fp: 1219.0000 - val_tn: 2637557.0000 - val_fn: 1229.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9990\n",
            "Epoch 187/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447014.9798 - fp: 2834.5253 - tn: 6295210.8485 - fn: 2845.4040 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187162.0000 - val_fp: 1317.0000 - val_tn: 2637459.0000 - val_fn: 1322.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9991\n",
            "Epoch 188/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0026 - tp: 446998.6566 - fp: 2837.3434 - tn: 6295208.0303 - fn: 2861.7273 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187268.0000 - val_fp: 1210.0000 - val_tn: 2637566.0000 - val_fn: 1216.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9935 - val_auc: 0.9991\n",
            "Epoch 189/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447035.9192 - fp: 2805.6465 - tn: 6295239.7273 - fn: 2824.4646 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187296.0000 - val_fp: 1179.0000 - val_tn: 2637597.0000 - val_fn: 1188.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9991\n",
            "Epoch 190/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447001.0000 - fp: 2836.4242 - tn: 6295208.9495 - fn: 2859.3838 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187292.0000 - val_fp: 1185.0000 - val_tn: 2637591.0000 - val_fn: 1192.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9990\n",
            "Epoch 191/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446983.5051 - fp: 2856.6364 - tn: 6295188.7374 - fn: 2876.8788 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187250.0000 - val_fp: 1227.0000 - val_tn: 2637549.0000 - val_fn: 1234.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9991\n",
            "Epoch 192/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0025 - tp: 446989.2626 - fp: 2850.7677 - tn: 6295194.6061 - fn: 2871.1212 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187204.0000 - val_fp: 1272.0000 - val_tn: 2637504.0000 - val_fn: 1280.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9991\n",
            "Epoch 193/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446979.7576 - fp: 2865.2323 - tn: 6295180.1414 - fn: 2880.6263 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187224.0000 - val_fp: 1251.0000 - val_tn: 2637525.0000 - val_fn: 1260.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9933 - val_auc: 0.9991\n",
            "Epoch 194/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0025 - tp: 446950.3939 - fp: 2891.8788 - tn: 6295153.4949 - fn: 2909.9899 - accuracy: 0.9991 - precision: 0.9935 - recall: 0.9935 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187161.0000 - val_fp: 1318.0000 - val_tn: 2637458.0000 - val_fn: 1323.0000 - val_accuracy: 0.9991 - val_precision: 0.9930 - val_recall: 0.9930 - val_auc: 0.9988\n",
            "Epoch 195/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0025 - tp: 446980.6061 - fp: 2858.3636 - tn: 6295187.0101 - fn: 2879.7778 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187234.0000 - val_fp: 1239.0000 - val_tn: 2637537.0000 - val_fn: 1250.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9990\n",
            "Epoch 196/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447048.7273 - fp: 2793.8182 - tn: 6295251.5556 - fn: 2811.6566 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9992 - val_loss: 0.0027 - val_tp: 187252.0000 - val_fp: 1222.0000 - val_tn: 2637554.0000 - val_fn: 1232.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9990\n",
            "Epoch 197/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0025 - tp: 447023.1212 - fp: 2818.0202 - tn: 6295227.3535 - fn: 2837.2626 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187236.0000 - val_fp: 1239.0000 - val_tn: 2637537.0000 - val_fn: 1248.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 198/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447080.4949 - fp: 2761.2525 - tn: 6295284.1212 - fn: 2779.8889 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187255.0000 - val_fp: 1222.0000 - val_tn: 2637554.0000 - val_fn: 1229.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9989\n",
            "Epoch 199/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447051.0707 - fp: 2792.8485 - tn: 6295252.5253 - fn: 2809.3131 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0028 - val_tp: 187080.0000 - val_fp: 1399.0000 - val_tn: 2637377.0000 - val_fn: 1404.0000 - val_accuracy: 0.9990 - val_precision: 0.9926 - val_recall: 0.9926 - val_auc: 0.9988\n",
            "Epoch 200/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447017.2929 - fp: 2824.8990 - tn: 6295220.4747 - fn: 2843.0909 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187287.0000 - val_fp: 1185.0000 - val_tn: 2637591.0000 - val_fn: 1197.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9936 - val_auc: 0.9991\n",
            "Epoch 201/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447025.4343 - fp: 2816.8384 - tn: 6295228.5354 - fn: 2834.9495 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187232.0000 - val_fp: 1244.0000 - val_tn: 2637532.0000 - val_fn: 1252.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 202/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446996.4747 - fp: 2846.9697 - tn: 6295198.4040 - fn: 2863.9091 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187253.0000 - val_fp: 1223.0000 - val_tn: 2637553.0000 - val_fn: 1231.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9991\n",
            "Epoch 203/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447001.2222 - fp: 2840.5253 - tn: 6295204.8485 - fn: 2859.1616 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187256.0000 - val_fp: 1223.0000 - val_tn: 2637553.0000 - val_fn: 1228.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9990\n",
            "Epoch 204/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 446988.0505 - fp: 2849.4646 - tn: 6295195.9091 - fn: 2872.3333 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187310.0000 - val_fp: 1170.0000 - val_tn: 2637606.0000 - val_fn: 1174.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9991\n",
            "Epoch 205/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447036.3636 - fp: 2806.1010 - tn: 6295239.2727 - fn: 2824.0202 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187184.0000 - val_fp: 1293.0000 - val_tn: 2637483.0000 - val_fn: 1300.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9931 - val_auc: 0.9992\n",
            "Epoch 206/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447029.2424 - fp: 2808.3232 - tn: 6295237.0505 - fn: 2831.1414 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187286.0000 - val_fp: 1191.0000 - val_tn: 2637585.0000 - val_fn: 1198.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9936 - val_auc: 0.9991\n",
            "Epoch 207/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447062.7071 - fp: 2783.3939 - tn: 6295261.9798 - fn: 2797.6768 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9992 - val_loss: 0.0027 - val_tp: 187190.0000 - val_fp: 1287.0000 - val_tn: 2637489.0000 - val_fn: 1294.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9931 - val_auc: 0.9989\n",
            "Epoch 208/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447002.7374 - fp: 2839.5859 - tn: 6295205.7879 - fn: 2857.6465 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187299.0000 - val_fp: 1179.0000 - val_tn: 2637597.0000 - val_fn: 1185.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9990\n",
            "Epoch 209/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447001.7980 - fp: 2839.1414 - tn: 6295206.2323 - fn: 2858.5859 - accuracy: 0.9991 - precision: 0.9936 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187222.0000 - val_fp: 1251.0000 - val_tn: 2637525.0000 - val_fn: 1262.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9933 - val_auc: 0.9992\n",
            "Epoch 210/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447006.0303 - fp: 2832.2929 - tn: 6295213.0808 - fn: 2854.3535 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187244.0000 - val_fp: 1231.0000 - val_tn: 2637545.0000 - val_fn: 1240.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 211/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447026.3333 - fp: 2815.9596 - tn: 6295229.4141 - fn: 2834.0505 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187269.0000 - val_fp: 1203.0000 - val_tn: 2637573.0000 - val_fn: 1215.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9991\n",
            "Epoch 212/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447022.7071 - fp: 2821.7778 - tn: 6295223.5960 - fn: 2837.6768 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187234.0000 - val_fp: 1240.0000 - val_tn: 2637536.0000 - val_fn: 1250.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9990\n",
            "Epoch 213/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447081.6465 - fp: 2761.2525 - tn: 6295284.1212 - fn: 2778.7374 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9938 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187250.0000 - val_fp: 1226.0000 - val_tn: 2637550.0000 - val_fn: 1234.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9993\n",
            "Epoch 214/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0025 - tp: 447035.5152 - fp: 2804.9192 - tn: 6295240.4545 - fn: 2824.8687 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9937 - auc: 0.9992 - val_loss: 0.0028 - val_tp: 187118.0000 - val_fp: 1353.0000 - val_tn: 2637423.0000 - val_fn: 1366.0000 - val_accuracy: 0.9990 - val_precision: 0.9928 - val_recall: 0.9928 - val_auc: 0.9988\n",
            "Epoch 215/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447047.1717 - fp: 2791.0707 - tn: 6295254.3030 - fn: 2813.2121 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0031 - val_tp: 186945.0000 - val_fp: 1531.0000 - val_tn: 2637245.0000 - val_fn: 1539.0000 - val_accuracy: 0.9989 - val_precision: 0.9919 - val_recall: 0.9918 - val_auc: 0.9986\n",
            "Epoch 216/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0025 - tp: 447006.2020 - fp: 2832.8384 - tn: 6295212.5354 - fn: 2854.1818 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9936 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187259.0000 - val_fp: 1211.0000 - val_tn: 2637565.0000 - val_fn: 1225.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9935 - val_auc: 0.9990\n",
            "Epoch 217/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447048.4747 - fp: 2789.5253 - tn: 6295255.8485 - fn: 2811.9091 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0027 - val_tp: 187179.0000 - val_fp: 1298.0000 - val_tn: 2637478.0000 - val_fn: 1305.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9931 - val_auc: 0.9989\n",
            "Epoch 218/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447086.5556 - fp: 2755.1212 - tn: 6295290.2525 - fn: 2773.8283 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187267.0000 - val_fp: 1207.0000 - val_tn: 2637569.0000 - val_fn: 1217.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9935 - val_auc: 0.9991\n",
            "Epoch 219/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447034.7374 - fp: 2807.5051 - tn: 6295237.8687 - fn: 2825.6465 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0030 - val_tp: 187008.0000 - val_fp: 1468.0000 - val_tn: 2637308.0000 - val_fn: 1476.0000 - val_accuracy: 0.9990 - val_precision: 0.9922 - val_recall: 0.9922 - val_auc: 0.9987\n",
            "Epoch 220/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447068.1515 - fp: 2778.0606 - tn: 6295267.3131 - fn: 2792.2323 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187288.0000 - val_fp: 1193.0000 - val_tn: 2637583.0000 - val_fn: 1196.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9990\n",
            "Epoch 221/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447069.7879 - fp: 2776.5152 - tn: 6295268.8586 - fn: 2790.5960 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187297.0000 - val_fp: 1181.0000 - val_tn: 2637595.0000 - val_fn: 1187.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9991\n",
            "Epoch 222/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447040.3030 - fp: 2805.7273 - tn: 6295239.6465 - fn: 2820.0808 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187276.0000 - val_fp: 1198.0000 - val_tn: 2637578.0000 - val_fn: 1208.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9991\n",
            "Epoch 223/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447085.0707 - fp: 2756.0202 - tn: 6295289.3535 - fn: 2775.3131 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187226.0000 - val_fp: 1247.0000 - val_tn: 2637529.0000 - val_fn: 1258.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9933 - val_auc: 0.9989\n",
            "Epoch 224/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447057.8485 - fp: 2781.1515 - tn: 6295264.2222 - fn: 2802.5354 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187282.0000 - val_fp: 1197.0000 - val_tn: 2637579.0000 - val_fn: 1202.0000 - val_accuracy: 0.9992 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9992\n",
            "Epoch 225/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447070.3838 - fp: 2776.5556 - tn: 6295268.8182 - fn: 2790.0000 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187212.0000 - val_fp: 1265.0000 - val_tn: 2637511.0000 - val_fn: 1272.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9933 - val_auc: 0.9989\n",
            "Epoch 226/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0024 - tp: 447074.1818 - fp: 2772.0909 - tn: 6295273.2828 - fn: 2786.2020 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187317.0000 - val_fp: 1160.0000 - val_tn: 2637616.0000 - val_fn: 1167.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9991\n",
            "Epoch 227/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447081.3535 - fp: 2756.4848 - tn: 6295288.8889 - fn: 2779.0303 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187317.0000 - val_fp: 1161.0000 - val_tn: 2637615.0000 - val_fn: 1167.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9991\n",
            "Epoch 228/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447151.7879 - fp: 2693.1717 - tn: 6295352.2020 - fn: 2708.5960 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187280.0000 - val_fp: 1193.0000 - val_tn: 2637583.0000 - val_fn: 1204.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9936 - val_auc: 0.9991\n",
            "Epoch 229/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447104.6061 - fp: 2738.1717 - tn: 6295307.2020 - fn: 2755.7778 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187302.0000 - val_fp: 1174.0000 - val_tn: 2637602.0000 - val_fn: 1182.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9937 - val_auc: 0.9990\n",
            "Epoch 230/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0024 - tp: 447073.2020 - fp: 2766.6162 - tn: 6295278.7576 - fn: 2787.1818 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187278.0000 - val_fp: 1200.0000 - val_tn: 2637576.0000 - val_fn: 1206.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9991\n",
            "Epoch 231/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0024 - tp: 447102.3939 - fp: 2743.7172 - tn: 6295301.6566 - fn: 2757.9899 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187328.0000 - val_fp: 1151.0000 - val_tn: 2637625.0000 - val_fn: 1156.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9939 - val_auc: 0.9991\n",
            "Epoch 232/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447129.6364 - fp: 2709.4646 - tn: 6295335.9091 - fn: 2730.7475 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187301.0000 - val_fp: 1177.0000 - val_tn: 2637599.0000 - val_fn: 1183.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9937 - val_auc: 0.9992\n",
            "Epoch 233/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0025 - tp: 447063.4747 - fp: 2774.5859 - tn: 6295270.7879 - fn: 2796.9091 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187259.0000 - val_fp: 1218.0000 - val_tn: 2637558.0000 - val_fn: 1225.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9993\n",
            "Epoch 234/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447152.2626 - fp: 2689.3737 - tn: 6295356.0000 - fn: 2708.1212 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187290.0000 - val_fp: 1190.0000 - val_tn: 2637586.0000 - val_fn: 1194.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9991\n",
            "Epoch 235/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447099.7576 - fp: 2743.0404 - tn: 6295302.3333 - fn: 2760.6263 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187260.0000 - val_fp: 1218.0000 - val_tn: 2637558.0000 - val_fn: 1224.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9992\n",
            "Epoch 236/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447074.8990 - fp: 2767.3838 - tn: 6295277.9899 - fn: 2785.4848 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9938 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187277.0000 - val_fp: 1202.0000 - val_tn: 2637574.0000 - val_fn: 1207.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9990\n",
            "Epoch 237/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447107.3939 - fp: 2736.0202 - tn: 6295309.3535 - fn: 2752.9899 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187248.0000 - val_fp: 1231.0000 - val_tn: 2637545.0000 - val_fn: 1236.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9934 - val_auc: 0.9990\n",
            "Epoch 238/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447113.8283 - fp: 2725.9394 - tn: 6295319.4343 - fn: 2746.5556 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9938 - auc: 0.9992 - val_loss: 0.0027 - val_tp: 187208.0000 - val_fp: 1265.0000 - val_tn: 2637511.0000 - val_fn: 1276.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9988\n",
            "Epoch 239/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0024 - tp: 447047.3636 - fp: 2797.3434 - tn: 6295248.0303 - fn: 2813.0202 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187329.0000 - val_fp: 1148.0000 - val_tn: 2637628.0000 - val_fn: 1155.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9939 - val_auc: 0.9992\n",
            "Epoch 240/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447095.5758 - fp: 2743.3232 - tn: 6295302.0505 - fn: 2764.8081 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9938 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187321.0000 - val_fp: 1157.0000 - val_tn: 2637619.0000 - val_fn: 1163.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9938 - val_auc: 0.9991\n",
            "Epoch 241/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447068.0909 - fp: 2765.5152 - tn: 6295279.8586 - fn: 2792.2929 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187274.0000 - val_fp: 1206.0000 - val_tn: 2637570.0000 - val_fn: 1210.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9990\n",
            "Epoch 242/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447125.7374 - fp: 2711.7778 - tn: 6295333.5960 - fn: 2734.6465 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187266.0000 - val_fp: 1212.0000 - val_tn: 2637564.0000 - val_fn: 1218.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9935 - val_auc: 0.9989\n",
            "Epoch 243/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447153.5051 - fp: 2691.4545 - tn: 6295353.9192 - fn: 2706.8788 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187276.0000 - val_fp: 1205.0000 - val_tn: 2637571.0000 - val_fn: 1208.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9990\n",
            "Epoch 244/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447148.3030 - fp: 2688.4848 - tn: 6295356.8889 - fn: 2712.0808 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187255.0000 - val_fp: 1224.0000 - val_tn: 2637552.0000 - val_fn: 1229.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9990\n",
            "Epoch 245/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447040.6162 - fp: 2799.3636 - tn: 6295246.0101 - fn: 2819.7677 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187204.0000 - val_fp: 1269.0000 - val_tn: 2637507.0000 - val_fn: 1280.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9932 - val_auc: 0.9988\n",
            "Epoch 246/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0024 - tp: 447062.8889 - fp: 2780.9798 - tn: 6295264.3939 - fn: 2797.4949 - accuracy: 0.9992 - precision: 0.9937 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187294.0000 - val_fp: 1179.0000 - val_tn: 2637597.0000 - val_fn: 1190.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9989\n",
            "Epoch 247/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447190.0000 - fp: 2651.5960 - tn: 6295393.7778 - fn: 2670.3838 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187265.0000 - val_fp: 1211.0000 - val_tn: 2637565.0000 - val_fn: 1219.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9935 - val_auc: 0.9989\n",
            "Epoch 248/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447137.0303 - fp: 2707.0909 - tn: 6295338.2828 - fn: 2723.3535 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187291.0000 - val_fp: 1182.0000 - val_tn: 2637594.0000 - val_fn: 1193.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9991\n",
            "Epoch 249/300\n",
            "98/98 [==============================] - 3s 28ms/step - loss: 0.0023 - tp: 447111.9192 - fp: 2729.6162 - tn: 6295315.7576 - fn: 2748.4646 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187319.0000 - val_fp: 1157.0000 - val_tn: 2637619.0000 - val_fn: 1165.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9938 - val_auc: 0.9991\n",
            "Epoch 250/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447084.0707 - fp: 2758.3434 - tn: 6295287.0303 - fn: 2776.3131 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187225.0000 - val_fp: 1253.0000 - val_tn: 2637523.0000 - val_fn: 1259.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9933 - val_auc: 0.9989\n",
            "Epoch 251/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447128.6061 - fp: 2714.1717 - tn: 6295331.2020 - fn: 2731.7778 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187240.0000 - val_fp: 1240.0000 - val_tn: 2637536.0000 - val_fn: 1244.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 252/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447141.6263 - fp: 2692.3333 - tn: 6295353.0404 - fn: 2718.7576 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187234.0000 - val_fp: 1247.0000 - val_tn: 2637529.0000 - val_fn: 1250.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 253/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447098.9495 - fp: 2743.7879 - tn: 6295301.5859 - fn: 2761.4343 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187281.0000 - val_fp: 1195.0000 - val_tn: 2637581.0000 - val_fn: 1203.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9936 - val_auc: 0.9989\n",
            "Epoch 254/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447183.3131 - fp: 2655.9596 - tn: 6295389.4141 - fn: 2677.0707 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187281.0000 - val_fp: 1194.0000 - val_tn: 2637582.0000 - val_fn: 1203.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9936 - val_auc: 0.9992\n",
            "Epoch 255/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447159.1414 - fp: 2683.6667 - tn: 6295361.7071 - fn: 2701.2424 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187252.0000 - val_fp: 1225.0000 - val_tn: 2637551.0000 - val_fn: 1232.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9989\n",
            "Epoch 256/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447161.6364 - fp: 2677.4646 - tn: 6295367.9091 - fn: 2698.7475 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0027 - val_tp: 187180.0000 - val_fp: 1297.0000 - val_tn: 2637479.0000 - val_fn: 1304.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9931 - val_auc: 0.9988\n",
            "Epoch 257/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447139.6263 - fp: 2707.0404 - tn: 6295338.3333 - fn: 2720.7576 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187307.0000 - val_fp: 1173.0000 - val_tn: 2637603.0000 - val_fn: 1177.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9991\n",
            "Epoch 258/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447173.5455 - fp: 2666.3535 - tn: 6295379.0202 - fn: 2686.8384 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187219.0000 - val_fp: 1261.0000 - val_tn: 2637515.0000 - val_fn: 1265.0000 - val_accuracy: 0.9991 - val_precision: 0.9933 - val_recall: 0.9933 - val_auc: 0.9989\n",
            "Epoch 259/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447086.3636 - fp: 2750.6970 - tn: 6295294.6768 - fn: 2774.0202 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187187.0000 - val_fp: 1292.0000 - val_tn: 2637484.0000 - val_fn: 1297.0000 - val_accuracy: 0.9991 - val_precision: 0.9931 - val_recall: 0.9931 - val_auc: 0.9989\n",
            "Epoch 260/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447113.9899 - fp: 2724.9596 - tn: 6295320.4141 - fn: 2746.3939 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0026 - val_tp: 187240.0000 - val_fp: 1238.0000 - val_tn: 2637538.0000 - val_fn: 1244.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 261/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447126.8182 - fp: 2718.0000 - tn: 6295327.3737 - fn: 2733.5657 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9938 - auc: 0.9992 - val_loss: 0.0028 - val_tp: 187062.0000 - val_fp: 1415.0000 - val_tn: 2637361.0000 - val_fn: 1422.0000 - val_accuracy: 0.9990 - val_precision: 0.9925 - val_recall: 0.9925 - val_auc: 0.9988\n",
            "Epoch 262/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447139.5960 - fp: 2699.8283 - tn: 6295345.5455 - fn: 2720.7879 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187294.0000 - val_fp: 1182.0000 - val_tn: 2637594.0000 - val_fn: 1190.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9993\n",
            "Epoch 263/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447145.8182 - fp: 2694.3030 - tn: 6295351.0707 - fn: 2714.5657 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187330.0000 - val_fp: 1148.0000 - val_tn: 2637628.0000 - val_fn: 1154.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9939 - val_auc: 0.9992\n",
            "Epoch 264/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447131.8889 - fp: 2705.9798 - tn: 6295339.3939 - fn: 2728.4949 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187191.0000 - val_fp: 1285.0000 - val_tn: 2637491.0000 - val_fn: 1293.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9931 - val_auc: 0.9989\n",
            "Epoch 265/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447216.8081 - fp: 2623.6061 - tn: 6295421.7677 - fn: 2643.5758 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0027 - val_tp: 187095.0000 - val_fp: 1384.0000 - val_tn: 2637392.0000 - val_fn: 1389.0000 - val_accuracy: 0.9990 - val_precision: 0.9927 - val_recall: 0.9926 - val_auc: 0.9988\n",
            "Epoch 266/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0024 - tp: 447160.9697 - fp: 2679.2020 - tn: 6295366.1717 - fn: 2699.4141 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9991 - val_loss: 0.0025 - val_tp: 187323.0000 - val_fp: 1151.0000 - val_tn: 2637625.0000 - val_fn: 1161.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9938 - val_auc: 0.9990\n",
            "Epoch 267/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447208.7273 - fp: 2628.8081 - tn: 6295416.5657 - fn: 2651.6566 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9942 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187341.0000 - val_fp: 1135.0000 - val_tn: 2637641.0000 - val_fn: 1143.0000 - val_accuracy: 0.9992 - val_precision: 0.9940 - val_recall: 0.9939 - val_auc: 0.9992\n",
            "Epoch 268/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447182.3838 - fp: 2653.9899 - tn: 6295391.3838 - fn: 2678.0000 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187355.0000 - val_fp: 1124.0000 - val_tn: 2637652.0000 - val_fn: 1129.0000 - val_accuracy: 0.9992 - val_precision: 0.9940 - val_recall: 0.9940 - val_auc: 0.9991\n",
            "Epoch 269/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447131.7172 - fp: 2707.2525 - tn: 6295338.1212 - fn: 2728.6667 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187360.0000 - val_fp: 1117.0000 - val_tn: 2637659.0000 - val_fn: 1124.0000 - val_accuracy: 0.9992 - val_precision: 0.9941 - val_recall: 0.9940 - val_auc: 0.9991\n",
            "Epoch 270/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447174.4343 - fp: 2665.6465 - tn: 6295379.7273 - fn: 2685.9495 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187293.0000 - val_fp: 1184.0000 - val_tn: 2637592.0000 - val_fn: 1191.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9990\n",
            "Epoch 271/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0024 - tp: 447139.6364 - fp: 2703.6768 - tn: 6295341.6970 - fn: 2720.7475 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9939 - auc: 0.9991 - val_loss: 0.0024 - val_tp: 187360.0000 - val_fp: 1118.0000 - val_tn: 2637658.0000 - val_fn: 1124.0000 - val_accuracy: 0.9992 - val_precision: 0.9941 - val_recall: 0.9940 - val_auc: 0.9991\n",
            "Epoch 272/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447237.8586 - fp: 2601.2424 - tn: 6295444.1313 - fn: 2622.5253 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9942 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187288.0000 - val_fp: 1188.0000 - val_tn: 2637588.0000 - val_fn: 1196.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9993\n",
            "Epoch 273/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0024 - tp: 447148.0909 - fp: 2690.2929 - tn: 6295355.0808 - fn: 2712.2929 - accuracy: 0.9992 - precision: 0.9939 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187276.0000 - val_fp: 1201.0000 - val_tn: 2637575.0000 - val_fn: 1208.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9936 - val_auc: 0.9989\n",
            "Epoch 274/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447178.7172 - fp: 2659.3939 - tn: 6295385.9798 - fn: 2681.6667 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187282.0000 - val_fp: 1194.0000 - val_tn: 2637582.0000 - val_fn: 1202.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9936 - val_auc: 0.9989\n",
            "Epoch 275/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447234.3535 - fp: 2600.2929 - tn: 6295445.0808 - fn: 2626.0303 - accuracy: 0.9992 - precision: 0.9943 - recall: 0.9942 - auc: 0.9992 - val_loss: 0.0030 - val_tp: 186969.0000 - val_fp: 1499.0000 - val_tn: 2637277.0000 - val_fn: 1515.0000 - val_accuracy: 0.9989 - val_precision: 0.9920 - val_recall: 0.9920 - val_auc: 0.9986\n",
            "Epoch 276/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447189.1818 - fp: 2648.9192 - tn: 6295396.4545 - fn: 2671.2020 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9991 - val_loss: 0.0024 - val_tp: 187318.0000 - val_fp: 1159.0000 - val_tn: 2637617.0000 - val_fn: 1166.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9938 - val_auc: 0.9993\n",
            "Epoch 277/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447185.5960 - fp: 2652.5455 - tn: 6295392.8283 - fn: 2674.7879 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187342.0000 - val_fp: 1137.0000 - val_tn: 2637639.0000 - val_fn: 1142.0000 - val_accuracy: 0.9992 - val_precision: 0.9940 - val_recall: 0.9939 - val_auc: 0.9990\n",
            "Epoch 278/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447209.0101 - fp: 2632.5960 - tn: 6295412.7778 - fn: 2651.3737 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187295.0000 - val_fp: 1182.0000 - val_tn: 2637594.0000 - val_fn: 1189.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9990\n",
            "Epoch 279/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447201.0202 - fp: 2636.8687 - tn: 6295408.5051 - fn: 2659.3636 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187250.0000 - val_fp: 1221.0000 - val_tn: 2637555.0000 - val_fn: 1234.0000 - val_accuracy: 0.9991 - val_precision: 0.9935 - val_recall: 0.9935 - val_auc: 0.9990\n",
            "Epoch 280/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447177.5859 - fp: 2655.9394 - tn: 6295389.4343 - fn: 2682.7980 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187300.0000 - val_fp: 1171.0000 - val_tn: 2637605.0000 - val_fn: 1184.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9937 - val_auc: 0.9991\n",
            "Epoch 281/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447186.5455 - fp: 2652.3838 - tn: 6295392.9899 - fn: 2673.8384 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187302.0000 - val_fp: 1175.0000 - val_tn: 2637601.0000 - val_fn: 1182.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9937 - val_auc: 0.9990\n",
            "Epoch 282/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447215.1919 - fp: 2620.1212 - tn: 6295425.2525 - fn: 2645.1919 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0027 - val_tp: 187108.0000 - val_fp: 1375.0000 - val_tn: 2637401.0000 - val_fn: 1376.0000 - val_accuracy: 0.9990 - val_precision: 0.9927 - val_recall: 0.9927 - val_auc: 0.9987\n",
            "Epoch 283/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447166.5152 - fp: 2674.2222 - tn: 6295371.1515 - fn: 2693.8687 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187333.0000 - val_fp: 1137.0000 - val_tn: 2637639.0000 - val_fn: 1151.0000 - val_accuracy: 0.9992 - val_precision: 0.9940 - val_recall: 0.9939 - val_auc: 0.9992\n",
            "Epoch 284/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447133.0202 - fp: 2703.9192 - tn: 6295341.4545 - fn: 2727.3636 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9939 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187339.0000 - val_fp: 1138.0000 - val_tn: 2637638.0000 - val_fn: 1145.0000 - val_accuracy: 0.9992 - val_precision: 0.9940 - val_recall: 0.9939 - val_auc: 0.9990\n",
            "Epoch 285/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447223.8283 - fp: 2613.2828 - tn: 6295432.0909 - fn: 2636.5556 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9942 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187378.0000 - val_fp: 1093.0000 - val_tn: 2637683.0000 - val_fn: 1106.0000 - val_accuracy: 0.9992 - val_precision: 0.9942 - val_recall: 0.9941 - val_auc: 0.9992\n",
            "Epoch 286/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0024 - tp: 447074.0202 - fp: 2761.0303 - tn: 6295284.3434 - fn: 2786.3636 - accuracy: 0.9992 - precision: 0.9938 - recall: 0.9937 - auc: 0.9991 - val_loss: 0.0024 - val_tp: 187339.0000 - val_fp: 1138.0000 - val_tn: 2637638.0000 - val_fn: 1145.0000 - val_accuracy: 0.9992 - val_precision: 0.9940 - val_recall: 0.9939 - val_auc: 0.9991\n",
            "Epoch 287/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447222.7677 - fp: 2610.0606 - tn: 6295435.3131 - fn: 2637.6162 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187312.0000 - val_fp: 1163.0000 - val_tn: 2637613.0000 - val_fn: 1172.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9992\n",
            "Epoch 288/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447198.5455 - fp: 2645.8687 - tn: 6295399.5051 - fn: 2661.8384 - accuracy: 0.9992 - precision: 0.9940 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187361.0000 - val_fp: 1114.0000 - val_tn: 2637662.0000 - val_fn: 1123.0000 - val_accuracy: 0.9992 - val_precision: 0.9941 - val_recall: 0.9940 - val_auc: 0.9991\n",
            "Epoch 289/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447198.2727 - fp: 2633.7677 - tn: 6295411.6061 - fn: 2662.1111 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187346.0000 - val_fp: 1134.0000 - val_tn: 2637642.0000 - val_fn: 1138.0000 - val_accuracy: 0.9992 - val_precision: 0.9940 - val_recall: 0.9940 - val_auc: 0.9990\n",
            "Epoch 290/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447201.8687 - fp: 2635.9697 - tn: 6295409.4040 - fn: 2658.5152 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187262.0000 - val_fp: 1212.0000 - val_tn: 2637564.0000 - val_fn: 1222.0000 - val_accuracy: 0.9991 - val_precision: 0.9936 - val_recall: 0.9935 - val_auc: 0.9989\n",
            "Epoch 291/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447237.5253 - fp: 2594.7677 - tn: 6295450.6061 - fn: 2622.8586 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9942 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187244.0000 - val_fp: 1235.0000 - val_tn: 2637541.0000 - val_fn: 1240.0000 - val_accuracy: 0.9991 - val_precision: 0.9934 - val_recall: 0.9934 - val_auc: 0.9989\n",
            "Epoch 292/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447218.0202 - fp: 2617.8687 - tn: 6295427.5051 - fn: 2642.3636 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9942 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187343.0000 - val_fp: 1132.0000 - val_tn: 2637644.0000 - val_fn: 1141.0000 - val_accuracy: 0.9992 - val_precision: 0.9940 - val_recall: 0.9939 - val_auc: 0.9992\n",
            "Epoch 293/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0023 - tp: 447211.2828 - fp: 2620.6061 - tn: 6295424.7677 - fn: 2649.1010 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187312.0000 - val_fp: 1158.0000 - val_tn: 2637618.0000 - val_fn: 1172.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9938 - val_auc: 0.9990\n",
            "Epoch 294/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0022 - tp: 447282.8384 - fp: 2557.8384 - tn: 6295487.5354 - fn: 2577.5455 - accuracy: 0.9992 - precision: 0.9944 - recall: 0.9943 - auc: 0.9992 - val_loss: 0.0025 - val_tp: 187298.0000 - val_fp: 1179.0000 - val_tn: 2637597.0000 - val_fn: 1186.0000 - val_accuracy: 0.9992 - val_precision: 0.9937 - val_recall: 0.9937 - val_auc: 0.9990\n",
            "Epoch 295/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0022 - tp: 447259.8889 - fp: 2575.0202 - tn: 6295470.3535 - fn: 2600.4949 - accuracy: 0.9992 - precision: 0.9943 - recall: 0.9942 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187321.0000 - val_fp: 1160.0000 - val_tn: 2637616.0000 - val_fn: 1163.0000 - val_accuracy: 0.9992 - val_precision: 0.9938 - val_recall: 0.9938 - val_auc: 0.9990\n",
            "Epoch 296/300\n",
            "98/98 [==============================] - 3s 28ms/step - loss: 0.0023 - tp: 447204.5253 - fp: 2626.9899 - tn: 6295418.3838 - fn: 2655.8586 - accuracy: 0.9992 - precision: 0.9941 - recall: 0.9940 - auc: 0.9992 - val_loss: 0.0026 - val_tp: 187200.0000 - val_fp: 1275.0000 - val_tn: 2637501.0000 - val_fn: 1284.0000 - val_accuracy: 0.9991 - val_precision: 0.9932 - val_recall: 0.9932 - val_auc: 0.9989\n",
            "Epoch 297/300\n",
            "98/98 [==============================] - 3s 27ms/step - loss: 0.0023 - tp: 447227.6162 - fp: 2614.2929 - tn: 6295431.0808 - fn: 2632.7677 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9941 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187333.0000 - val_fp: 1143.0000 - val_tn: 2637633.0000 - val_fn: 1151.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9939 - val_auc: 0.9990\n",
            "Epoch 298/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0022 - tp: 447265.4949 - fp: 2566.5354 - tn: 6295478.8384 - fn: 2594.8889 - accuracy: 0.9992 - precision: 0.9944 - recall: 0.9943 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187381.0000 - val_fp: 1098.0000 - val_tn: 2637678.0000 - val_fn: 1103.0000 - val_accuracy: 0.9992 - val_precision: 0.9942 - val_recall: 0.9941 - val_auc: 0.9992\n",
            "Epoch 299/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0022 - tp: 447253.7576 - fp: 2589.7576 - tn: 6295455.6162 - fn: 2606.6263 - accuracy: 0.9992 - precision: 0.9942 - recall: 0.9942 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187361.0000 - val_fp: 1112.0000 - val_tn: 2637664.0000 - val_fn: 1123.0000 - val_accuracy: 0.9992 - val_precision: 0.9941 - val_recall: 0.9940 - val_auc: 0.9992\n",
            "Epoch 300/300\n",
            "98/98 [==============================] - 3s 26ms/step - loss: 0.0022 - tp: 447270.7273 - fp: 2566.7576 - tn: 6295478.6162 - fn: 2589.6566 - accuracy: 0.9992 - precision: 0.9944 - recall: 0.9943 - auc: 0.9992 - val_loss: 0.0024 - val_tp: 187336.0000 - val_fp: 1144.0000 - val_tn: 2637632.0000 - val_fn: 1148.0000 - val_accuracy: 0.9992 - val_precision: 0.9939 - val_recall: 0.9939 - val_auc: 0.9993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7q_MD9Q5_Jd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPDH0IA_8xee"
      },
      "source": [
        "y_pred=model_dnn.predict(X_test)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PdaWVhX9BvJ",
        "outputId": "8bfc1f01-2124-458b-ed55-51462e5a2399"
      },
      "source": [
        "accuracy = model_dnn.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)\r\n",
        "print('Accuracy is: ', accuracy)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 0s 20ms/step - loss: 0.0027 - tp: 187226.0000 - fp: 1239.0000 - tn: 2637523.0000 - fn: 1257.0000 - accuracy: 0.9991 - precision: 0.9934 - recall: 0.9933 - auc: 0.9991\n",
            "Accuracy is:  [0.0027007064782083035, 187226.0, 1239.0, 2637523.0, 1257.0, 0.9991171360015869, 0.9934258460998535, 0.9933309555053711, 0.9990645051002502]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZcyLQPqAlWS",
        "outputId": "28d83710-cf1e-4d60-8f2d-f791a3984efd"
      },
      "source": [
        "display_metrics(y_test_ada, np.argmax(y_pred, axis = 1), labels_d)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.99\n",
            "\n",
            "Micro Precision: 0.99\n",
            "Micro Recall: 0.99\n",
            "Micro F1-score: 0.99\n",
            "\n",
            "Macro Precision: 0.89\n",
            "Macro Recall: 0.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.89\n",
            "\n",
            "Weighted Precision: 0.99\n",
            "Weighted Recall: 0.99\n",
            "Weighted F1-score: 0.99\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       1.00      0.99      0.99    105019\n",
            "                       Bot       0.90      0.99      0.94       280\n",
            "                      DDoS       1.00      1.00      1.00     19271\n",
            "             DoS GoldenEye       1.00      0.98      0.99      1542\n",
            "                  DoS Hulk       1.00      1.00      1.00     34547\n",
            "          DoS Slowhttptest       1.00      0.99      0.99       828\n",
            "             DoS slowloris       0.99      0.99      0.99       834\n",
            "               FTP-Patator       1.00      1.00      1.00      1178\n",
            "                Heartbleed       1.00      1.00      1.00         2\n",
            "              Infiltration       1.00      1.00      1.00         1\n",
            "                  PortScan       0.98      0.98      0.98     23846\n",
            "               SSH-Patator       0.98      0.98      0.98       826\n",
            "  Web Attack � Brute Force       0.81      0.81      0.81       209\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         7\n",
            "          Web Attack � XSS       0.72      0.55      0.62        93\n",
            "\n",
            "                  accuracy                           0.99    188483\n",
            "                 macro avg       0.89      0.88      0.89    188483\n",
            "              weighted avg       0.99      0.99      0.99    188483\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erUa8gpIZ04U"
      },
      "source": [
        "# Model 10: CNN1D "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTmLBZDpZ04U"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Flatten, Dense, Activation,Dropout\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLXy3meMZ04U",
        "outputId": "f144a82f-a850-49c4-974e-1d2ae9ca3ca2"
      },
      "source": [
        "#hyper-params\n",
        "batch_size = 1024 # increasing batch size with more gpu added\n",
        "input_dim = X_train.shape[1]\n",
        "num_class = 15                   # 15 intrusion classes, including benign traffic class\n",
        "num_epochs = 30\n",
        "learning_rates = 1e-3\n",
        "regularizations = 1e-3\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "print(input_dim)\n",
        "print(num_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNRbsq6CZ04V",
        "outputId": "148ee859-e320-4a22-c118-cabe52ab7d56"
      },
      "source": [
        "#X_train_r = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_train_r = np.zeros((len(X_train), input_dim, 1))\n",
        "X_train_r[:, :, 0] = X_train[:, :input_dim]\n",
        "print(X_train_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjmIpn1HZ04W",
        "outputId": "2c1c2f93-014b-4c08-8435-e739318768fc"
      },
      "source": [
        "X_test_r = np.zeros((len(X_test), input_dim, 1))\n",
        "X_test_r[:, :, 0] = X_test[:, :input_dim]\n",
        "print(X_test_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_KlI1VeZ04W",
        "outputId": "68ffab00-7db7-42ca-d671-16330e9fa8f9"
      },
      "source": [
        "X_val_r = np.zeros((len(X_val), input_dim, 1))\n",
        "X_val_r[:, :, 0] = X_val[:, :input_dim]\n",
        "print(X_val_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkQlzQU0ajuD",
        "outputId": "71d58c31-8c34-435a-934b-9006668ffdc5"
      },
      "source": [
        "X_train_r"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[7.66048676e-01],\n",
              "        [5.93603125e-03],\n",
              "        [1.00000000e+00],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[7.29167620e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [7.12500000e-01],\n",
              "        [7.12500000e-01]],\n",
              "\n",
              "       [[7.86144808e-01],\n",
              "        [6.76005616e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [8.96864890e-03],\n",
              "        [4.90833333e-01],\n",
              "        [4.83333333e-01]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[2.53009842e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[7.85610742e-01],\n",
              "        [8.08765183e-04],\n",
              "        [1.00000000e+00],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]],\n",
              "\n",
              "       [[8.26276036e-01],\n",
              "        [1.22077764e-03],\n",
              "        [3.52941176e-01],\n",
              "        ...,\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00],\n",
              "        [0.00000000e+00]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpTZU5OPZ04W",
        "outputId": "31aa9725-0fe7-4c0d-9ab7-1b4176ad7fcd"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', input_shape=(71,1)))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=3))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_class))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_8 (Conv1D)            (None, 71, 32)            128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 71, 32)            284       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 71, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 69, 128)           12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 69, 128)           276       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 69, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 8832)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               883300    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 15)                1515      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 897,919\n",
            "Trainable params: 897,639\n",
            "Non-trainable params: 280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEpzDBNyC7P2"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_accuracy', \r\n",
        "    verbose=1,\r\n",
        "    patience=10,\r\n",
        "    mode='max',\r\n",
        "    restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb52JhHSZ04W"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFJZ11t1Z04X"
      },
      "source": [
        "## Step 5. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq31n_0YZ04X",
        "outputId": "ad9b4e62-35a9-429f-9d10-4086725a878f"
      },
      "source": [
        "# fit network\n",
        "epochs = 50\n",
        "model.fit(X_train_r, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_r, y_test), verbose=1, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 0.9541 - val_accuracy: 0.8272\n",
            "Epoch 2/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.5939 - val_accuracy: 0.8978\n",
            "Epoch 3/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0106 - accuracy: 0.9963 - val_loss: 0.4867 - val_accuracy: 0.9373\n",
            "Epoch 4/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 1.2544 - val_accuracy: 0.7860\n",
            "Epoch 5/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.5194 - val_accuracy: 0.9147\n",
            "Epoch 6/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 0.7574 - val_accuracy: 0.8717\n",
            "Epoch 7/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.4678 - val_accuracy: 0.9551\n",
            "Epoch 8/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.3560 - val_accuracy: 0.9581\n",
            "Epoch 9/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 0.7849 - val_accuracy: 0.8832\n",
            "Epoch 10/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0091 - accuracy: 0.9967 - val_loss: 0.3110 - val_accuracy: 0.9654\n",
            "Epoch 11/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 0.7204 - val_accuracy: 0.8893\n",
            "Epoch 12/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 0.5198 - val_accuracy: 0.9458\n",
            "Epoch 13/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0090 - accuracy: 0.9967 - val_loss: 0.9469 - val_accuracy: 0.8524\n",
            "Epoch 14/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0085 - accuracy: 0.9968 - val_loss: 0.3824 - val_accuracy: 0.9637\n",
            "Epoch 15/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 0.3664 - val_accuracy: 0.9600\n",
            "Epoch 16/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0088 - accuracy: 0.9967 - val_loss: 0.5777 - val_accuracy: 0.9319\n",
            "Epoch 17/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0087 - accuracy: 0.9967 - val_loss: 0.3704 - val_accuracy: 0.9621\n",
            "Epoch 18/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0086 - accuracy: 0.9968 - val_loss: 0.4081 - val_accuracy: 0.9647\n",
            "Epoch 19/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.4933 - val_accuracy: 0.9559\n",
            "Epoch 20/50\n",
            "544/544 [==============================] - 10s 19ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 1.0666 - val_accuracy: 0.8716\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00020: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7fd100bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSo0TpT5Z04X",
        "outputId": "d07be9f6-400d-481c-8ee2-ebceae01716b"
      },
      "source": [
        "# evaluate model\n",
        "accuracy = model.evaluate(X_val_r, y_val, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "272/272 [==============================] - 2s 6ms/step - loss: 79.3116 - accuracy: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqGNmOrhH1CL"
      },
      "source": [
        "y_pred=model.predict(X_val_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltyd2KVLIBCK",
        "outputId": "aad22263-814e-4f9e-a3c2-621c490ef3df"
      },
      "source": [
        "display_metrics(y_val_ada, np.argmax(y_pred, axis = 1), labels_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 0.50\n",
            "\n",
            "Micro Precision: 0.50\n",
            "Micro Recall: 0.50\n",
            "Micro F1-score: 0.50\n",
            "\n",
            "Macro Precision: 0.03\n",
            "Macro Recall: 0.07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro F1-score: 0.04\n",
            "\n",
            "Weighted Precision: 0.25\n",
            "Weighted Recall: 0.50\n",
            "Weighted F1-score: 0.33\n",
            "\n",
            "Classification Report\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.50      1.00      0.67    139135\n",
            "                       Bot       0.00      0.00      0.00       489\n",
            "                      DDoS       0.00      0.00      0.00     32006\n",
            "             DoS GoldenEye       0.00      0.00      0.00      2573\n",
            "                  DoS Hulk       0.00      0.00      0.00     57531\n",
            "          DoS Slowhttptest       0.00      0.00      0.00      1374\n",
            "             DoS slowloris       0.00      0.00      0.00      1449\n",
            "               FTP-Patator       0.00      0.00      0.00      1983\n",
            "                Heartbleed       0.00      0.00      0.00         2\n",
            "              Infiltration       0.00      0.00      0.00         9\n",
            "                  PortScan       0.00      0.00      0.00     39701\n",
            "               SSH-Patator       0.00      0.00      0.00      1474\n",
            "  Web Attack � Brute Force       0.00      0.00      0.00       376\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         5\n",
            "          Web Attack � XSS       0.00      0.00      0.00       163\n",
            "\n",
            "                  accuracy                           0.50    278270\n",
            "                 macro avg       0.03      0.07      0.04    278270\n",
            "              weighted avg       0.25      0.50      0.33    278270\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbhp6oW_Z04X"
      },
      "source": [
        ""
      ]
    }
  ]
}